<!-- Code generated by gomarkdoc. DO NOT EDIT -->

# sarama

```go
import "github.com/vmware/service-level-indicator-exporter-for-kafka/vendor/github.com/Shopify/sarama"
```

Package sarama is a pure Go client library for dealing with Apache Kafka \(versions 0.8 and later\). It includes a high\-level API for easily producing and consuming messages, and a low\-level API for controlling bytes on the wire when the high\-level API is insufficient. Usage examples for the high\-level APIs are provided inline with their full documentation.

To produce messages, use either the AsyncProducer or the SyncProducer. The AsyncProducer accepts messages on a channel and produces them asynchronously in the background as efficiently as possible; it is preferred in most cases. The SyncProducer provides a method which will block until Kafka acknowledges the message as produced. This can be useful but comes with two caveats: it will generally be less efficient, and the actual durability guarantees depend on the configured value of \`Producer.RequiredAcks\`. There are configurations where a message acknowledged by the SyncProducer can still sometimes be lost.

To consume messages, use Consumer or Consumer\-Group API.

For lower\-level needs, the Broker and Request/Response objects permit precise control over each connection and message sent on the wire; the Client provides higher\-level metadata management that is shared between the producers and the consumer. The Request/Response objects and properties are mostly undocumented, as they line up exactly with the protocol fields documented by Kafka at https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol

Metrics are exposed through https://github.com/rcrowley/go-metrics library in a local registry.

Broker related metrics:

```
+----------------------------------------------+------------+---------------------------------------------------------------+
| Name                                         | Type       | Description                                                   |
+----------------------------------------------+------------+---------------------------------------------------------------+
| incoming-byte-rate                           | meter      | Bytes/second read off all brokers                             |
| incoming-byte-rate-for-broker-<broker-id>    | meter      | Bytes/second read off a given broker                          |
| outgoing-byte-rate                           | meter      | Bytes/second written off all brokers                          |
| outgoing-byte-rate-for-broker-<broker-id>    | meter      | Bytes/second written off a given broker                       |
| request-rate                                 | meter      | Requests/second sent to all brokers                           |
| request-rate-for-broker-<broker-id>          | meter      | Requests/second sent to a given broker                        |
| request-size                                 | histogram  | Distribution of the request size in bytes for all brokers     |
| request-size-for-broker-<broker-id>          | histogram  | Distribution of the request size in bytes for a given broker  |
| request-latency-in-ms                        | histogram  | Distribution of the request latency in ms for all brokers     |
| request-latency-in-ms-for-broker-<broker-id> | histogram  | Distribution of the request latency in ms for a given broker  |
| response-rate                                | meter      | Responses/second received from all brokers                    |
| response-rate-for-broker-<broker-id>         | meter      | Responses/second received from a given broker                 |
| response-size                                | histogram  | Distribution of the response size in bytes for all brokers    |
| response-size-for-broker-<broker-id>         | histogram  | Distribution of the response size in bytes for a given broker |
| requests-in-flight                           | counter    | The current number of in-flight requests awaiting a response  |
|                                              |            | for all brokers                                               |
| requests-in-flight-for-broker-<broker-id>    | counter    | The current number of in-flight requests awaiting a response  |
|                                              |            | for a given broker                                            |
+----------------------------------------------+------------+---------------------------------------------------------------+
```

Note that we do not gather specific metrics for seed brokers but they are part of the "all brokers" metrics.

Producer related metrics:

```
+-------------------------------------------+------------+--------------------------------------------------------------------------------------+
| Name                                      | Type       | Description                                                                          |
+-------------------------------------------+------------+--------------------------------------------------------------------------------------+
| batch-size                                | histogram  | Distribution of the number of bytes sent per partition per request for all topics    |
| batch-size-for-topic-<topic>              | histogram  | Distribution of the number of bytes sent per partition per request for a given topic |
| record-send-rate                          | meter      | Records/second sent to all topics                                                    |
| record-send-rate-for-topic-<topic>        | meter      | Records/second sent to a given topic                                                 |
| records-per-request                       | histogram  | Distribution of the number of records sent per request for all topics                |
| records-per-request-for-topic-<topic>     | histogram  | Distribution of the number of records sent per request for a given topic             |
| compression-ratio                         | histogram  | Distribution of the compression ratio times 100 of record batches for all topics     |
| compression-ratio-for-topic-<topic>       | histogram  | Distribution of the compression ratio times 100 of record batches for a given topic  |
+-------------------------------------------+------------+--------------------------------------------------------------------------------------+
```

Consumer related metrics:

```
+-------------------------------------------+------------+--------------------------------------------------------------------------------------+
| Name                                      | Type       | Description                                                                          |
+-------------------------------------------+------------+--------------------------------------------------------------------------------------+
| consumer-batch-size                       | histogram  | Distribution of the number of messages in a batch                                    |
| consumer-fetch-rate                       | meter      | Fetch requests/second sent to all brokers                                            |
| consumer-fetch-rate-for-broker-<broker>   | meter      | Fetch requests/second sent to a given broker                                         |
| consumer-fetch-rate-for-topic-<topic>     | meter      | Fetch requests/second sent for a given topic                                         |
| consumer-fetch-response-size              | histogram  | Distribution of the fetch response size in bytes                                     |
| consumer-group-join-total-<GroupID>       | counter    | Total count of consumer group join attempts                                          |
| consumer-group-join-failed-<GroupID>      | counter    | Total count of consumer group join failures                                          |
| consumer-group-sync-total-<GroupID>       | counter    | Total count of consumer group sync attempts                                          |
| consumer-group-sync-failed-<GroupID>      | counter    | Total count of consumer group sync failures                                          |
+-------------------------------------------+------------+--------------------------------------------------------------------------------------+
```

## Index

- [Constants](<#constants>)
- [Variables](<#variables>)
- [type AbortedTransaction](<#type-abortedtransaction>)
- [type AccessToken](<#type-accesstoken>)
- [type AccessTokenProvider](<#type-accesstokenprovider>)
- [type Acl](<#type-acl>)
- [type AclCreation](<#type-aclcreation>)
- [type AclCreationResponse](<#type-aclcreationresponse>)
- [type AclFilter](<#type-aclfilter>)
- [type AclOperation](<#type-acloperation>)
  - [func (a *AclOperation) MarshalText() ([]byte, error)](<#func-acloperation-marshaltext>)
  - [func (a *AclOperation) String() string](<#func-acloperation-string>)
  - [func (a *AclOperation) UnmarshalText(text []byte) error](<#func-acloperation-unmarshaltext>)
- [type AclPermissionType](<#type-aclpermissiontype>)
  - [func (a *AclPermissionType) MarshalText() ([]byte, error)](<#func-aclpermissiontype-marshaltext>)
  - [func (a *AclPermissionType) String() string](<#func-aclpermissiontype-string>)
  - [func (a *AclPermissionType) UnmarshalText(text []byte) error](<#func-aclpermissiontype-unmarshaltext>)
- [type AclResourcePatternType](<#type-aclresourcepatterntype>)
  - [func (a *AclResourcePatternType) MarshalText() ([]byte, error)](<#func-aclresourcepatterntype-marshaltext>)
  - [func (a *AclResourcePatternType) String() string](<#func-aclresourcepatterntype-string>)
  - [func (a *AclResourcePatternType) UnmarshalText(text []byte) error](<#func-aclresourcepatterntype-unmarshaltext>)
- [type AclResourceType](<#type-aclresourcetype>)
  - [func (a *AclResourceType) MarshalText() ([]byte, error)](<#func-aclresourcetype-marshaltext>)
  - [func (a *AclResourceType) String() string](<#func-aclresourcetype-string>)
  - [func (a *AclResourceType) UnmarshalText(text []byte) error](<#func-aclresourcetype-unmarshaltext>)
- [type AddOffsetsToTxnRequest](<#type-addoffsetstotxnrequest>)
- [type AddOffsetsToTxnResponse](<#type-addoffsetstotxnresponse>)
- [type AddPartitionsToTxnRequest](<#type-addpartitionstotxnrequest>)
- [type AddPartitionsToTxnResponse](<#type-addpartitionstotxnresponse>)
- [type AlterClientQuotasEntry](<#type-alterclientquotasentry>)
- [type AlterClientQuotasEntryResponse](<#type-alterclientquotasentryresponse>)
- [type AlterClientQuotasRequest](<#type-alterclientquotasrequest>)
- [type AlterClientQuotasResponse](<#type-alterclientquotasresponse>)
- [type AlterConfigsRequest](<#type-alterconfigsrequest>)
- [type AlterConfigsResource](<#type-alterconfigsresource>)
- [type AlterConfigsResourceResponse](<#type-alterconfigsresourceresponse>)
- [type AlterConfigsResponse](<#type-alterconfigsresponse>)
- [type AlterPartitionReassignmentsRequest](<#type-alterpartitionreassignmentsrequest>)
  - [func (r *AlterPartitionReassignmentsRequest) AddBlock(topic string, partitionID int32, replicas []int32)](<#func-alterpartitionreassignmentsrequest-addblock>)
- [type AlterPartitionReassignmentsResponse](<#type-alterpartitionreassignmentsresponse>)
  - [func (r *AlterPartitionReassignmentsResponse) AddError(topic string, partition int32, kerror KError, message *string)](<#func-alterpartitionreassignmentsresponse-adderror>)
- [type AlterUserScramCredentialsDelete](<#type-alteruserscramcredentialsdelete>)
- [type AlterUserScramCredentialsRequest](<#type-alteruserscramcredentialsrequest>)
- [type AlterUserScramCredentialsResponse](<#type-alteruserscramcredentialsresponse>)
- [type AlterUserScramCredentialsResult](<#type-alteruserscramcredentialsresult>)
- [type AlterUserScramCredentialsUpsert](<#type-alteruserscramcredentialsupsert>)
- [type ApiVersionsRequest](<#type-apiversionsrequest>)
- [type ApiVersionsResponse](<#type-apiversionsresponse>)
- [type ApiVersionsResponseKey](<#type-apiversionsresponsekey>)
- [type AsyncProducer](<#type-asyncproducer>)
  - [func NewAsyncProducer(addrs []string, conf *Config) (AsyncProducer, error)](<#func-newasyncproducer>)
  - [func NewAsyncProducerFromClient(client Client) (AsyncProducer, error)](<#func-newasyncproducerfromclient>)
- [type BalanceStrategy](<#type-balancestrategy>)
- [type BalanceStrategyPlan](<#type-balancestrategyplan>)
  - [func (p BalanceStrategyPlan) Add(memberID, topic string, partitions ...int32)](<#func-balancestrategyplan-add>)
- [type Broker](<#type-broker>)
  - [func NewBroker(addr string) *Broker](<#func-newbroker>)
  - [func (b *Broker) AddOffsetsToTxn(request *AddOffsetsToTxnRequest) (*AddOffsetsToTxnResponse, error)](<#func-broker-addoffsetstotxn>)
  - [func (b *Broker) AddPartitionsToTxn(request *AddPartitionsToTxnRequest) (*AddPartitionsToTxnResponse, error)](<#func-broker-addpartitionstotxn>)
  - [func (b *Broker) Addr() string](<#func-broker-addr>)
  - [func (b *Broker) AlterClientQuotas(request *AlterClientQuotasRequest) (*AlterClientQuotasResponse, error)](<#func-broker-alterclientquotas>)
  - [func (b *Broker) AlterConfigs(request *AlterConfigsRequest) (*AlterConfigsResponse, error)](<#func-broker-alterconfigs>)
  - [func (b *Broker) AlterPartitionReassignments(request *AlterPartitionReassignmentsRequest) (*AlterPartitionReassignmentsResponse, error)](<#func-broker-alterpartitionreassignments>)
  - [func (b *Broker) AlterUserScramCredentials(req *AlterUserScramCredentialsRequest) (*AlterUserScramCredentialsResponse, error)](<#func-broker-alteruserscramcredentials>)
  - [func (b *Broker) ApiVersions(request *ApiVersionsRequest) (*ApiVersionsResponse, error)](<#func-broker-apiversions>)
  - [func (b *Broker) AsyncProduce(request *ProduceRequest, cb ProduceCallback) error](<#func-broker-asyncproduce>)
  - [func (b *Broker) Close() error](<#func-broker-close>)
  - [func (b *Broker) CommitOffset(request *OffsetCommitRequest) (*OffsetCommitResponse, error)](<#func-broker-commitoffset>)
  - [func (b *Broker) Connected() (bool, error)](<#func-broker-connected>)
  - [func (b *Broker) CreateAcls(request *CreateAclsRequest) (*CreateAclsResponse, error)](<#func-broker-createacls>)
  - [func (b *Broker) CreatePartitions(request *CreatePartitionsRequest) (*CreatePartitionsResponse, error)](<#func-broker-createpartitions>)
  - [func (b *Broker) CreateTopics(request *CreateTopicsRequest) (*CreateTopicsResponse, error)](<#func-broker-createtopics>)
  - [func (b *Broker) DeleteAcls(request *DeleteAclsRequest) (*DeleteAclsResponse, error)](<#func-broker-deleteacls>)
  - [func (b *Broker) DeleteGroups(request *DeleteGroupsRequest) (*DeleteGroupsResponse, error)](<#func-broker-deletegroups>)
  - [func (b *Broker) DeleteOffsets(request *DeleteOffsetsRequest) (*DeleteOffsetsResponse, error)](<#func-broker-deleteoffsets>)
  - [func (b *Broker) DeleteRecords(request *DeleteRecordsRequest) (*DeleteRecordsResponse, error)](<#func-broker-deleterecords>)
  - [func (b *Broker) DeleteTopics(request *DeleteTopicsRequest) (*DeleteTopicsResponse, error)](<#func-broker-deletetopics>)
  - [func (b *Broker) DescribeAcls(request *DescribeAclsRequest) (*DescribeAclsResponse, error)](<#func-broker-describeacls>)
  - [func (b *Broker) DescribeClientQuotas(request *DescribeClientQuotasRequest) (*DescribeClientQuotasResponse, error)](<#func-broker-describeclientquotas>)
  - [func (b *Broker) DescribeConfigs(request *DescribeConfigsRequest) (*DescribeConfigsResponse, error)](<#func-broker-describeconfigs>)
  - [func (b *Broker) DescribeGroups(request *DescribeGroupsRequest) (*DescribeGroupsResponse, error)](<#func-broker-describegroups>)
  - [func (b *Broker) DescribeLogDirs(request *DescribeLogDirsRequest) (*DescribeLogDirsResponse, error)](<#func-broker-describelogdirs>)
  - [func (b *Broker) DescribeUserScramCredentials(req *DescribeUserScramCredentialsRequest) (*DescribeUserScramCredentialsResponse, error)](<#func-broker-describeuserscramcredentials>)
  - [func (b *Broker) EndTxn(request *EndTxnRequest) (*EndTxnResponse, error)](<#func-broker-endtxn>)
  - [func (b *Broker) Fetch(request *FetchRequest) (*FetchResponse, error)](<#func-broker-fetch>)
  - [func (b *Broker) FetchOffset(request *OffsetFetchRequest) (*OffsetFetchResponse, error)](<#func-broker-fetchoffset>)
  - [func (b *Broker) FindCoordinator(request *FindCoordinatorRequest) (*FindCoordinatorResponse, error)](<#func-broker-findcoordinator>)
  - [func (b *Broker) GetAvailableOffsets(request *OffsetRequest) (*OffsetResponse, error)](<#func-broker-getavailableoffsets>)
  - [func (b *Broker) GetConsumerMetadata(request *ConsumerMetadataRequest) (*ConsumerMetadataResponse, error)](<#func-broker-getconsumermetadata>)
  - [func (b *Broker) GetMetadata(request *MetadataRequest) (*MetadataResponse, error)](<#func-broker-getmetadata>)
  - [func (b *Broker) Heartbeat(request *HeartbeatRequest) (*HeartbeatResponse, error)](<#func-broker-heartbeat>)
  - [func (b *Broker) ID() int32](<#func-broker-id>)
  - [func (b *Broker) IncrementalAlterConfigs(request *IncrementalAlterConfigsRequest) (*IncrementalAlterConfigsResponse, error)](<#func-broker-incrementalalterconfigs>)
  - [func (b *Broker) InitProducerID(request *InitProducerIDRequest) (*InitProducerIDResponse, error)](<#func-broker-initproducerid>)
  - [func (b *Broker) JoinGroup(request *JoinGroupRequest) (*JoinGroupResponse, error)](<#func-broker-joingroup>)
  - [func (b *Broker) LeaveGroup(request *LeaveGroupRequest) (*LeaveGroupResponse, error)](<#func-broker-leavegroup>)
  - [func (b *Broker) ListGroups(request *ListGroupsRequest) (*ListGroupsResponse, error)](<#func-broker-listgroups>)
  - [func (b *Broker) ListPartitionReassignments(request *ListPartitionReassignmentsRequest) (*ListPartitionReassignmentsResponse, error)](<#func-broker-listpartitionreassignments>)
  - [func (b *Broker) Open(conf *Config) error](<#func-broker-open>)
  - [func (b *Broker) Produce(request *ProduceRequest) (*ProduceResponse, error)](<#func-broker-produce>)
  - [func (b *Broker) Rack() string](<#func-broker-rack>)
  - [func (b *Broker) ResponseSize() int](<#func-broker-responsesize>)
  - [func (b *Broker) SyncGroup(request *SyncGroupRequest) (*SyncGroupResponse, error)](<#func-broker-syncgroup>)
  - [func (b *Broker) TLSConnectionState() (state tls.ConnectionState, ok bool)](<#func-broker-tlsconnectionstate>)
  - [func (b *Broker) TxnOffsetCommit(request *TxnOffsetCommitRequest) (*TxnOffsetCommitResponse, error)](<#func-broker-txnoffsetcommit>)
- [type ByteEncoder](<#type-byteencoder>)
  - [func (b ByteEncoder) Encode() ([]byte, error)](<#func-byteencoder-encode>)
  - [func (b ByteEncoder) Length() int](<#func-byteencoder-length>)
- [type Client](<#type-client>)
  - [func NewClient(addrs []string, conf *Config) (Client, error)](<#func-newclient>)
- [type ClientQuotasOp](<#type-clientquotasop>)
- [type ClusterAdmin](<#type-clusteradmin>)
  - [func NewClusterAdmin(addrs []string, conf *Config) (ClusterAdmin, error)](<#func-newclusteradmin>)
  - [func NewClusterAdminFromClient(client Client) (ClusterAdmin, error)](<#func-newclusteradminfromclient>)
- [type CompressionCodec](<#type-compressioncodec>)
  - [func (cc CompressionCodec) MarshalText() ([]byte, error)](<#func-compressioncodec-marshaltext>)
  - [func (cc CompressionCodec) String() string](<#func-compressioncodec-string>)
  - [func (cc *CompressionCodec) UnmarshalText(text []byte) error](<#func-compressioncodec-unmarshaltext>)
- [type Config](<#type-config>)
  - [func NewConfig() *Config](<#func-newconfig>)
  - [func (c *Config) Validate() error](<#func-config-validate>)
- [type ConfigEntry](<#type-configentry>)
- [type ConfigResource](<#type-configresource>)
- [type ConfigResourceType](<#type-configresourcetype>)
- [type ConfigSource](<#type-configsource>)
  - [func (s ConfigSource) String() string](<#func-configsource-string>)
- [type ConfigSynonym](<#type-configsynonym>)
- [type ConfigurationError](<#type-configurationerror>)
  - [func (err ConfigurationError) Error() string](<#func-configurationerror-error>)
- [type Consumer](<#type-consumer>)
  - [func NewConsumer(addrs []string, config *Config) (Consumer, error)](<#func-newconsumer>)
  - [func NewConsumerFromClient(client Client) (Consumer, error)](<#func-newconsumerfromclient>)
- [type ConsumerError](<#type-consumererror>)
  - [func (ce ConsumerError) Error() string](<#func-consumererror-error>)
  - [func (ce ConsumerError) Unwrap() error](<#func-consumererror-unwrap>)
- [type ConsumerErrors](<#type-consumererrors>)
  - [func (ce ConsumerErrors) Error() string](<#func-consumererrors-error>)
- [type ConsumerGroup](<#type-consumergroup>)
  - [func NewConsumerGroup(addrs []string, groupID string, config *Config) (ConsumerGroup, error)](<#func-newconsumergroup>)
  - [func NewConsumerGroupFromClient(groupID string, client Client) (ConsumerGroup, error)](<#func-newconsumergroupfromclient>)
- [type ConsumerGroupClaim](<#type-consumergroupclaim>)
- [type ConsumerGroupHandler](<#type-consumergrouphandler>)
- [type ConsumerGroupMemberAssignment](<#type-consumergroupmemberassignment>)
- [type ConsumerGroupMemberMetadata](<#type-consumergroupmembermetadata>)
- [type ConsumerGroupSession](<#type-consumergroupsession>)
- [type ConsumerInterceptor](<#type-consumerinterceptor>)
- [type ConsumerMessage](<#type-consumermessage>)
- [type ConsumerMetadataRequest](<#type-consumermetadatarequest>)
- [type ConsumerMetadataResponse](<#type-consumermetadataresponse>)
- [type ControlRecord](<#type-controlrecord>)
- [type ControlRecordType](<#type-controlrecordtype>)
- [type CoordinatorType](<#type-coordinatortype>)
- [type CreateAclsRequest](<#type-createaclsrequest>)
- [type CreateAclsResponse](<#type-createaclsresponse>)
- [type CreatePartitionsRequest](<#type-createpartitionsrequest>)
- [type CreatePartitionsResponse](<#type-createpartitionsresponse>)
- [type CreateTopicsRequest](<#type-createtopicsrequest>)
- [type CreateTopicsResponse](<#type-createtopicsresponse>)
- [type DeleteAclsRequest](<#type-deleteaclsrequest>)
- [type DeleteAclsResponse](<#type-deleteaclsresponse>)
- [type DeleteGroupsRequest](<#type-deletegroupsrequest>)
  - [func (r *DeleteGroupsRequest) AddGroup(group string)](<#func-deletegroupsrequest-addgroup>)
- [type DeleteGroupsResponse](<#type-deletegroupsresponse>)
- [type DeleteOffsetsRequest](<#type-deleteoffsetsrequest>)
  - [func (r *DeleteOffsetsRequest) AddPartition(topic string, partitionID int32)](<#func-deleteoffsetsrequest-addpartition>)
- [type DeleteOffsetsResponse](<#type-deleteoffsetsresponse>)
  - [func (r *DeleteOffsetsResponse) AddError(topic string, partition int32, errorCode KError)](<#func-deleteoffsetsresponse-adderror>)
- [type DeleteRecordsRequest](<#type-deleterecordsrequest>)
- [type DeleteRecordsRequestTopic](<#type-deleterecordsrequesttopic>)
- [type DeleteRecordsResponse](<#type-deleterecordsresponse>)
- [type DeleteRecordsResponsePartition](<#type-deleterecordsresponsepartition>)
- [type DeleteRecordsResponseTopic](<#type-deleterecordsresponsetopic>)
- [type DeleteTopicsRequest](<#type-deletetopicsrequest>)
- [type DeleteTopicsResponse](<#type-deletetopicsresponse>)
- [type DescribeAclsRequest](<#type-describeaclsrequest>)
- [type DescribeAclsResponse](<#type-describeaclsresponse>)
- [type DescribeClientQuotasEntry](<#type-describeclientquotasentry>)
- [type DescribeClientQuotasRequest](<#type-describeclientquotasrequest>)
- [type DescribeClientQuotasResponse](<#type-describeclientquotasresponse>)
- [type DescribeConfigsRequest](<#type-describeconfigsrequest>)
- [type DescribeConfigsResponse](<#type-describeconfigsresponse>)
- [type DescribeGroupsRequest](<#type-describegroupsrequest>)
  - [func (r *DescribeGroupsRequest) AddGroup(group string)](<#func-describegroupsrequest-addgroup>)
- [type DescribeGroupsResponse](<#type-describegroupsresponse>)
- [type DescribeLogDirsRequest](<#type-describelogdirsrequest>)
- [type DescribeLogDirsRequestTopic](<#type-describelogdirsrequesttopic>)
- [type DescribeLogDirsResponse](<#type-describelogdirsresponse>)
- [type DescribeLogDirsResponseDirMetadata](<#type-describelogdirsresponsedirmetadata>)
- [type DescribeLogDirsResponsePartition](<#type-describelogdirsresponsepartition>)
- [type DescribeLogDirsResponseTopic](<#type-describelogdirsresponsetopic>)
- [type DescribeUserScramCredentialsRequest](<#type-describeuserscramcredentialsrequest>)
- [type DescribeUserScramCredentialsRequestUser](<#type-describeuserscramcredentialsrequestuser>)
- [type DescribeUserScramCredentialsResponse](<#type-describeuserscramcredentialsresponse>)
- [type DescribeUserScramCredentialsResult](<#type-describeuserscramcredentialsresult>)
- [type DynamicConsistencyPartitioner](<#type-dynamicconsistencypartitioner>)
- [type Encoder](<#type-encoder>)
- [type EndTxnRequest](<#type-endtxnrequest>)
- [type EndTxnResponse](<#type-endtxnresponse>)
- [type FetchRequest](<#type-fetchrequest>)
  - [func (r *FetchRequest) AddBlock(topic string, partitionID int32, fetchOffset int64, maxBytes int32)](<#func-fetchrequest-addblock>)
- [type FetchResponse](<#type-fetchresponse>)
  - [func (r *FetchResponse) AddControlRecord(topic string, partition int32, offset int64, producerID int64, recordType ControlRecordType)](<#func-fetchresponse-addcontrolrecord>)
  - [func (r *FetchResponse) AddControlRecordWithTimestamp(topic string, partition int32, offset int64, producerID int64, recordType ControlRecordType, timestamp time.Time)](<#func-fetchresponse-addcontrolrecordwithtimestamp>)
  - [func (r *FetchResponse) AddError(topic string, partition int32, err KError)](<#func-fetchresponse-adderror>)
  - [func (r *FetchResponse) AddMessage(topic string, partition int32, key, value Encoder, offset int64)](<#func-fetchresponse-addmessage>)
  - [func (r *FetchResponse) AddMessageWithTimestamp(topic string, partition int32, key, value Encoder, offset int64, timestamp time.Time, version int8)](<#func-fetchresponse-addmessagewithtimestamp>)
  - [func (r *FetchResponse) AddRecord(topic string, partition int32, key, value Encoder, offset int64)](<#func-fetchresponse-addrecord>)
  - [func (r *FetchResponse) AddRecordBatch(topic string, partition int32, key, value Encoder, offset int64, producerID int64, isTransactional bool)](<#func-fetchresponse-addrecordbatch>)
  - [func (r *FetchResponse) AddRecordBatchWithTimestamp(topic string, partition int32, key, value Encoder, offset int64, producerID int64, isTransactional bool, timestamp time.Time)](<#func-fetchresponse-addrecordbatchwithtimestamp>)
  - [func (r *FetchResponse) AddRecordWithTimestamp(topic string, partition int32, key, value Encoder, offset int64, timestamp time.Time)](<#func-fetchresponse-addrecordwithtimestamp>)
  - [func (r *FetchResponse) GetBlock(topic string, partition int32) *FetchResponseBlock](<#func-fetchresponse-getblock>)
  - [func (r *FetchResponse) SetLastOffsetDelta(topic string, partition int32, offset int32)](<#func-fetchresponse-setlastoffsetdelta>)
  - [func (r *FetchResponse) SetLastStableOffset(topic string, partition int32, offset int64)](<#func-fetchresponse-setlaststableoffset>)
- [type FetchResponseBlock](<#type-fetchresponseblock>)
- [type FilterResponse](<#type-filterresponse>)
- [type FindCoordinatorRequest](<#type-findcoordinatorrequest>)
- [type FindCoordinatorResponse](<#type-findcoordinatorresponse>)
- [type GSSAPIConfig](<#type-gssapiconfig>)
- [type GSSAPIKerberosAuth](<#type-gssapikerberosauth>)
  - [func (krbAuth *GSSAPIKerberosAuth) Authorize(broker *Broker) error](<#func-gssapikerberosauth-authorize>)
- [type GSSApiHandlerFunc](<#type-gssapihandlerfunc>)
- [type GroupDescription](<#type-groupdescription>)
- [type GroupMember](<#type-groupmember>)
- [type GroupMemberDescription](<#type-groupmemberdescription>)
  - [func (gmd *GroupMemberDescription) GetMemberAssignment() (*ConsumerGroupMemberAssignment, error)](<#func-groupmemberdescription-getmemberassignment>)
  - [func (gmd *GroupMemberDescription) GetMemberMetadata() (*ConsumerGroupMemberMetadata, error)](<#func-groupmemberdescription-getmembermetadata>)
- [type GroupProtocol](<#type-groupprotocol>)
- [type HashPartitionerOption](<#type-hashpartitioneroption>)
  - [func WithAbsFirst() HashPartitionerOption](<#func-withabsfirst>)
  - [func WithCustomFallbackPartitioner(randomHP Partitioner) HashPartitionerOption](<#func-withcustomfallbackpartitioner>)
  - [func WithCustomHashFunction(hasher func() hash.Hash32) HashPartitionerOption](<#func-withcustomhashfunction>)
- [type HeartbeatRequest](<#type-heartbeatrequest>)
- [type HeartbeatResponse](<#type-heartbeatresponse>)
- [type IncrementalAlterConfigsEntry](<#type-incrementalalterconfigsentry>)
- [type IncrementalAlterConfigsOperation](<#type-incrementalalterconfigsoperation>)
- [type IncrementalAlterConfigsRequest](<#type-incrementalalterconfigsrequest>)
- [type IncrementalAlterConfigsResource](<#type-incrementalalterconfigsresource>)
- [type IncrementalAlterConfigsResponse](<#type-incrementalalterconfigsresponse>)
- [type InitProducerIDRequest](<#type-initproduceridrequest>)
- [type InitProducerIDResponse](<#type-initproduceridresponse>)
- [type IsolationLevel](<#type-isolationlevel>)
- [type JoinGroupRequest](<#type-joingrouprequest>)
  - [func (r *JoinGroupRequest) AddGroupProtocol(name string, metadata []byte)](<#func-joingrouprequest-addgroupprotocol>)
  - [func (r *JoinGroupRequest) AddGroupProtocolMetadata(name string, metadata *ConsumerGroupMemberMetadata) error](<#func-joingrouprequest-addgroupprotocolmetadata>)
- [type JoinGroupResponse](<#type-joingroupresponse>)
  - [func (r *JoinGroupResponse) GetMembers() (map[string]ConsumerGroupMemberMetadata, error)](<#func-joingroupresponse-getmembers>)
- [type KError](<#type-kerror>)
  - [func (err KError) Error() string](<#func-kerror-error>)
- [type KafkaGSSAPIHandler](<#type-kafkagssapihandler>)
  - [func (h *KafkaGSSAPIHandler) MockKafkaGSSAPI(buffer []byte) []byte](<#func-kafkagssapihandler-mockkafkagssapi>)
- [type KafkaVersion](<#type-kafkaversion>)
  - [func ParseKafkaVersion(s string) (KafkaVersion, error)](<#func-parsekafkaversion>)
  - [func (v KafkaVersion) IsAtLeast(other KafkaVersion) bool](<#func-kafkaversion-isatleast>)
  - [func (v KafkaVersion) String() string](<#func-kafkaversion-string>)
- [type KerberosClient](<#type-kerberosclient>)
  - [func NewKerberosClient(config *GSSAPIConfig) (KerberosClient, error)](<#func-newkerberosclient>)
- [type KerberosGoKrb5Client](<#type-kerberosgokrb5client>)
  - [func (c *KerberosGoKrb5Client) CName() types.PrincipalName](<#func-kerberosgokrb5client-cname>)
  - [func (c *KerberosGoKrb5Client) Domain() string](<#func-kerberosgokrb5client-domain>)
- [type LeaveGroupRequest](<#type-leavegrouprequest>)
- [type LeaveGroupResponse](<#type-leavegroupresponse>)
- [type ListGroupsRequest](<#type-listgroupsrequest>)
- [type ListGroupsResponse](<#type-listgroupsresponse>)
- [type ListPartitionReassignmentsRequest](<#type-listpartitionreassignmentsrequest>)
  - [func (r *ListPartitionReassignmentsRequest) AddBlock(topic string, partitionIDs []int32)](<#func-listpartitionreassignmentsrequest-addblock>)
- [type ListPartitionReassignmentsResponse](<#type-listpartitionreassignmentsresponse>)
  - [func (r *ListPartitionReassignmentsResponse) AddBlock(topic string, partition int32, replicas, addingReplicas, removingReplicas []int32)](<#func-listpartitionreassignmentsresponse-addblock>)
- [type MatchingAcl](<#type-matchingacl>)
- [type MemberIdentity](<#type-memberidentity>)
- [type MemberResponse](<#type-memberresponse>)
- [type Message](<#type-message>)
- [type MessageBlock](<#type-messageblock>)
  - [func (msb *MessageBlock) Messages() []*MessageBlock](<#func-messageblock-messages>)
- [type MessageSet](<#type-messageset>)
- [type MetadataRequest](<#type-metadatarequest>)
- [type MetadataResponse](<#type-metadataresponse>)
  - [func (r *MetadataResponse) AddBroker(addr string, id int32)](<#func-metadataresponse-addbroker>)
  - [func (r *MetadataResponse) AddTopic(topic string, err KError) *TopicMetadata](<#func-metadataresponse-addtopic>)
  - [func (r *MetadataResponse) AddTopicPartition(topic string, partition, brokerID int32, replicas, isr []int32, offline []int32, err KError)](<#func-metadataresponse-addtopicpartition>)
- [type MockAlterConfigsResponse](<#type-mockalterconfigsresponse>)
  - [func NewMockAlterConfigsResponse(t TestReporter) *MockAlterConfigsResponse](<#func-newmockalterconfigsresponse>)
  - [func (mr *MockAlterConfigsResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mockalterconfigsresponse-for>)
- [type MockAlterConfigsResponseWithErrorCode](<#type-mockalterconfigsresponsewitherrorcode>)
  - [func NewMockAlterConfigsResponseWithErrorCode(t TestReporter) *MockAlterConfigsResponseWithErrorCode](<#func-newmockalterconfigsresponsewitherrorcode>)
  - [func (mr *MockAlterConfigsResponseWithErrorCode) For(reqBody versionedDecoder) encoderWithHeader](<#func-mockalterconfigsresponsewitherrorcode-for>)
- [type MockAlterPartitionReassignmentsResponse](<#type-mockalterpartitionreassignmentsresponse>)
  - [func NewMockAlterPartitionReassignmentsResponse(t TestReporter) *MockAlterPartitionReassignmentsResponse](<#func-newmockalterpartitionreassignmentsresponse>)
  - [func (mr *MockAlterPartitionReassignmentsResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mockalterpartitionreassignmentsresponse-for>)
- [type MockApiVersionsResponse](<#type-mockapiversionsresponse>)
  - [func NewMockApiVersionsResponse(t TestReporter) *MockApiVersionsResponse](<#func-newmockapiversionsresponse>)
  - [func (m *MockApiVersionsResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mockapiversionsresponse-for>)
  - [func (m *MockApiVersionsResponse) SetApiKeys(apiKeys []ApiVersionsResponseKey) *MockApiVersionsResponse](<#func-mockapiversionsresponse-setapikeys>)
- [type MockBroker](<#type-mockbroker>)
  - [func NewMockBroker(t TestReporter, brokerID int32) *MockBroker](<#func-newmockbroker>)
  - [func NewMockBrokerAddr(t TestReporter, brokerID int32, addr string) *MockBroker](<#func-newmockbrokeraddr>)
  - [func NewMockBrokerListener(t TestReporter, brokerID int32, listener net.Listener) *MockBroker](<#func-newmockbrokerlistener>)
  - [func (b *MockBroker) Addr() string](<#func-mockbroker-addr>)
  - [func (b *MockBroker) BrokerID() int32](<#func-mockbroker-brokerid>)
  - [func (b *MockBroker) Close()](<#func-mockbroker-close>)
  - [func (b *MockBroker) History() []RequestResponse](<#func-mockbroker-history>)
  - [func (b *MockBroker) Port() int32](<#func-mockbroker-port>)
  - [func (b *MockBroker) Returns(e encoderWithHeader)](<#func-mockbroker-returns>)
  - [func (b *MockBroker) SetGSSAPIHandler(handler GSSApiHandlerFunc)](<#func-mockbroker-setgssapihandler>)
  - [func (b *MockBroker) SetHandlerByMap(handlerMap map[string]MockResponse)](<#func-mockbroker-sethandlerbymap>)
  - [func (b *MockBroker) SetLatency(latency time.Duration)](<#func-mockbroker-setlatency>)
  - [func (b *MockBroker) SetNotifier(notifier RequestNotifierFunc)](<#func-mockbroker-setnotifier>)
- [type MockConsumerMetadataResponse](<#type-mockconsumermetadataresponse>)
  - [func NewMockConsumerMetadataResponse(t TestReporter) *MockConsumerMetadataResponse](<#func-newmockconsumermetadataresponse>)
  - [func (mr *MockConsumerMetadataResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mockconsumermetadataresponse-for>)
  - [func (mr *MockConsumerMetadataResponse) SetCoordinator(group string, broker *MockBroker) *MockConsumerMetadataResponse](<#func-mockconsumermetadataresponse-setcoordinator>)
  - [func (mr *MockConsumerMetadataResponse) SetError(group string, kerror KError) *MockConsumerMetadataResponse](<#func-mockconsumermetadataresponse-seterror>)
- [type MockCreateAclsResponse](<#type-mockcreateaclsresponse>)
  - [func NewMockCreateAclsResponse(t TestReporter) *MockCreateAclsResponse](<#func-newmockcreateaclsresponse>)
  - [func (mr *MockCreateAclsResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mockcreateaclsresponse-for>)
- [type MockCreateAclsResponseError](<#type-mockcreateaclsresponseerror>)
  - [func NewMockCreateAclsResponseWithError(t TestReporter) *MockCreateAclsResponseError](<#func-newmockcreateaclsresponsewitherror>)
  - [func (mr *MockCreateAclsResponseError) For(reqBody versionedDecoder) encoderWithHeader](<#func-mockcreateaclsresponseerror-for>)
- [type MockCreatePartitionsResponse](<#type-mockcreatepartitionsresponse>)
  - [func NewMockCreatePartitionsResponse(t TestReporter) *MockCreatePartitionsResponse](<#func-newmockcreatepartitionsresponse>)
  - [func (mr *MockCreatePartitionsResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mockcreatepartitionsresponse-for>)
- [type MockCreateTopicsResponse](<#type-mockcreatetopicsresponse>)
  - [func NewMockCreateTopicsResponse(t TestReporter) *MockCreateTopicsResponse](<#func-newmockcreatetopicsresponse>)
  - [func (mr *MockCreateTopicsResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mockcreatetopicsresponse-for>)
- [type MockDeleteAclsResponse](<#type-mockdeleteaclsresponse>)
  - [func NewMockDeleteAclsResponse(t TestReporter) *MockDeleteAclsResponse](<#func-newmockdeleteaclsresponse>)
  - [func (mr *MockDeleteAclsResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mockdeleteaclsresponse-for>)
- [type MockDeleteGroupsResponse](<#type-mockdeletegroupsresponse>)
  - [func NewMockDeleteGroupsRequest(t TestReporter) *MockDeleteGroupsResponse](<#func-newmockdeletegroupsrequest>)
  - [func (m *MockDeleteGroupsResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mockdeletegroupsresponse-for>)
  - [func (m *MockDeleteGroupsResponse) SetDeletedGroups(groups []string) *MockDeleteGroupsResponse](<#func-mockdeletegroupsresponse-setdeletedgroups>)
- [type MockDeleteOffsetResponse](<#type-mockdeleteoffsetresponse>)
  - [func NewMockDeleteOffsetRequest(t TestReporter) *MockDeleteOffsetResponse](<#func-newmockdeleteoffsetrequest>)
  - [func (m *MockDeleteOffsetResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mockdeleteoffsetresponse-for>)
  - [func (m *MockDeleteOffsetResponse) SetDeletedOffset(errorCode KError, topic string, partition int32, errorPartition KError) *MockDeleteOffsetResponse](<#func-mockdeleteoffsetresponse-setdeletedoffset>)
- [type MockDeleteRecordsResponse](<#type-mockdeleterecordsresponse>)
  - [func NewMockDeleteRecordsResponse(t TestReporter) *MockDeleteRecordsResponse](<#func-newmockdeleterecordsresponse>)
  - [func (mr *MockDeleteRecordsResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mockdeleterecordsresponse-for>)
- [type MockDeleteTopicsResponse](<#type-mockdeletetopicsresponse>)
  - [func NewMockDeleteTopicsResponse(t TestReporter) *MockDeleteTopicsResponse](<#func-newmockdeletetopicsresponse>)
  - [func (mr *MockDeleteTopicsResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mockdeletetopicsresponse-for>)
- [type MockDescribeConfigsResponse](<#type-mockdescribeconfigsresponse>)
  - [func NewMockDescribeConfigsResponse(t TestReporter) *MockDescribeConfigsResponse](<#func-newmockdescribeconfigsresponse>)
  - [func (mr *MockDescribeConfigsResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mockdescribeconfigsresponse-for>)
- [type MockDescribeConfigsResponseWithErrorCode](<#type-mockdescribeconfigsresponsewitherrorcode>)
  - [func NewMockDescribeConfigsResponseWithErrorCode(t TestReporter) *MockDescribeConfigsResponseWithErrorCode](<#func-newmockdescribeconfigsresponsewitherrorcode>)
  - [func (mr *MockDescribeConfigsResponseWithErrorCode) For(reqBody versionedDecoder) encoderWithHeader](<#func-mockdescribeconfigsresponsewitherrorcode-for>)
- [type MockDescribeGroupsResponse](<#type-mockdescribegroupsresponse>)
  - [func NewMockDescribeGroupsResponse(t TestReporter) *MockDescribeGroupsResponse](<#func-newmockdescribegroupsresponse>)
  - [func (m *MockDescribeGroupsResponse) AddGroupDescription(groupID string, description *GroupDescription) *MockDescribeGroupsResponse](<#func-mockdescribegroupsresponse-addgroupdescription>)
  - [func (m *MockDescribeGroupsResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mockdescribegroupsresponse-for>)
- [type MockDescribeLogDirsResponse](<#type-mockdescribelogdirsresponse>)
  - [func NewMockDescribeLogDirsResponse(t TestReporter) *MockDescribeLogDirsResponse](<#func-newmockdescribelogdirsresponse>)
  - [func (m *MockDescribeLogDirsResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mockdescribelogdirsresponse-for>)
  - [func (m *MockDescribeLogDirsResponse) SetLogDirs(logDirPath string, topicPartitions map[string]int) *MockDescribeLogDirsResponse](<#func-mockdescribelogdirsresponse-setlogdirs>)
- [type MockFetchResponse](<#type-mockfetchresponse>)
  - [func NewMockFetchResponse(t TestReporter, batchSize int) *MockFetchResponse](<#func-newmockfetchresponse>)
  - [func (mfr *MockFetchResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mockfetchresponse-for>)
  - [func (mfr *MockFetchResponse) SetHighWaterMark(topic string, partition int32, offset int64) *MockFetchResponse](<#func-mockfetchresponse-sethighwatermark>)
  - [func (mfr *MockFetchResponse) SetMessage(topic string, partition int32, offset int64, msg Encoder) *MockFetchResponse](<#func-mockfetchresponse-setmessage>)
  - [func (mfr *MockFetchResponse) SetMessageWithKey(topic string, partition int32, offset int64, key, msg Encoder) *MockFetchResponse](<#func-mockfetchresponse-setmessagewithkey>)
- [type MockFindCoordinatorResponse](<#type-mockfindcoordinatorresponse>)
  - [func NewMockFindCoordinatorResponse(t TestReporter) *MockFindCoordinatorResponse](<#func-newmockfindcoordinatorresponse>)
  - [func (mr *MockFindCoordinatorResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mockfindcoordinatorresponse-for>)
  - [func (mr *MockFindCoordinatorResponse) SetCoordinator(coordinatorType CoordinatorType, group string, broker *MockBroker) *MockFindCoordinatorResponse](<#func-mockfindcoordinatorresponse-setcoordinator>)
  - [func (mr *MockFindCoordinatorResponse) SetError(coordinatorType CoordinatorType, group string, kerror KError) *MockFindCoordinatorResponse](<#func-mockfindcoordinatorresponse-seterror>)
- [type MockHeartbeatResponse](<#type-mockheartbeatresponse>)
  - [func NewMockHeartbeatResponse(t TestReporter) *MockHeartbeatResponse](<#func-newmockheartbeatresponse>)
  - [func (m *MockHeartbeatResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mockheartbeatresponse-for>)
  - [func (m *MockHeartbeatResponse) SetError(kerr KError) *MockHeartbeatResponse](<#func-mockheartbeatresponse-seterror>)
- [type MockIncrementalAlterConfigsResponse](<#type-mockincrementalalterconfigsresponse>)
  - [func NewMockIncrementalAlterConfigsResponse(t TestReporter) *MockIncrementalAlterConfigsResponse](<#func-newmockincrementalalterconfigsresponse>)
  - [func (mr *MockIncrementalAlterConfigsResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mockincrementalalterconfigsresponse-for>)
- [type MockIncrementalAlterConfigsResponseWithErrorCode](<#type-mockincrementalalterconfigsresponsewitherrorcode>)
  - [func NewMockIncrementalAlterConfigsResponseWithErrorCode(t TestReporter) *MockIncrementalAlterConfigsResponseWithErrorCode](<#func-newmockincrementalalterconfigsresponsewitherrorcode>)
  - [func (mr *MockIncrementalAlterConfigsResponseWithErrorCode) For(reqBody versionedDecoder) encoderWithHeader](<#func-mockincrementalalterconfigsresponsewitherrorcode-for>)
- [type MockJoinGroupResponse](<#type-mockjoingroupresponse>)
  - [func NewMockJoinGroupResponse(t TestReporter) *MockJoinGroupResponse](<#func-newmockjoingroupresponse>)
  - [func (m *MockJoinGroupResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mockjoingroupresponse-for>)
  - [func (m *MockJoinGroupResponse) SetError(kerr KError) *MockJoinGroupResponse](<#func-mockjoingroupresponse-seterror>)
  - [func (m *MockJoinGroupResponse) SetGenerationId(id int32) *MockJoinGroupResponse](<#func-mockjoingroupresponse-setgenerationid>)
  - [func (m *MockJoinGroupResponse) SetGroupProtocol(proto string) *MockJoinGroupResponse](<#func-mockjoingroupresponse-setgroupprotocol>)
  - [func (m *MockJoinGroupResponse) SetLeaderId(id string) *MockJoinGroupResponse](<#func-mockjoingroupresponse-setleaderid>)
  - [func (m *MockJoinGroupResponse) SetMember(id string, meta *ConsumerGroupMemberMetadata) *MockJoinGroupResponse](<#func-mockjoingroupresponse-setmember>)
  - [func (m *MockJoinGroupResponse) SetMemberId(id string) *MockJoinGroupResponse](<#func-mockjoingroupresponse-setmemberid>)
  - [func (m *MockJoinGroupResponse) SetThrottleTime(t int32) *MockJoinGroupResponse](<#func-mockjoingroupresponse-setthrottletime>)
- [type MockKerberosClient](<#type-mockkerberosclient>)
  - [func (c *MockKerberosClient) CName() types.PrincipalName](<#func-mockkerberosclient-cname>)
  - [func (c *MockKerberosClient) Destroy()](<#func-mockkerberosclient-destroy>)
  - [func (c *MockKerberosClient) Domain() string](<#func-mockkerberosclient-domain>)
  - [func (c *MockKerberosClient) GetServiceTicket(spn string) (messages.Ticket, types.EncryptionKey, error)](<#func-mockkerberosclient-getserviceticket>)
  - [func (c *MockKerberosClient) Login() error](<#func-mockkerberosclient-login>)
- [type MockLeaveGroupResponse](<#type-mockleavegroupresponse>)
  - [func NewMockLeaveGroupResponse(t TestReporter) *MockLeaveGroupResponse](<#func-newmockleavegroupresponse>)
  - [func (m *MockLeaveGroupResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mockleavegroupresponse-for>)
  - [func (m *MockLeaveGroupResponse) SetError(kerr KError) *MockLeaveGroupResponse](<#func-mockleavegroupresponse-seterror>)
- [type MockListAclsResponse](<#type-mocklistaclsresponse>)
  - [func NewMockListAclsResponse(t TestReporter) *MockListAclsResponse](<#func-newmocklistaclsresponse>)
  - [func (mr *MockListAclsResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mocklistaclsresponse-for>)
- [type MockListGroupsResponse](<#type-mocklistgroupsresponse>)
  - [func NewMockListGroupsResponse(t TestReporter) *MockListGroupsResponse](<#func-newmocklistgroupsresponse>)
  - [func (m *MockListGroupsResponse) AddGroup(groupID, protocolType string) *MockListGroupsResponse](<#func-mocklistgroupsresponse-addgroup>)
  - [func (m *MockListGroupsResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mocklistgroupsresponse-for>)
- [type MockListPartitionReassignmentsResponse](<#type-mocklistpartitionreassignmentsresponse>)
  - [func NewMockListPartitionReassignmentsResponse(t TestReporter) *MockListPartitionReassignmentsResponse](<#func-newmocklistpartitionreassignmentsresponse>)
  - [func (mr *MockListPartitionReassignmentsResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mocklistpartitionreassignmentsresponse-for>)
- [type MockMetadataResponse](<#type-mockmetadataresponse>)
  - [func NewMockMetadataResponse(t TestReporter) *MockMetadataResponse](<#func-newmockmetadataresponse>)
  - [func (mmr *MockMetadataResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mockmetadataresponse-for>)
  - [func (mmr *MockMetadataResponse) SetBroker(addr string, brokerID int32) *MockMetadataResponse](<#func-mockmetadataresponse-setbroker>)
  - [func (mmr *MockMetadataResponse) SetController(brokerID int32) *MockMetadataResponse](<#func-mockmetadataresponse-setcontroller>)
  - [func (mmr *MockMetadataResponse) SetLeader(topic string, partition, brokerID int32) *MockMetadataResponse](<#func-mockmetadataresponse-setleader>)
- [type MockOffsetCommitResponse](<#type-mockoffsetcommitresponse>)
  - [func NewMockOffsetCommitResponse(t TestReporter) *MockOffsetCommitResponse](<#func-newmockoffsetcommitresponse>)
  - [func (mr *MockOffsetCommitResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mockoffsetcommitresponse-for>)
  - [func (mr *MockOffsetCommitResponse) SetError(group, topic string, partition int32, kerror KError) *MockOffsetCommitResponse](<#func-mockoffsetcommitresponse-seterror>)
- [type MockOffsetFetchResponse](<#type-mockoffsetfetchresponse>)
  - [func NewMockOffsetFetchResponse(t TestReporter) *MockOffsetFetchResponse](<#func-newmockoffsetfetchresponse>)
  - [func (mr *MockOffsetFetchResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mockoffsetfetchresponse-for>)
  - [func (mr *MockOffsetFetchResponse) SetError(kerror KError) *MockOffsetFetchResponse](<#func-mockoffsetfetchresponse-seterror>)
  - [func (mr *MockOffsetFetchResponse) SetOffset(group, topic string, partition int32, offset int64, metadata string, kerror KError) *MockOffsetFetchResponse](<#func-mockoffsetfetchresponse-setoffset>)
- [type MockOffsetResponse](<#type-mockoffsetresponse>)
  - [func NewMockOffsetResponse(t TestReporter) *MockOffsetResponse](<#func-newmockoffsetresponse>)
  - [func (mor *MockOffsetResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mockoffsetresponse-for>)
  - [func (mor *MockOffsetResponse) SetOffset(topic string, partition int32, time, offset int64) *MockOffsetResponse](<#func-mockoffsetresponse-setoffset>)
- [type MockProduceResponse](<#type-mockproduceresponse>)
  - [func NewMockProduceResponse(t TestReporter) *MockProduceResponse](<#func-newmockproduceresponse>)
  - [func (mr *MockProduceResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mockproduceresponse-for>)
  - [func (mr *MockProduceResponse) SetError(topic string, partition int32, kerror KError) *MockProduceResponse](<#func-mockproduceresponse-seterror>)
  - [func (mr *MockProduceResponse) SetVersion(version int16) *MockProduceResponse](<#func-mockproduceresponse-setversion>)
- [type MockResponse](<#type-mockresponse>)
- [type MockSaslAuthenticateResponse](<#type-mocksaslauthenticateresponse>)
  - [func NewMockSaslAuthenticateResponse(t TestReporter) *MockSaslAuthenticateResponse](<#func-newmocksaslauthenticateresponse>)
  - [func (msar *MockSaslAuthenticateResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mocksaslauthenticateresponse-for>)
  - [func (msar *MockSaslAuthenticateResponse) SetAuthBytes(saslAuthBytes []byte) *MockSaslAuthenticateResponse](<#func-mocksaslauthenticateresponse-setauthbytes>)
  - [func (msar *MockSaslAuthenticateResponse) SetError(kerror KError) *MockSaslAuthenticateResponse](<#func-mocksaslauthenticateresponse-seterror>)
  - [func (msar *MockSaslAuthenticateResponse) SetSessionLifetimeMs(sessionLifetimeMs int64) *MockSaslAuthenticateResponse](<#func-mocksaslauthenticateresponse-setsessionlifetimems>)
- [type MockSaslHandshakeResponse](<#type-mocksaslhandshakeresponse>)
  - [func NewMockSaslHandshakeResponse(t TestReporter) *MockSaslHandshakeResponse](<#func-newmocksaslhandshakeresponse>)
  - [func (mshr *MockSaslHandshakeResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mocksaslhandshakeresponse-for>)
  - [func (mshr *MockSaslHandshakeResponse) SetEnabledMechanisms(enabledMechanisms []string) *MockSaslHandshakeResponse](<#func-mocksaslhandshakeresponse-setenabledmechanisms>)
  - [func (mshr *MockSaslHandshakeResponse) SetError(kerror KError) *MockSaslHandshakeResponse](<#func-mocksaslhandshakeresponse-seterror>)
- [type MockSequence](<#type-mocksequence>)
  - [func NewMockSequence(responses ...interface{}) *MockSequence](<#func-newmocksequence>)
  - [func (mc *MockSequence) For(reqBody versionedDecoder) (res encoderWithHeader)](<#func-mocksequence-for>)
- [type MockSyncGroupResponse](<#type-mocksyncgroupresponse>)
  - [func NewMockSyncGroupResponse(t TestReporter) *MockSyncGroupResponse](<#func-newmocksyncgroupresponse>)
  - [func (m *MockSyncGroupResponse) For(reqBody versionedDecoder) encoderWithHeader](<#func-mocksyncgroupresponse-for>)
  - [func (m *MockSyncGroupResponse) SetError(kerr KError) *MockSyncGroupResponse](<#func-mocksyncgroupresponse-seterror>)
  - [func (m *MockSyncGroupResponse) SetMemberAssignment(assignment *ConsumerGroupMemberAssignment) *MockSyncGroupResponse](<#func-mocksyncgroupresponse-setmemberassignment>)
- [type MockWrapper](<#type-mockwrapper>)
  - [func NewMockWrapper(res encoderWithHeader) *MockWrapper](<#func-newmockwrapper>)
  - [func (mw *MockWrapper) For(reqBody versionedDecoder) (res encoderWithHeader)](<#func-mockwrapper-for>)
- [type OffsetCommitRequest](<#type-offsetcommitrequest>)
  - [func (r *OffsetCommitRequest) AddBlock(topic string, partitionID int32, offset int64, leaderEpoch int32, timestamp int64, metadata string)](<#func-offsetcommitrequest-addblock>)
  - [func (r *OffsetCommitRequest) Offset(topic string, partitionID int32) (int64, string, error)](<#func-offsetcommitrequest-offset>)
- [type OffsetCommitResponse](<#type-offsetcommitresponse>)
  - [func (r *OffsetCommitResponse) AddError(topic string, partition int32, kerror KError)](<#func-offsetcommitresponse-adderror>)
- [type OffsetFetchRequest](<#type-offsetfetchrequest>)
  - [func (r *OffsetFetchRequest) AddPartition(topic string, partitionID int32)](<#func-offsetfetchrequest-addpartition>)
  - [func (r *OffsetFetchRequest) ZeroPartitions()](<#func-offsetfetchrequest-zeropartitions>)
- [type OffsetFetchResponse](<#type-offsetfetchresponse>)
  - [func (r *OffsetFetchResponse) AddBlock(topic string, partition int32, block *OffsetFetchResponseBlock)](<#func-offsetfetchresponse-addblock>)
  - [func (r *OffsetFetchResponse) GetBlock(topic string, partition int32) *OffsetFetchResponseBlock](<#func-offsetfetchresponse-getblock>)
- [type OffsetFetchResponseBlock](<#type-offsetfetchresponseblock>)
- [type OffsetManager](<#type-offsetmanager>)
  - [func NewOffsetManagerFromClient(group string, client Client) (OffsetManager, error)](<#func-newoffsetmanagerfromclient>)
- [type OffsetRequest](<#type-offsetrequest>)
  - [func (r *OffsetRequest) AddBlock(topic string, partitionID int32, time int64, maxOffsets int32)](<#func-offsetrequest-addblock>)
  - [func (r *OffsetRequest) ReplicaID() int32](<#func-offsetrequest-replicaid>)
  - [func (r *OffsetRequest) SetReplicaID(id int32)](<#func-offsetrequest-setreplicaid>)
- [type OffsetResponse](<#type-offsetresponse>)
  - [func (r *OffsetResponse) AddTopicPartition(topic string, partition int32, offset int64)](<#func-offsetresponse-addtopicpartition>)
  - [func (r *OffsetResponse) GetBlock(topic string, partition int32) *OffsetResponseBlock](<#func-offsetresponse-getblock>)
- [type OffsetResponseBlock](<#type-offsetresponseblock>)
- [type OwnedPartition](<#type-ownedpartition>)
- [type PacketDecodingError](<#type-packetdecodingerror>)
  - [func (err PacketDecodingError) Error() string](<#func-packetdecodingerror-error>)
- [type PacketEncodingError](<#type-packetencodingerror>)
  - [func (err PacketEncodingError) Error() string](<#func-packetencodingerror-error>)
- [type PartitionConsumer](<#type-partitionconsumer>)
- [type PartitionError](<#type-partitionerror>)
- [type PartitionMetadata](<#type-partitionmetadata>)
- [type PartitionOffsetManager](<#type-partitionoffsetmanager>)
- [type PartitionOffsetMetadata](<#type-partitionoffsetmetadata>)
- [type PartitionReplicaReassignmentsStatus](<#type-partitionreplicareassignmentsstatus>)
- [type Partitioner](<#type-partitioner>)
  - [func NewHashPartitioner(topic string) Partitioner](<#func-newhashpartitioner>)
  - [func NewManualPartitioner(topic string) Partitioner](<#func-newmanualpartitioner>)
  - [func NewRandomPartitioner(topic string) Partitioner](<#func-newrandompartitioner>)
  - [func NewReferenceHashPartitioner(topic string) Partitioner](<#func-newreferencehashpartitioner>)
  - [func NewRoundRobinPartitioner(topic string) Partitioner](<#func-newroundrobinpartitioner>)
- [type PartitionerConstructor](<#type-partitionerconstructor>)
  - [func NewCustomHashPartitioner(hasher func() hash.Hash32) PartitionerConstructor](<#func-newcustomhashpartitioner>)
  - [func NewCustomPartitioner(options ...HashPartitionerOption) PartitionerConstructor](<#func-newcustompartitioner>)
- [type ProduceCallback](<#type-producecallback>)
- [type ProduceRequest](<#type-producerequest>)
  - [func (r *ProduceRequest) AddBatch(topic string, partition int32, batch *RecordBatch)](<#func-producerequest-addbatch>)
  - [func (r *ProduceRequest) AddMessage(topic string, partition int32, msg *Message)](<#func-producerequest-addmessage>)
  - [func (r *ProduceRequest) AddSet(topic string, partition int32, set *MessageSet)](<#func-producerequest-addset>)
- [type ProduceResponse](<#type-produceresponse>)
  - [func (r *ProduceResponse) AddTopicPartition(topic string, partition int32, err KError)](<#func-produceresponse-addtopicpartition>)
  - [func (r *ProduceResponse) GetBlock(topic string, partition int32) *ProduceResponseBlock](<#func-produceresponse-getblock>)
- [type ProduceResponseBlock](<#type-produceresponseblock>)
- [type ProducerError](<#type-producererror>)
  - [func (pe ProducerError) Error() string](<#func-producererror-error>)
  - [func (pe ProducerError) Unwrap() error](<#func-producererror-unwrap>)
- [type ProducerErrors](<#type-producererrors>)
  - [func (pe ProducerErrors) Error() string](<#func-producererrors-error>)
- [type ProducerInterceptor](<#type-producerinterceptor>)
- [type ProducerMessage](<#type-producermessage>)
  - [func (m *ProducerMessage) ByteSize(version int) int](<#func-producermessage-bytesize>)
- [type ProducerTxnStatusFlag](<#type-producertxnstatusflag>)
  - [func (s ProducerTxnStatusFlag) String() string](<#func-producertxnstatusflag-string>)
- [type QuotaEntityComponent](<#type-quotaentitycomponent>)
- [type QuotaEntityType](<#type-quotaentitytype>)
- [type QuotaFilterComponent](<#type-quotafiltercomponent>)
- [type QuotaMatchType](<#type-quotamatchtype>)
- [type Record](<#type-record>)
- [type RecordBatch](<#type-recordbatch>)
  - [func (b *RecordBatch) LastOffset() int64](<#func-recordbatch-lastoffset>)
- [type RecordHeader](<#type-recordheader>)
- [type Records](<#type-records>)
- [type RequestNotifierFunc](<#type-requestnotifierfunc>)
- [type RequestResponse](<#type-requestresponse>)
- [type RequiredAcks](<#type-requiredacks>)
- [type Resource](<#type-resource>)
- [type ResourceAcls](<#type-resourceacls>)
- [type ResourceResponse](<#type-resourceresponse>)
- [type SASLMechanism](<#type-saslmechanism>)
- [type SCRAMClient](<#type-scramclient>)
- [type SaslAuthenticateRequest](<#type-saslauthenticaterequest>)
- [type SaslAuthenticateResponse](<#type-saslauthenticateresponse>)
- [type SaslHandshakeRequest](<#type-saslhandshakerequest>)
- [type SaslHandshakeResponse](<#type-saslhandshakeresponse>)
- [type ScramMechanismType](<#type-scrammechanismtype>)
  - [func (s ScramMechanismType) String() string](<#func-scrammechanismtype-string>)
- [type StdLogger](<#type-stdlogger>)
- [type StickyAssignorUserData](<#type-stickyassignoruserdata>)
- [type StickyAssignorUserDataV0](<#type-stickyassignoruserdatav0>)
- [type StickyAssignorUserDataV1](<#type-stickyassignoruserdatav1>)
- [type StringEncoder](<#type-stringencoder>)
  - [func (s StringEncoder) Encode() ([]byte, error)](<#func-stringencoder-encode>)
  - [func (s StringEncoder) Length() int](<#func-stringencoder-length>)
- [type SyncGroupRequest](<#type-syncgrouprequest>)
  - [func (r *SyncGroupRequest) AddGroupAssignment(memberId string, memberAssignment []byte)](<#func-syncgrouprequest-addgroupassignment>)
  - [func (r *SyncGroupRequest) AddGroupAssignmentMember(memberId string, memberAssignment *ConsumerGroupMemberAssignment) error](<#func-syncgrouprequest-addgroupassignmentmember>)
- [type SyncGroupRequestAssignment](<#type-syncgrouprequestassignment>)
- [type SyncGroupResponse](<#type-syncgroupresponse>)
  - [func (r *SyncGroupResponse) GetMemberAssignment() (*ConsumerGroupMemberAssignment, error)](<#func-syncgroupresponse-getmemberassignment>)
- [type SyncProducer](<#type-syncproducer>)
  - [func NewSyncProducer(addrs []string, config *Config) (SyncProducer, error)](<#func-newsyncproducer>)
  - [func NewSyncProducerFromClient(client Client) (SyncProducer, error)](<#func-newsyncproducerfromclient>)
- [type TestReporter](<#type-testreporter>)
- [type Timestamp](<#type-timestamp>)
- [type TopicDetail](<#type-topicdetail>)
- [type TopicError](<#type-topicerror>)
  - [func (t *TopicError) Error() string](<#func-topicerror-error>)
  - [func (t *TopicError) Unwrap() error](<#func-topicerror-unwrap>)
- [type TopicMetadata](<#type-topicmetadata>)
- [type TopicPartition](<#type-topicpartition>)
- [type TopicPartitionError](<#type-topicpartitionerror>)
  - [func (t *TopicPartitionError) Error() string](<#func-topicpartitionerror-error>)
  - [func (t *TopicPartitionError) Unwrap() error](<#func-topicpartitionerror-unwrap>)
- [type TxnOffsetCommitRequest](<#type-txnoffsetcommitrequest>)
- [type TxnOffsetCommitResponse](<#type-txnoffsetcommitresponse>)
- [type UserScramCredentialsResponseInfo](<#type-userscramcredentialsresponseinfo>)
- [type ZstdDecoderParams](<#type-zstddecoderparams>)
- [type ZstdEncoderParams](<#type-zstdencoderparams>)


## Constants

```go
const (
    // RangeBalanceStrategyName identifies strategies that use the range partition assignment strategy
    RangeBalanceStrategyName = "range"

    // RoundRobinBalanceStrategyName identifies strategies that use the round-robin partition assignment strategy
    RoundRobinBalanceStrategyName = "roundrobin"

    // StickyBalanceStrategyName identifies strategies that use the sticky-partition assignment strategy
    StickyBalanceStrategyName = "sticky"
)
```

```go
const (
    // SASLTypeOAuth represents the SASL/OAUTHBEARER mechanism (Kafka 2.0.0+)
    SASLTypeOAuth = "OAUTHBEARER"
    // SASLTypePlaintext represents the SASL/PLAIN mechanism
    SASLTypePlaintext = "PLAIN"
    // SASLTypeSCRAMSHA256 represents the SCRAM-SHA-256 mechanism.
    SASLTypeSCRAMSHA256 = "SCRAM-SHA-256"
    // SASLTypeSCRAMSHA512 represents the SCRAM-SHA-512 mechanism.
    SASLTypeSCRAMSHA512 = "SCRAM-SHA-512"
    SASLTypeGSSAPI      = "GSSAPI"
    // SASLHandshakeV0 is v0 of the Kafka SASL handshake protocol. Client and
    // server negotiate SASL auth using opaque packets.
    SASLHandshakeV0 = int16(0)
    // SASLHandshakeV1 is v1 of the Kafka SASL handshake protocol. Client and
    // server negotiate SASL by wrapping tokens with Kafka protocol headers.
    SASLHandshakeV1 = int16(1)
    // SASLExtKeyAuth is the reserved extension key name sent as part of the
    // SASL/OAUTHBEARER initial client response
    SASLExtKeyAuth = "auth"
)
```

```go
const (
    // OffsetNewest stands for the log head offset, i.e. the offset that will be
    // assigned to the next message that will be produced to the partition. You
    // can send this to a client's GetOffset method to get this offset, or when
    // calling ConsumePartition to start consuming new messages.
    OffsetNewest int64 = -1
    // OffsetOldest stands for the oldest offset available on the broker for a
    // partition. You can send this to a client's GetOffset method to get this
    // offset, or when calling ConsumePartition to start consuming from the
    // oldest offset that is still available on the broker.
    OffsetOldest int64 = -2
)
```

```go
const (
    TOK_ID_KRB_AP_REQ   = 256
    GSS_API_GENERIC_TAG = 0x60
    KRB5_USER_AUTH      = 1
    KRB5_KEYTAB_AUTH    = 2
    GSS_API_INITIAL     = 1
    GSS_API_VERIFY      = 2
    GSS_API_FINISH      = 3
)
```

APIKeySASLAuth is the API key for the SaslAuthenticate Kafka API

```go
const APIKeySASLAuth = 36
```

GroupGenerationUndefined is a special value for the group generation field of Offset Commit Requests that should be used when a consumer group does not rely on Kafka for partition management.

```go
const GroupGenerationUndefined = -1
```

```go
const MAX_GROUP_INSTANCE_ID_LENGTH = 249
```

ReceiveTime is a special value for the timestamp field of Offset Commit Requests which tells the broker to set the timestamp to the time at which the request was received. The timestamp is only used if message version 1 is used, which requires kafka 0.8.2.

```go
const ReceiveTime int64 = -1
```

## Variables

```go
var (
    // Logger is the instance of a StdLogger interface that Sarama writes connection
    // management events to. By default it is set to discard all log messages via ioutil.Discard,
    // but you can set it to redirect wherever you want.
    Logger StdLogger = log.New(io.Discard, "[Sarama] ", log.LstdFlags)

    // PanicHandler is called for recovering from panics spawned internally to the library (and thus
    // not recoverable by the caller's goroutine). Defaults to nil, which means panics are not recovered.
    PanicHandler func(interface{})

    // MaxRequestSize is the maximum size (in bytes) of any request that Sarama will attempt to send. Trying
    // to send a request larger than this will result in an PacketEncodingError. The default of 100 MiB is aligned
    // with Kafka's default `socket.request.max.bytes`, which is the largest request the broker will attempt
    // to process.
    MaxRequestSize int32 = 100 * 1024 * 1024

    // MaxResponseSize is the maximum size (in bytes) of any response that Sarama will attempt to parse. If
    // a broker returns a response message larger than this value, Sarama will return a PacketDecodingError to
    // protect the client from running out of memory. Please note that brokers do not have any natural limit on
    // the size of responses they send. In particular, they can send arbitrarily large fetch responses to consumers
    // (see https://issues.apache.org/jira/browse/KAFKA-2063).
    MaxResponseSize int32 = 100 * 1024 * 1024
)
```

Effective constants defining the supported kafka versions.

```go
var (
    V0_8_2_0  = newKafkaVersion(0, 8, 2, 0)
    V0_8_2_1  = newKafkaVersion(0, 8, 2, 1)
    V0_8_2_2  = newKafkaVersion(0, 8, 2, 2)
    V0_9_0_0  = newKafkaVersion(0, 9, 0, 0)
    V0_9_0_1  = newKafkaVersion(0, 9, 0, 1)
    V0_10_0_0 = newKafkaVersion(0, 10, 0, 0)
    V0_10_0_1 = newKafkaVersion(0, 10, 0, 1)
    V0_10_1_0 = newKafkaVersion(0, 10, 1, 0)
    V0_10_1_1 = newKafkaVersion(0, 10, 1, 1)
    V0_10_2_0 = newKafkaVersion(0, 10, 2, 0)
    V0_10_2_1 = newKafkaVersion(0, 10, 2, 1)
    V0_10_2_2 = newKafkaVersion(0, 10, 2, 2)
    V0_11_0_0 = newKafkaVersion(0, 11, 0, 0)
    V0_11_0_1 = newKafkaVersion(0, 11, 0, 1)
    V0_11_0_2 = newKafkaVersion(0, 11, 0, 2)
    V1_0_0_0  = newKafkaVersion(1, 0, 0, 0)
    V1_0_1_0  = newKafkaVersion(1, 0, 1, 0)
    V1_0_2_0  = newKafkaVersion(1, 0, 2, 0)
    V1_1_0_0  = newKafkaVersion(1, 1, 0, 0)
    V1_1_1_0  = newKafkaVersion(1, 1, 1, 0)
    V2_0_0_0  = newKafkaVersion(2, 0, 0, 0)
    V2_0_1_0  = newKafkaVersion(2, 0, 1, 0)
    V2_1_0_0  = newKafkaVersion(2, 1, 0, 0)
    V2_1_1_0  = newKafkaVersion(2, 1, 1, 0)
    V2_2_0_0  = newKafkaVersion(2, 2, 0, 0)
    V2_2_1_0  = newKafkaVersion(2, 2, 1, 0)
    V2_2_2_0  = newKafkaVersion(2, 2, 2, 0)
    V2_3_0_0  = newKafkaVersion(2, 3, 0, 0)
    V2_3_1_0  = newKafkaVersion(2, 3, 1, 0)
    V2_4_0_0  = newKafkaVersion(2, 4, 0, 0)
    V2_4_1_0  = newKafkaVersion(2, 4, 1, 0)
    V2_5_0_0  = newKafkaVersion(2, 5, 0, 0)
    V2_5_1_0  = newKafkaVersion(2, 5, 1, 0)
    V2_6_0_0  = newKafkaVersion(2, 6, 0, 0)
    V2_6_1_0  = newKafkaVersion(2, 6, 1, 0)
    V2_6_2_0  = newKafkaVersion(2, 6, 2, 0)
    V2_6_3_0  = newKafkaVersion(2, 6, 3, 0)
    V2_7_0_0  = newKafkaVersion(2, 7, 0, 0)
    V2_7_1_0  = newKafkaVersion(2, 7, 1, 0)
    V2_7_2_0  = newKafkaVersion(2, 7, 2, 0)
    V2_8_0_0  = newKafkaVersion(2, 8, 0, 0)
    V2_8_1_0  = newKafkaVersion(2, 8, 1, 0)
    V2_8_2_0  = newKafkaVersion(2, 8, 2, 0)
    V3_0_0_0  = newKafkaVersion(3, 0, 0, 0)
    V3_0_1_0  = newKafkaVersion(3, 0, 1, 0)
    V3_0_2_0  = newKafkaVersion(3, 0, 2, 0)
    V3_1_0_0  = newKafkaVersion(3, 1, 0, 0)
    V3_1_1_0  = newKafkaVersion(3, 1, 1, 0)
    V3_1_2_0  = newKafkaVersion(3, 1, 2, 0)
    V3_2_0_0  = newKafkaVersion(3, 2, 0, 0)
    V3_2_1_0  = newKafkaVersion(3, 2, 1, 0)
    V3_2_2_0  = newKafkaVersion(3, 2, 2, 0)
    V3_2_3_0  = newKafkaVersion(3, 2, 3, 0)

    SupportedVersions = []KafkaVersion{
        V0_8_2_0,
        V0_8_2_1,
        V0_8_2_2,
        V0_9_0_0,
        V0_9_0_1,
        V0_10_0_0,
        V0_10_0_1,
        V0_10_1_0,
        V0_10_1_1,
        V0_10_2_0,
        V0_10_2_1,
        V0_10_2_2,
        V0_11_0_0,
        V0_11_0_1,
        V0_11_0_2,
        V1_0_0_0,
        V1_0_1_0,
        V1_0_2_0,
        V1_1_0_0,
        V1_1_1_0,
        V2_0_0_0,
        V2_0_1_0,
        V2_1_0_0,
        V2_1_1_0,
        V2_2_0_0,
        V2_2_1_0,
        V2_2_2_0,
        V2_3_0_0,
        V2_3_1_0,
        V2_4_0_0,
        V2_4_1_0,
        V2_5_0_0,
        V2_5_1_0,
        V2_6_0_0,
        V2_6_1_0,
        V2_6_2_0,
        V2_7_0_0,
        V2_7_1_0,
        V2_8_0_0,
        V2_8_1_0,
        V2_8_2_0,
        V3_0_0_0,
        V3_0_1_0,
        V3_0_2_0,
        V3_1_0_0,
        V3_1_1_0,
        V3_1_2_0,
        V3_2_0_0,
        V3_2_1_0,
        V3_2_2_0,
        V3_2_3_0,
    }
    MinVersion     = V0_8_2_0
    MaxVersion     = V3_2_3_0
    DefaultVersion = V1_0_0_0
)
```

BalanceStrategyRange is the default and assigns partitions as ranges to consumer group members. This follows the same logic as https://kafka.apache.org/31/javadoc/org/apache/kafka/clients/consumer/RangeAssignor.html

Example with two topics T1 and T2 with six partitions each \(0..5\) and two members \(M1, M2\):

```
M1: {T1: [0, 1, 2], T2: [0, 1, 2]}
M2: {T2: [3, 4, 5], T2: [3, 4, 5]}
```

```go
var BalanceStrategyRange = &balanceStrategy{
    name: RangeBalanceStrategyName,
    coreFn: func(plan BalanceStrategyPlan, memberIDs []string, topic string, partitions []int32) {
        partitionsPerConsumer := len(partitions) / len(memberIDs)
        consumersWithExtraPartition := len(partitions) % len(memberIDs)

        sort.Strings(memberIDs)

        for i, memberID := range memberIDs {
            min := i*partitionsPerConsumer + int(math.Min(float64(consumersWithExtraPartition), float64(i)))
            extra := 0
            if i < consumersWithExtraPartition {
                extra = 1
            }
            max := min + partitionsPerConsumer + extra
            plan.Add(memberID, topic, partitions[min:max]...)
        }
    },
}
```

BalanceStrategyRoundRobin assigns partitions to members in alternating order. For example, there are two topics \(t0, t1\) and two consumer \(m0, m1\), and each topic has three partitions \(p0, p1, p2\): M0: \[t0p0, t0p2, t1p1\] M1: \[t0p1, t1p0, t1p2\]

```go
var BalanceStrategyRoundRobin = new(roundRobinBalancer)
```

BalanceStrategySticky assigns partitions to members with an attempt to preserve earlier assignments while maintain a balanced partition distribution. Example with topic T with six partitions \(0..5\) and two members \(M1, M2\):

```
M1: {T: [0, 2, 4]}
M2: {T: [1, 3, 5]}
```

On reassignment with an additional consumer, you might get an assignment plan like:

```
M1: {T: [0, 2]}
M2: {T: [1, 3]}
M3: {T: [4, 5]}
```

```go
var BalanceStrategySticky = &stickyBalanceStrategy{}
```

ErrAddPartitionsToTxn is returned when AddPartitionsToTxn failed multiple times

```go
var ErrAddPartitionsToTxn = errors.New("transaction manager: failed to send partitions to transaction")
```

ErrAlreadyConnected is the error returned when calling Open\(\) on a Broker that is already connected or connecting.

```go
var ErrAlreadyConnected = errors.New("kafka: broker connection already initiated")
```

ErrBrokerNotFound is the error returned when there's no broker found for the requested ID.

```go
var ErrBrokerNotFound = errors.New("kafka: broker for ID is not found")
```

ErrCannotTransitionNilError when transition is attempted with an nil error.

```go
var ErrCannotTransitionNilError = errors.New("transaction manager: cannot transition with a nil error")
```

ErrClosedClient is the error returned when a method is called on a client that has been closed.

```go
var ErrClosedClient = errors.New("kafka: tried to use a client that was closed")
```

ErrClosedConsumerGroup is the error returned when a method is called on a consumer group that has been closed.

```go
var ErrClosedConsumerGroup = errors.New("kafka: tried to use a consumer group that was closed")
```

ErrConsumerOffsetNotAdvanced is returned when a partition consumer didn't advance its offset after parsing a RecordBatch.

```go
var ErrConsumerOffsetNotAdvanced = errors.New("kafka: consumer offset was not advanced after a RecordBatch")
```

ErrControllerNotAvailable is returned when server didn't give correct controller id. May be kafka server's version is lower than 0.10.0.0.

```go
var ErrControllerNotAvailable = errors.New("kafka: controller is not available")
```

ErrCreateACLs is the type of error returned when ACL creation failed

```go
var ErrCreateACLs = errors.New("kafka server: failed to create one or more ACL rules")
```

ErrDeleteRecords is the type of error returned when fail to delete the required records

```go
var ErrDeleteRecords = errors.New("kafka server: failed to delete records")
```

ErrIncompleteResponse is the error returned when the server returns a syntactically valid response, but it does not contain the expected information.

```go
var ErrIncompleteResponse = errors.New("kafka: response did not contain all the expected topic/partition blocks")
```

ErrInsufficientData is returned when decoding and the packet is truncated. This can be expected when requesting messages, since as an optimization the server is allowed to return a partial message at the end of the message set.

```go
var ErrInsufficientData = errors.New("kafka: insufficient data to decode packet, more bytes expected")
```

ErrInvalidPartition is the error returned when a partitioner returns an invalid partition index \(meaning one outside of the range \[0...numPartitions\-1\]\).

```go
var ErrInvalidPartition = errors.New("kafka: partitioner returned an invalid partition index")
```

ErrMessageTooLarge is returned when the next message to consume is larger than the configured Consumer.Fetch.Max

```go
var ErrMessageTooLarge = errors.New("kafka: message is larger than Consumer.Fetch.Max")
```

ErrNoTopicsToUpdateMetadata is returned when Meta.Full is set to false but no specific topics were found to update the metadata.

```go
var ErrNoTopicsToUpdateMetadata = errors.New("kafka: no specific topics to update metadata")
```

ErrNonTransactedProducer when calling BeginTxn, CommitTxn or AbortTxn on a non transactional producer.

```go
var ErrNonTransactedProducer = errors.New("transaction manager: you need to add TransactionalID to producer")
```

ErrNotConnected is the error returned when trying to send or call Close\(\) on a Broker that is not connected.

```go
var ErrNotConnected = errors.New("kafka: broker not connected")
```

ErrOutOfBrokers is the error returned when the client has run out of brokers to talk to because all of them errored or otherwise failed to respond.

```go
var ErrOutOfBrokers = errors.New("kafka: client has run out of available brokers to talk to")
```

ErrReassignPartitions is returned when altering partition assignments for a topic fails

```go
var ErrReassignPartitions = errors.New("failed to reassign partitions for topic")
```

ErrShuttingDown is returned when a producer receives a message during shutdown.

```go
var ErrShuttingDown = errors.New("kafka: message received by producer in process of shutting down")
```

ErrTransactionNotReady when transaction status is invalid for the current action.

```go
var ErrTransactionNotReady = errors.New("transaction manager: transaction is not ready")
```

ErrTransitionNotAllowed when txnmgr state transiion is not valid.

```go
var ErrTransitionNotAllowed = errors.New("transaction manager: invalid transition attempted")
```

ErrTxnOffsetCommit is returned when TxnOffsetCommit failed multiple times

```go
var ErrTxnOffsetCommit = errors.New("transaction manager: failed to send offsets to transaction")
```

ErrTxnUnableToParseResponse when response is nil

```go
var ErrTxnUnableToParseResponse = errors.New("transaction manager: unable to parse response")
```

ErrUnknownScramMechanism is returned when user tries to AlterUserScramCredentials with unknown SCRAM mechanism

```go
var ErrUnknownScramMechanism = errors.New("kafka: unknown SCRAM mechanism provided")
```

```go
var GROUP_INSTANCE_ID_REGEXP = regexp.MustCompile(`^[0-9a-zA-Z\._\-]+$`)
```

MultiErrorFormat specifies the formatter applied to format multierrors. The default implementation is a consensed version of the hashicorp/go\-multierror default one

```go
var MultiErrorFormat multierror.ErrorFormatFunc = func(es []error) string {
    if len(es) == 1 {
        return es[0].Error()
    }

    points := make([]string, len(es))
    for i, err := range es {
        points[i] = fmt.Sprintf("* %s", err)
    }

    return fmt.Sprintf(
        "%d errors occurred:\n\t%s\n",
        len(es), strings.Join(points, "\n\t"))
}
```

```go
var NoNode = &Broker{id: -1, addr: ":-1"}
```

## type AbortedTransaction

```go
type AbortedTransaction struct {
    // ProducerID contains the producer id associated with the aborted transaction.
    ProducerID int64
    // FirstOffset contains the first offset in the aborted transaction.
    FirstOffset int64
}
```

## type AccessToken

AccessToken contains an access token used to authenticate a SASL/OAUTHBEARER client along with associated metadata.

```go
type AccessToken struct {
    // Token is the access token payload.
    Token string
    // Extensions is a optional map of arbitrary key-value pairs that can be
    // sent with the SASL/OAUTHBEARER initial client response. These values are
    // ignored by the SASL server if they are unexpected. This feature is only
    // supported by Kafka >= 2.1.0.
    Extensions map[string]string
}
```

## type AccessTokenProvider

AccessTokenProvider is the interface that encapsulates how implementors can generate access tokens for Kafka broker authentication.

```go
type AccessTokenProvider interface {
    // Token returns an access token. The implementation should ensure token
    // reuse so that multiple calls at connect time do not create multiple
    // tokens. The implementation should also periodically refresh the token in
    // order to guarantee that each call returns an unexpired token.  This
    // method should not block indefinitely--a timeout error should be returned
    // after a short period of inactivity so that the broker connection logic
    // can log debugging information and retry.
    Token() (*AccessToken, error)
}
```

## type Acl

Acl holds information about acl type

```go
type Acl struct {
    Principal      string
    Host           string
    Operation      AclOperation
    PermissionType AclPermissionType
}
```

## type AclCreation

AclCreation is a wrapper around Resource and Acl type

```go
type AclCreation struct {
    Resource
    Acl
}
```

## type AclCreationResponse

AclCreationResponse is an acl creation response type

```go
type AclCreationResponse struct {
    Err    KError
    ErrMsg *string
}
```

## type AclFilter

```go
type AclFilter struct {
    Version                   int
    ResourceType              AclResourceType
    ResourceName              *string
    ResourcePatternTypeFilter AclResourcePatternType
    Principal                 *string
    Host                      *string
    Operation                 AclOperation
    PermissionType            AclPermissionType
}
```

## type AclOperation

```go
type AclOperation int
```

ref: https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/acl/AclOperation.java

```go
const (
    AclOperationUnknown AclOperation = iota
    AclOperationAny
    AclOperationAll
    AclOperationRead
    AclOperationWrite
    AclOperationCreate
    AclOperationDelete
    AclOperationAlter
    AclOperationDescribe
    AclOperationClusterAction
    AclOperationDescribeConfigs
    AclOperationAlterConfigs
    AclOperationIdempotentWrite
)
```

### func \(\*AclOperation\) MarshalText

```go
func (a *AclOperation) MarshalText() ([]byte, error)
```

MarshalText returns the text form of the AclOperation \(name without prefix\)

### func \(\*AclOperation\) String

```go
func (a *AclOperation) String() string
```

### func \(\*AclOperation\) UnmarshalText

```go
func (a *AclOperation) UnmarshalText(text []byte) error
```

UnmarshalText takes a text reprentation of the operation and converts it to an AclOperation

## type AclPermissionType

```go
type AclPermissionType int
```

ref: https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/acl/AclPermissionType.java

```go
const (
    AclPermissionUnknown AclPermissionType = iota
    AclPermissionAny
    AclPermissionDeny
    AclPermissionAllow
)
```

### func \(\*AclPermissionType\) MarshalText

```go
func (a *AclPermissionType) MarshalText() ([]byte, error)
```

MarshalText returns the text form of the AclPermissionType \(name without prefix\)

### func \(\*AclPermissionType\) String

```go
func (a *AclPermissionType) String() string
```

### func \(\*AclPermissionType\) UnmarshalText

```go
func (a *AclPermissionType) UnmarshalText(text []byte) error
```

UnmarshalText takes a text reprentation of the permission type and converts it to an AclPermissionType

## type AclResourcePatternType

```go
type AclResourcePatternType int
```

ref: https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/resource/PatternType.java

```go
const (
    AclPatternUnknown AclResourcePatternType = iota
    AclPatternAny
    AclPatternMatch
    AclPatternLiteral
    AclPatternPrefixed
)
```

### func \(\*AclResourcePatternType\) MarshalText

```go
func (a *AclResourcePatternType) MarshalText() ([]byte, error)
```

MarshalText returns the text form of the AclResourcePatternType \(name without prefix\)

### func \(\*AclResourcePatternType\) String

```go
func (a *AclResourcePatternType) String() string
```

### func \(\*AclResourcePatternType\) UnmarshalText

```go
func (a *AclResourcePatternType) UnmarshalText(text []byte) error
```

UnmarshalText takes a text reprentation of the resource pattern type and converts it to an AclResourcePatternType

## type AclResourceType

```go
type AclResourceType int
```

ref: https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/resource/ResourceType.java

```go
const (
    AclResourceUnknown AclResourceType = iota
    AclResourceAny
    AclResourceTopic
    AclResourceGroup
    AclResourceCluster
    AclResourceTransactionalID
    AclResourceDelegationToken
)
```

### func \(\*AclResourceType\) MarshalText

```go
func (a *AclResourceType) MarshalText() ([]byte, error)
```

MarshalText returns the text form of the AclResourceType \(name without prefix\)

### func \(\*AclResourceType\) String

```go
func (a *AclResourceType) String() string
```

### func \(\*AclResourceType\) UnmarshalText

```go
func (a *AclResourceType) UnmarshalText(text []byte) error
```

UnmarshalText takes a text reprentation of the resource type and converts it to an AclResourceType

## type AddOffsetsToTxnRequest

AddOffsetsToTxnRequest adds offsets to a transaction request

```go
type AddOffsetsToTxnRequest struct {
    TransactionalID string
    ProducerID      int64
    ProducerEpoch   int16
    GroupID         string
}
```

## type AddOffsetsToTxnResponse

AddOffsetsToTxnResponse is a response type for adding offsets to txns

```go
type AddOffsetsToTxnResponse struct {
    ThrottleTime time.Duration
    Err          KError
}
```

## type AddPartitionsToTxnRequest

AddPartitionsToTxnRequest is a add paartition request

```go
type AddPartitionsToTxnRequest struct {
    TransactionalID string
    ProducerID      int64
    ProducerEpoch   int16
    TopicPartitions map[string][]int32
}
```

## type AddPartitionsToTxnResponse

AddPartitionsToTxnResponse is a partition errors to transaction type

```go
type AddPartitionsToTxnResponse struct {
    ThrottleTime time.Duration
    Errors       map[string][]*PartitionError
}
```

## type AlterClientQuotasEntry

```go
type AlterClientQuotasEntry struct {
    Entity []QuotaEntityComponent // The quota entity to alter.
    Ops    []ClientQuotasOp       // An individual quota configuration entry to alter.
}
```

## type AlterClientQuotasEntryResponse

```go
type AlterClientQuotasEntryResponse struct {
    ErrorCode KError                 // The error code, or `0` if the quota alteration succeeded.
    ErrorMsg  *string                // The error message, or `null` if the quota alteration succeeded.
    Entity    []QuotaEntityComponent // The quota entity altered.
}
```

## type AlterClientQuotasRequest

```go
type AlterClientQuotasRequest struct {
    Entries      []AlterClientQuotasEntry // The quota configuration entries to alter.
    ValidateOnly bool                     // Whether the alteration should be validated, but not performed.
}
```

## type AlterClientQuotasResponse

```go
type AlterClientQuotasResponse struct {
    ThrottleTime time.Duration                    // The duration in milliseconds for which the request was throttled due to a quota violation, or zero if the request did not violate any quota.
    Entries      []AlterClientQuotasEntryResponse // The quota configuration entries altered.
}
```

## type AlterConfigsRequest

AlterConfigsRequest is an alter config request type

```go
type AlterConfigsRequest struct {
    Resources    []*AlterConfigsResource
    ValidateOnly bool
}
```

## type AlterConfigsResource

AlterConfigsResource is an alter config resource type

```go
type AlterConfigsResource struct {
    Type          ConfigResourceType
    Name          string
    ConfigEntries map[string]*string
}
```

## type AlterConfigsResourceResponse

AlterConfigsResourceResponse is a response type for alter config resource

```go
type AlterConfigsResourceResponse struct {
    ErrorCode int16
    ErrorMsg  string
    Type      ConfigResourceType
    Name      string
}
```

## type AlterConfigsResponse

AlterConfigsResponse is a response type for alter config

```go
type AlterConfigsResponse struct {
    ThrottleTime time.Duration
    Resources    []*AlterConfigsResourceResponse
}
```

## type AlterPartitionReassignmentsRequest

```go
type AlterPartitionReassignmentsRequest struct {
    TimeoutMs int32

    Version int16
    // contains filtered or unexported fields
}
```

### func \(\*AlterPartitionReassignmentsRequest\) AddBlock

```go
func (r *AlterPartitionReassignmentsRequest) AddBlock(topic string, partitionID int32, replicas []int32)
```

## type AlterPartitionReassignmentsResponse

```go
type AlterPartitionReassignmentsResponse struct {
    Version        int16
    ThrottleTimeMs int32
    ErrorCode      KError
    ErrorMessage   *string
    Errors         map[string]map[int32]*alterPartitionReassignmentsErrorBlock
}
```

### func \(\*AlterPartitionReassignmentsResponse\) AddError

```go
func (r *AlterPartitionReassignmentsResponse) AddError(topic string, partition int32, kerror KError, message *string)
```

## type AlterUserScramCredentialsDelete

```go
type AlterUserScramCredentialsDelete struct {
    Name      string
    Mechanism ScramMechanismType
}
```

## type AlterUserScramCredentialsRequest

```go
type AlterUserScramCredentialsRequest struct {
    Version int16

    // Deletions represent list of SCRAM credentials to remove
    Deletions []AlterUserScramCredentialsDelete

    // Upsertions represent list of SCRAM credentials to update/insert
    Upsertions []AlterUserScramCredentialsUpsert
}
```

## type AlterUserScramCredentialsResponse

```go
type AlterUserScramCredentialsResponse struct {
    Version int16

    ThrottleTime time.Duration

    Results []*AlterUserScramCredentialsResult
}
```

## type AlterUserScramCredentialsResult

```go
type AlterUserScramCredentialsResult struct {
    User string

    ErrorCode    KError
    ErrorMessage *string
}
```

## type AlterUserScramCredentialsUpsert

```go
type AlterUserScramCredentialsUpsert struct {
    Name       string
    Mechanism  ScramMechanismType
    Iterations int32
    Salt       []byte

    // This field is never transmitted over the wire
    // @see: https://tools.ietf.org/html/rfc5802
    Password []byte
    // contains filtered or unexported fields
}
```

## type ApiVersionsRequest

```go
type ApiVersionsRequest struct {
    // Version defines the protocol version to use for encode and decode
    Version int16
    // ClientSoftwareName contains the name of the client.
    ClientSoftwareName string
    // ClientSoftwareVersion contains the version of the client.
    ClientSoftwareVersion string
}
```

## type ApiVersionsResponse

```go
type ApiVersionsResponse struct {
    // Version defines the protocol version to use for encode and decode
    Version int16
    // ErrorCode contains the top-level error code.
    ErrorCode int16
    // ApiKeys contains the APIs supported by the broker.
    ApiKeys []ApiVersionsResponseKey
    // ThrottleTimeMs contains the duration in milliseconds for which the request was throttled due to a quota violation, or zero if the request did not violate any quota.
    ThrottleTimeMs int32
}
```

## type ApiVersionsResponseKey

ApiVersionsResponseKey contains the APIs supported by the broker.

```go
type ApiVersionsResponseKey struct {
    // Version defines the protocol version to use for encode and decode
    Version int16
    // ApiKey contains the API index.
    ApiKey int16
    // MinVersion contains the minimum supported version, inclusive.
    MinVersion int16
    // MaxVersion contains the maximum supported version, inclusive.
    MaxVersion int16
}
```

## type AsyncProducer

AsyncProducer publishes Kafka messages using a non\-blocking API. It routes messages to the correct broker for the provided topic\-partition, refreshing metadata as appropriate, and parses responses for errors. You must read from the Errors\(\) channel or the producer will deadlock. You must call Close\(\) or AsyncClose\(\) on a producer to avoid leaks and message lost: it will not be garbage\-collected automatically when it passes out of scope and buffered messages may not be flushed.

```go
type AsyncProducer interface {

    // AsyncClose triggers a shutdown of the producer. The shutdown has completed
    // when both the Errors and Successes channels have been closed. When calling
    // AsyncClose, you *must* continue to read from those channels in order to
    // drain the results of any messages in flight.
    AsyncClose()

    // Close shuts down the producer and waits for any buffered messages to be
    // flushed. You must call this function before a producer object passes out of
    // scope, as it may otherwise leak memory. You must call this before process
    // shutting down, or you may lose messages. You must call this before calling
    // Close on the underlying client.
    Close() error

    // Input is the input channel for the user to write messages to that they
    // wish to send.
    Input() chan<- *ProducerMessage

    // Successes is the success output channel back to the user when Return.Successes is
    // enabled. If Return.Successes is true, you MUST read from this channel or the
    // Producer will deadlock. It is suggested that you send and read messages
    // together in a single select statement.
    Successes() <-chan *ProducerMessage

    // Errors is the error output channel back to the user. You MUST read from this
    // channel or the Producer will deadlock when the channel is full. Alternatively,
    // you can set Producer.Return.Errors in your config to false, which prevents
    // errors to be returned.
    Errors() <-chan *ProducerError

    // IsTransactional return true when current producer is is transactional.
    IsTransactional() bool

    // TxnStatus return current producer transaction status.
    TxnStatus() ProducerTxnStatusFlag

    // BeginTxn mark current transaction as ready.
    BeginTxn() error

    // CommitTxn commit current transaction.
    CommitTxn() error

    // AbortTxn abort current transaction.
    AbortTxn() error

    // AddOffsetsToTxn add associated offsets to current transaction.
    AddOffsetsToTxn(offsets map[string][]*PartitionOffsetMetadata, groupId string) error

    // AddMessageToTxn add message offsets to current transaction.
    AddMessageToTxn(msg *ConsumerMessage, groupId string, metadata *string) error
}
```

### func NewAsyncProducer

```go
func NewAsyncProducer(addrs []string, conf *Config) (AsyncProducer, error)
```

NewAsyncProducer creates a new AsyncProducer using the given broker addresses and configuration.

### func NewAsyncProducerFromClient

```go
func NewAsyncProducerFromClient(client Client) (AsyncProducer, error)
```

NewAsyncProducerFromClient creates a new Producer using the given client. It is still necessary to call Close\(\) on the underlying client when shutting down this producer.

## type BalanceStrategy

BalanceStrategy is used to balance topics and partitions across members of a consumer group

```go
type BalanceStrategy interface {
    // Name uniquely identifies the strategy.
    Name() string

    // Plan accepts a map of `memberID -> metadata` and a map of `topic -> partitions`
    // and returns a distribution plan.
    Plan(members map[string]ConsumerGroupMemberMetadata, topics map[string][]int32) (BalanceStrategyPlan, error)

    // AssignmentData returns the serialized assignment data for the specified
    // memberID
    AssignmentData(memberID string, topics map[string][]int32, generationID int32) ([]byte, error)
}
```

## type BalanceStrategyPlan

BalanceStrategyPlan is the results of any BalanceStrategy.Plan attempt. It contains an allocation of topic/partitions by memberID in the form of a \`memberID \-\> topic \-\> partitions\` map.

```go
type BalanceStrategyPlan map[string]map[string][]int32
```

### func \(BalanceStrategyPlan\) Add

```go
func (p BalanceStrategyPlan) Add(memberID, topic string, partitions ...int32)
```

Add assigns a topic with a number partitions to a member.

## type Broker

Broker represents a single Kafka broker connection. All operations on this object are entirely concurrency\-safe.

```go
type Broker struct {
    // contains filtered or unexported fields
}
```

### func NewBroker

```go
func NewBroker(addr string) *Broker
```

NewBroker creates and returns a Broker targeting the given host:port address. This does not attempt to actually connect, you have to call Open\(\) for that.

### func \(\*Broker\) AddOffsetsToTxn

```go
func (b *Broker) AddOffsetsToTxn(request *AddOffsetsToTxnRequest) (*AddOffsetsToTxnResponse, error)
```

AddOffsetsToTxn sends a request to add offsets to txn and returns a response or error

### func \(\*Broker\) AddPartitionsToTxn

```go
func (b *Broker) AddPartitionsToTxn(request *AddPartitionsToTxnRequest) (*AddPartitionsToTxnResponse, error)
```

AddPartitionsToTxn send a request to add partition to txn and returns a response or error

### func \(\*Broker\) Addr

```go
func (b *Broker) Addr() string
```

Addr returns the broker address as either retrieved from Kafka's metadata or passed to NewBroker.

### func \(\*Broker\) AlterClientQuotas

```go
func (b *Broker) AlterClientQuotas(request *AlterClientQuotasRequest) (*AlterClientQuotasResponse, error)
```

AlterClientQuotas sends a request to alter the broker's quotas

### func \(\*Broker\) AlterConfigs

```go
func (b *Broker) AlterConfigs(request *AlterConfigsRequest) (*AlterConfigsResponse, error)
```

AlterConfigs sends a request to alter config and return a response or error

### func \(\*Broker\) AlterPartitionReassignments

```go
func (b *Broker) AlterPartitionReassignments(request *AlterPartitionReassignmentsRequest) (*AlterPartitionReassignmentsResponse, error)
```

AlterPartitionReassignments sends a alter partition reassignments request and returns alter partition reassignments response

### func \(\*Broker\) AlterUserScramCredentials

```go
func (b *Broker) AlterUserScramCredentials(req *AlterUserScramCredentialsRequest) (*AlterUserScramCredentialsResponse, error)
```

### func \(\*Broker\) ApiVersions

```go
func (b *Broker) ApiVersions(request *ApiVersionsRequest) (*ApiVersionsResponse, error)
```

ApiVersions return api version response or error

### func \(\*Broker\) AsyncProduce

```go
func (b *Broker) AsyncProduce(request *ProduceRequest, cb ProduceCallback) error
```

AsyncProduce sends a produce request and eventually call the provided callback with a produce response or an error.

Waiting for the response is generally not blocking on the contrary to using Produce. If the maximum number of in flight request configured is reached then the request will be blocked till a previous response is received.

When configured with RequiredAcks == NoResponse, the callback will not be invoked. If an error is returned because the request could not be sent then the callback will not be invoked either.

Make sure not to Close the broker in the callback as it will lead to a deadlock.

### func \(\*Broker\) Close

```go
func (b *Broker) Close() error
```

Close closes the broker resources

### func \(\*Broker\) CommitOffset

```go
func (b *Broker) CommitOffset(request *OffsetCommitRequest) (*OffsetCommitResponse, error)
```

CommitOffset return an Offset commit response or error

### func \(\*Broker\) Connected

```go
func (b *Broker) Connected() (bool, error)
```

Connected returns true if the broker is connected and false otherwise. If the broker is not connected but it had tried to connect, the error from that connection attempt is also returned.

### func \(\*Broker\) CreateAcls

```go
func (b *Broker) CreateAcls(request *CreateAclsRequest) (*CreateAclsResponse, error)
```

CreateAcls sends a create acl request and returns a response or error

### func \(\*Broker\) CreatePartitions

```go
func (b *Broker) CreatePartitions(request *CreatePartitionsRequest) (*CreatePartitionsResponse, error)
```

CreatePartitions sends a create partition request and returns create partitions response or error

### func \(\*Broker\) CreateTopics

```go
func (b *Broker) CreateTopics(request *CreateTopicsRequest) (*CreateTopicsResponse, error)
```

CreateTopics send a create topic request and returns create topic response

### func \(\*Broker\) DeleteAcls

```go
func (b *Broker) DeleteAcls(request *DeleteAclsRequest) (*DeleteAclsResponse, error)
```

DeleteAcls sends a delete acl request and returns a response or error

### func \(\*Broker\) DeleteGroups

```go
func (b *Broker) DeleteGroups(request *DeleteGroupsRequest) (*DeleteGroupsResponse, error)
```

DeleteGroups sends a request to delete groups and returns a response or error

### func \(\*Broker\) DeleteOffsets

```go
func (b *Broker) DeleteOffsets(request *DeleteOffsetsRequest) (*DeleteOffsetsResponse, error)
```

DeleteOffsets sends a request to delete group offsets and returns a response or error

### func \(\*Broker\) DeleteRecords

```go
func (b *Broker) DeleteRecords(request *DeleteRecordsRequest) (*DeleteRecordsResponse, error)
```

DeleteRecords send a request to delete records and return delete record response or error

### func \(\*Broker\) DeleteTopics

```go
func (b *Broker) DeleteTopics(request *DeleteTopicsRequest) (*DeleteTopicsResponse, error)
```

DeleteTopics sends a delete topic request and returns delete topic response

### func \(\*Broker\) DescribeAcls

```go
func (b *Broker) DescribeAcls(request *DescribeAclsRequest) (*DescribeAclsResponse, error)
```

DescribeAcls sends a describe acl request and returns a response or error

### func \(\*Broker\) DescribeClientQuotas

```go
func (b *Broker) DescribeClientQuotas(request *DescribeClientQuotasRequest) (*DescribeClientQuotasResponse, error)
```

DescribeClientQuotas sends a request to get the broker's quotas

### func \(\*Broker\) DescribeConfigs

```go
func (b *Broker) DescribeConfigs(request *DescribeConfigsRequest) (*DescribeConfigsResponse, error)
```

DescribeConfigs sends a request to describe config and returns a response or error

### func \(\*Broker\) DescribeGroups

```go
func (b *Broker) DescribeGroups(request *DescribeGroupsRequest) (*DescribeGroupsResponse, error)
```

DescribeGroups return describe group response or error

### func \(\*Broker\) DescribeLogDirs

```go
func (b *Broker) DescribeLogDirs(request *DescribeLogDirsRequest) (*DescribeLogDirsResponse, error)
```

DescribeLogDirs sends a request to get the broker's log dir paths and sizes

### func \(\*Broker\) DescribeUserScramCredentials

```go
func (b *Broker) DescribeUserScramCredentials(req *DescribeUserScramCredentialsRequest) (*DescribeUserScramCredentialsResponse, error)
```

DescribeUserScramCredentials sends a request to get SCRAM users

### func \(\*Broker\) EndTxn

```go
func (b *Broker) EndTxn(request *EndTxnRequest) (*EndTxnResponse, error)
```

EndTxn sends a request to end txn and returns a response or error

### func \(\*Broker\) Fetch

```go
func (b *Broker) Fetch(request *FetchRequest) (*FetchResponse, error)
```

Fetch returns a FetchResponse or error

### func \(\*Broker\) FetchOffset

```go
func (b *Broker) FetchOffset(request *OffsetFetchRequest) (*OffsetFetchResponse, error)
```

FetchOffset returns an offset fetch response or error

### func \(\*Broker\) FindCoordinator

```go
func (b *Broker) FindCoordinator(request *FindCoordinatorRequest) (*FindCoordinatorResponse, error)
```

FindCoordinator sends a find coordinate request and returns a response or error

### func \(\*Broker\) GetAvailableOffsets

```go
func (b *Broker) GetAvailableOffsets(request *OffsetRequest) (*OffsetResponse, error)
```

GetAvailableOffsets return an offset response or error

### func \(\*Broker\) GetConsumerMetadata

```go
func (b *Broker) GetConsumerMetadata(request *ConsumerMetadataRequest) (*ConsumerMetadataResponse, error)
```

GetConsumerMetadata send a consumer metadata request and returns a consumer metadata response or error

### func \(\*Broker\) GetMetadata

```go
func (b *Broker) GetMetadata(request *MetadataRequest) (*MetadataResponse, error)
```

GetMetadata send a metadata request and returns a metadata response or error

### func \(\*Broker\) Heartbeat

```go
func (b *Broker) Heartbeat(request *HeartbeatRequest) (*HeartbeatResponse, error)
```

Heartbeat returns a heartbeat response or error

### func \(\*Broker\) ID

```go
func (b *Broker) ID() int32
```

ID returns the broker ID retrieved from Kafka's metadata, or \-1 if that is not known.

### func \(\*Broker\) IncrementalAlterConfigs

```go
func (b *Broker) IncrementalAlterConfigs(request *IncrementalAlterConfigsRequest) (*IncrementalAlterConfigsResponse, error)
```

IncrementalAlterConfigs sends a request to incremental alter config and return a response or error

### func \(\*Broker\) InitProducerID

```go
func (b *Broker) InitProducerID(request *InitProducerIDRequest) (*InitProducerIDResponse, error)
```

InitProducerID sends an init producer request and returns a response or error

### func \(\*Broker\) JoinGroup

```go
func (b *Broker) JoinGroup(request *JoinGroupRequest) (*JoinGroupResponse, error)
```

JoinGroup returns a join group response or error

### func \(\*Broker\) LeaveGroup

```go
func (b *Broker) LeaveGroup(request *LeaveGroupRequest) (*LeaveGroupResponse, error)
```

LeaveGroup return a leave group response or error

### func \(\*Broker\) ListGroups

```go
func (b *Broker) ListGroups(request *ListGroupsRequest) (*ListGroupsResponse, error)
```

ListGroups return a list group response or error

### func \(\*Broker\) ListPartitionReassignments

```go
func (b *Broker) ListPartitionReassignments(request *ListPartitionReassignmentsRequest) (*ListPartitionReassignmentsResponse, error)
```

ListPartitionReassignments sends a list partition reassignments request and returns list partition reassignments response

### func \(\*Broker\) Open

```go
func (b *Broker) Open(conf *Config) error
```

Open tries to connect to the Broker if it is not already connected or connecting, but does not block waiting for the connection to complete. This means that any subsequent operations on the broker will block waiting for the connection to succeed or fail. To get the effect of a fully synchronous Open call, follow it by a call to Connected\(\). The only errors Open will return directly are ConfigurationError or AlreadyConnected. If conf is nil, the result of NewConfig\(\) is used.

### func \(\*Broker\) Produce

```go
func (b *Broker) Produce(request *ProduceRequest) (*ProduceResponse, error)
```

Produce returns a produce response or error

### func \(\*Broker\) Rack

```go
func (b *Broker) Rack() string
```

Rack returns the broker's rack as retrieved from Kafka's metadata or the empty string if it is not known.  The returned value corresponds to the broker's broker.rack configuration setting.  Requires protocol version to be at least v0.10.0.0.

### func \(\*Broker\) ResponseSize

```go
func (b *Broker) ResponseSize() int
```

### func \(\*Broker\) SyncGroup

```go
func (b *Broker) SyncGroup(request *SyncGroupRequest) (*SyncGroupResponse, error)
```

SyncGroup returns a sync group response or error

### func \(\*Broker\) TLSConnectionState

```go
func (b *Broker) TLSConnectionState() (state tls.ConnectionState, ok bool)
```

TLSConnectionState returns the client's TLS connection state. The second return value is false if this is not a tls connection or the connection has not yet been established.

### func \(\*Broker\) TxnOffsetCommit

```go
func (b *Broker) TxnOffsetCommit(request *TxnOffsetCommitRequest) (*TxnOffsetCommitResponse, error)
```

TxnOffsetCommit sends a request to commit transaction offsets and returns a response or error

## type ByteEncoder

ByteEncoder implements the Encoder interface for Go byte slices so that they can be used as the Key or Value in a ProducerMessage.

```go
type ByteEncoder []byte
```

### func \(ByteEncoder\) Encode

```go
func (b ByteEncoder) Encode() ([]byte, error)
```

### func \(ByteEncoder\) Length

```go
func (b ByteEncoder) Length() int
```

## type Client

Client is a generic Kafka client. It manages connections to one or more Kafka brokers. You MUST call Close\(\) on a client to avoid leaks, it will not be garbage\-collected automatically when it passes out of scope. It is safe to share a client amongst many users, however Kafka will process requests from a single client strictly in serial, so it is generally more efficient to use the default one client per producer/consumer.

```go
type Client interface {
    // Config returns the Config struct of the client. This struct should not be
    // altered after it has been created.
    Config() *Config

    // Controller returns the cluster controller broker. It will return a
    // locally cached value if it's available. You can call RefreshController
    // to update the cached value. Requires Kafka 0.10 or higher.
    Controller() (*Broker, error)

    // RefreshController retrieves the cluster controller from fresh metadata
    // and stores it in the local cache. Requires Kafka 0.10 or higher.
    RefreshController() (*Broker, error)

    // Brokers returns the current set of active brokers as retrieved from cluster metadata.
    Brokers() []*Broker

    // Broker returns the active Broker if available for the broker ID.
    Broker(brokerID int32) (*Broker, error)

    // Topics returns the set of available topics as retrieved from cluster metadata.
    Topics() ([]string, error)

    // Partitions returns the sorted list of all partition IDs for the given topic.
    Partitions(topic string) ([]int32, error)

    // WritablePartitions returns the sorted list of all writable partition IDs for
    // the given topic, where "writable" means "having a valid leader accepting
    // writes".
    WritablePartitions(topic string) ([]int32, error)

    // Leader returns the broker object that is the leader of the current
    // topic/partition, as determined by querying the cluster metadata.
    Leader(topic string, partitionID int32) (*Broker, error)

    // Replicas returns the set of all replica IDs for the given partition.
    Replicas(topic string, partitionID int32) ([]int32, error)

    // InSyncReplicas returns the set of all in-sync replica IDs for the given
    // partition. In-sync replicas are replicas which are fully caught up with
    // the partition leader.
    InSyncReplicas(topic string, partitionID int32) ([]int32, error)

    // OfflineReplicas returns the set of all offline replica IDs for the given
    // partition. Offline replicas are replicas which are offline
    OfflineReplicas(topic string, partitionID int32) ([]int32, error)

    // RefreshBrokers takes a list of addresses to be used as seed brokers.
    // Existing broker connections are closed and the updated list of seed brokers
    // will be used for the next metadata fetch.
    RefreshBrokers(addrs []string) error

    // RefreshMetadata takes a list of topics and queries the cluster to refresh the
    // available metadata for those topics. If no topics are provided, it will refresh
    // metadata for all topics.
    RefreshMetadata(topics ...string) error

    // GetOffset queries the cluster to get the most recent available offset at the
    // given time (in milliseconds) on the topic/partition combination.
    // Time should be OffsetOldest for the earliest available offset,
    // OffsetNewest for the offset of the message that will be produced next, or a time.
    GetOffset(topic string, partitionID int32, time int64) (int64, error)

    // Coordinator returns the coordinating broker for a consumer group. It will
    // return a locally cached value if it's available. You can call
    // RefreshCoordinator to update the cached value. This function only works on
    // Kafka 0.8.2 and higher.
    Coordinator(consumerGroup string) (*Broker, error)

    // RefreshCoordinator retrieves the coordinator for a consumer group and stores it
    // in local cache. This function only works on Kafka 0.8.2 and higher.
    RefreshCoordinator(consumerGroup string) error

    // Coordinator returns the coordinating broker for a transaction id. It will
    // return a locally cached value if it's available. You can call
    // RefreshCoordinator to update the cached value. This function only works on
    // Kafka 0.11.0.0 and higher.
    TransactionCoordinator(transactionID string) (*Broker, error)

    // RefreshCoordinator retrieves the coordinator for a transaction id and stores it
    // in local cache. This function only works on Kafka 0.11.0.0 and higher.
    RefreshTransactionCoordinator(transactionID string) error

    // InitProducerID retrieves information required for Idempotent Producer
    InitProducerID() (*InitProducerIDResponse, error)

    // LeastLoadedBroker retrieves broker that has the least responses pending
    LeastLoadedBroker() *Broker

    // Close shuts down all broker connections managed by this client. It is required
    // to call this function before a client object passes out of scope, as it will
    // otherwise leak memory. You must close any Producers or Consumers using a client
    // before you close the client.
    Close() error

    // Closed returns true if the client has already had Close called on it
    Closed() bool
}
```

### func NewClient

```go
func NewClient(addrs []string, conf *Config) (Client, error)
```

NewClient creates a new Client. It connects to one of the given broker addresses and uses that broker to automatically fetch metadata on the rest of the kafka cluster. If metadata cannot be retrieved from any of the given broker addresses, the client is not created.

## type ClientQuotasOp

```go
type ClientQuotasOp struct {
    Key    string  // The quota configuration key.
    Value  float64 // The value to set, otherwise ignored if the value is to be removed.
    Remove bool    // Whether the quota configuration value should be removed, otherwise set.
}
```

## type ClusterAdmin

ClusterAdmin is the administrative client for Kafka, which supports managing and inspecting topics, brokers, configurations and ACLs. The minimum broker version required is 0.10.0.0. Methods with stricter requirements will specify the minimum broker version required. You MUST call Close\(\) on a client to avoid leaks

```go
type ClusterAdmin interface {
    // Creates a new topic. This operation is supported by brokers with version 0.10.1.0 or higher.
    // It may take several seconds after CreateTopic returns success for all the brokers
    // to become aware that the topic has been created. During this time, listTopics
    // may not return information about the new topic.The validateOnly option is supported from version 0.10.2.0.
    CreateTopic(topic string, detail *TopicDetail, validateOnly bool) error

    // List the topics available in the cluster with the default options.
    ListTopics() (map[string]TopicDetail, error)

    // Describe some topics in the cluster.
    DescribeTopics(topics []string) (metadata []*TopicMetadata, err error)

    // Delete a topic. It may take several seconds after the DeleteTopic to returns success
    // and for all the brokers to become aware that the topics are gone.
    // During this time, listTopics  may continue to return information about the deleted topic.
    // If delete.topic.enable is false on the brokers, deleteTopic will mark
    // the topic for deletion, but not actually delete them.
    // This operation is supported by brokers with version 0.10.1.0 or higher.
    DeleteTopic(topic string) error

    // Increase the number of partitions of the topics  according to the corresponding values.
    // If partitions are increased for a topic that has a key, the partition logic or ordering of
    // the messages will be affected. It may take several seconds after this method returns
    // success for all the brokers to become aware that the partitions have been created.
    // During this time, ClusterAdmin#describeTopics may not return information about the
    // new partitions. This operation is supported by brokers with version 1.0.0 or higher.
    CreatePartitions(topic string, count int32, assignment [][]int32, validateOnly bool) error

    // Alter the replica assignment for partitions.
    // This operation is supported by brokers with version 2.4.0.0 or higher.
    AlterPartitionReassignments(topic string, assignment [][]int32) error

    // Provides info on ongoing partitions replica reassignments.
    // This operation is supported by brokers with version 2.4.0.0 or higher.
    ListPartitionReassignments(topics string, partitions []int32) (topicStatus map[string]map[int32]*PartitionReplicaReassignmentsStatus, err error)

    // Delete records whose offset is smaller than the given offset of the corresponding partition.
    // This operation is supported by brokers with version 0.11.0.0 or higher.
    DeleteRecords(topic string, partitionOffsets map[int32]int64) error

    // Get the configuration for the specified resources.
    // The returned configuration includes default values and the Default is true
    // can be used to distinguish them from user supplied values.
    // Config entries where ReadOnly is true cannot be updated.
    // The value of config entries where Sensitive is true is always nil so
    // sensitive information is not disclosed.
    // This operation is supported by brokers with version 0.11.0.0 or higher.
    DescribeConfig(resource ConfigResource) ([]ConfigEntry, error)

    // Update the configuration for the specified resources with the default options.
    // This operation is supported by brokers with version 0.11.0.0 or higher.
    // The resources with their configs (topic is the only resource type with configs
    // that can be updated currently Updates are not transactional so they may succeed
    // for some resources while fail for others. The configs for a particular resource are updated automatically.
    AlterConfig(resourceType ConfigResourceType, name string, entries map[string]*string, validateOnly bool) error

    // IncrementalAlterConfig Incrementally Update the configuration for the specified resources with the default options.
    // This operation is supported by brokers with version 2.3.0.0 or higher.
    // Updates are not transactional so they may succeed for some resources while fail for others.
    // The configs for a particular resource are updated automatically.
    IncrementalAlterConfig(resourceType ConfigResourceType, name string, entries map[string]IncrementalAlterConfigsEntry, validateOnly bool) error

    // Creates an access control list (ACL) which is bound to a specific resource.
    // This operation is not transactional so it may succeed or fail.
    // If you attempt to add an ACL that duplicates an existing ACL, no error will be raised, but
    // no changes will be made. This operation is supported by brokers with version 0.11.0.0 or higher.
    // Deprecated: Use CreateACLs instead.
    CreateACL(resource Resource, acl Acl) error

    // Creates access control lists (ACLs) which are bound to specific resources.
    // This operation is not transactional so it may succeed for some ACLs while fail for others.
    // If you attempt to add an ACL that duplicates an existing ACL, no error will be raised, but
    // no changes will be made. This operation is supported by brokers with version 0.11.0.0 or higher.
    CreateACLs([]*ResourceAcls) error

    // Lists access control lists (ACLs) according to the supplied filter.
    // it may take some time for changes made by createAcls or deleteAcls to be reflected in the output of ListAcls
    // This operation is supported by brokers with version 0.11.0.0 or higher.
    ListAcls(filter AclFilter) ([]ResourceAcls, error)

    // Deletes access control lists (ACLs) according to the supplied filters.
    // This operation is not transactional so it may succeed for some ACLs while fail for others.
    // This operation is supported by brokers with version 0.11.0.0 or higher.
    DeleteACL(filter AclFilter, validateOnly bool) ([]MatchingAcl, error)

    // List the consumer groups available in the cluster.
    ListConsumerGroups() (map[string]string, error)

    // Describe the given consumer groups.
    DescribeConsumerGroups(groups []string) ([]*GroupDescription, error)

    // List the consumer group offsets available in the cluster.
    ListConsumerGroupOffsets(group string, topicPartitions map[string][]int32) (*OffsetFetchResponse, error)

    // Deletes a consumer group offset
    DeleteConsumerGroupOffset(group string, topic string, partition int32) error

    // Delete a consumer group.
    DeleteConsumerGroup(group string) error

    // Get information about the nodes in the cluster
    DescribeCluster() (brokers []*Broker, controllerID int32, err error)

    // Get information about all log directories on the given set of brokers
    DescribeLogDirs(brokers []int32) (map[int32][]DescribeLogDirsResponseDirMetadata, error)

    // Get information about SCRAM users
    DescribeUserScramCredentials(users []string) ([]*DescribeUserScramCredentialsResult, error)

    // Delete SCRAM users
    DeleteUserScramCredentials(delete []AlterUserScramCredentialsDelete) ([]*AlterUserScramCredentialsResult, error)

    // Upsert SCRAM users
    UpsertUserScramCredentials(upsert []AlterUserScramCredentialsUpsert) ([]*AlterUserScramCredentialsResult, error)

    // Get client quota configurations corresponding to the specified filter.
    // This operation is supported by brokers with version 2.6.0.0 or higher.
    DescribeClientQuotas(components []QuotaFilterComponent, strict bool) ([]DescribeClientQuotasEntry, error)

    // Alters client quota configurations with the specified alterations.
    // This operation is supported by brokers with version 2.6.0.0 or higher.
    AlterClientQuotas(entity []QuotaEntityComponent, op ClientQuotasOp, validateOnly bool) error

    // Controller returns the cluster controller broker. It will return a
    // locally cached value if it's available.
    Controller() (*Broker, error)

    // Remove members from the consumer group by given member identities.
    // This operation is supported by brokers with version 2.3 or higher
    // This is for static membership feature. KIP-345
    RemoveMemberFromConsumerGroup(groupId string, groupInstanceIds []string) (*LeaveGroupResponse, error)

    // Close shuts down the admin and closes underlying client.
    Close() error
}
```

### func NewClusterAdmin

```go
func NewClusterAdmin(addrs []string, conf *Config) (ClusterAdmin, error)
```

NewClusterAdmin creates a new ClusterAdmin using the given broker addresses and configuration.

### func NewClusterAdminFromClient

```go
func NewClusterAdminFromClient(client Client) (ClusterAdmin, error)
```

NewClusterAdminFromClient creates a new ClusterAdmin using the given client. Note that underlying client will also be closed on admin's Close\(\) call.

## type CompressionCodec

CompressionCodec represents the various compression codecs recognized by Kafka in messages.

```go
type CompressionCodec int8
```

```go
const (
    // CompressionNone no compression
    CompressionNone CompressionCodec = iota
    // CompressionGZIP compression using GZIP
    CompressionGZIP
    // CompressionSnappy compression using snappy
    CompressionSnappy
    // CompressionLZ4 compression using LZ4
    CompressionLZ4
    // CompressionZSTD compression using ZSTD
    CompressionZSTD

    // CompressionLevelDefault is the constant to use in CompressionLevel
    // to have the default compression level for any codec. The value is picked
    // that we don't use any existing compression levels.
    CompressionLevelDefault = -1000
)
```

### func \(CompressionCodec\) MarshalText

```go
func (cc CompressionCodec) MarshalText() ([]byte, error)
```

MarshalText transforms a CompressionCodec into its string representation.

### func \(CompressionCodec\) String

```go
func (cc CompressionCodec) String() string
```

### func \(\*CompressionCodec\) UnmarshalText

```go
func (cc *CompressionCodec) UnmarshalText(text []byte) error
```

UnmarshalText returns a CompressionCodec from its string representation.

## type Config

Config is used to pass multiple configuration options to Sarama's constructors.

```go
type Config struct {
    // Admin is the namespace for ClusterAdmin properties used by the administrative Kafka client.
    Admin struct {
        Retry struct {
            // The total number of times to retry sending (retriable) admin requests (default 5).
            // Similar to the `retries` setting of the JVM AdminClientConfig.
            Max int
            // Backoff time between retries of a failed request (default 100ms)
            Backoff time.Duration
        }
        // The maximum duration the administrative Kafka client will wait for ClusterAdmin operations,
        // including topics, brokers, configurations and ACLs (defaults to 3 seconds).
        Timeout time.Duration
    }

    // Net is the namespace for network-level properties used by the Broker, and
    // shared by the Client/Producer/Consumer.
    Net struct {
        // How many outstanding requests a connection is allowed to have before
        // sending on it blocks (default 5).
        // Throughput can improve but message ordering is not guaranteed if Producer.Idempotent is disabled, see:
        // https://kafka.apache.org/protocol#protocol_network
        // https://kafka.apache.org/28/documentation.html#producerconfigs_max.in.flight.requests.per.connection
        MaxOpenRequests int

        // All three of the below configurations are similar to the
        // `socket.timeout.ms` setting in JVM kafka. All of them default
        // to 30 seconds.
        DialTimeout  time.Duration // How long to wait for the initial connection.
        ReadTimeout  time.Duration // How long to wait for a response.
        WriteTimeout time.Duration // How long to wait for a transmit.

        TLS struct {
            // Whether or not to use TLS when connecting to the broker
            // (defaults to false).
            Enable bool
            // The TLS configuration to use for secure connections if
            // enabled (defaults to nil).
            Config *tls.Config
        }

        // SASL based authentication with broker. While there are multiple SASL authentication methods
        // the current implementation is limited to plaintext (SASL/PLAIN) authentication
        SASL struct {
            // Whether or not to use SASL authentication when connecting to the broker
            // (defaults to false).
            Enable bool
            // SASLMechanism is the name of the enabled SASL mechanism.
            // Possible values: OAUTHBEARER, PLAIN (defaults to PLAIN).
            Mechanism SASLMechanism
            // Version is the SASL Protocol Version to use
            // Kafka > 1.x should use V1, except on Azure EventHub which use V0
            Version int16
            // Whether or not to send the Kafka SASL handshake first if enabled
            // (defaults to true). You should only set this to false if you're using
            // a non-Kafka SASL proxy.
            Handshake bool
            // AuthIdentity is an (optional) authorization identity (authzid) to
            // use for SASL/PLAIN authentication (if different from User) when
            // an authenticated user is permitted to act as the presented
            // alternative user. See RFC4616 for details.
            AuthIdentity string
            // User is the authentication identity (authcid) to present for
            // SASL/PLAIN or SASL/SCRAM authentication
            User string
            // Password for SASL/PLAIN authentication
            Password string
            // authz id used for SASL/SCRAM authentication
            SCRAMAuthzID string
            // SCRAMClientGeneratorFunc is a generator of a user provided implementation of a SCRAM
            // client used to perform the SCRAM exchange with the server.
            SCRAMClientGeneratorFunc func() SCRAMClient
            // TokenProvider is a user-defined callback for generating
            // access tokens for SASL/OAUTHBEARER auth. See the
            // AccessTokenProvider interface docs for proper implementation
            // guidelines.
            TokenProvider AccessTokenProvider

            GSSAPI GSSAPIConfig
        }

        // KeepAlive specifies the keep-alive period for an active network connection (defaults to 0).
        // If zero or positive, keep-alives are enabled.
        // If negative, keep-alives are disabled.
        KeepAlive time.Duration

        // LocalAddr is the local address to use when dialing an
        // address. The address must be of a compatible type for the
        // network being dialed.
        // If nil, a local address is automatically chosen.
        LocalAddr net.Addr

        Proxy struct {
            // Whether or not to use proxy when connecting to the broker
            // (defaults to false).
            Enable bool
            // The proxy dialer to use enabled (defaults to nil).
            Dialer proxy.Dialer
        }
    }

    // Metadata is the namespace for metadata management properties used by the
    // Client, and shared by the Producer/Consumer.
    Metadata struct {
        Retry struct {
            // The total number of times to retry a metadata request when the
            // cluster is in the middle of a leader election (default 3).
            Max int
            // How long to wait for leader election to occur before retrying
            // (default 250ms). Similar to the JVM's `retry.backoff.ms`.
            Backoff time.Duration
            // Called to compute backoff time dynamically. Useful for implementing
            // more sophisticated backoff strategies. This takes precedence over
            // `Backoff` if set.
            BackoffFunc func(retries, maxRetries int) time.Duration
        }
        // How frequently to refresh the cluster metadata in the background.
        // Defaults to 10 minutes. Set to 0 to disable. Similar to
        // `topic.metadata.refresh.interval.ms` in the JVM version.
        RefreshFrequency time.Duration

        // Whether to maintain a full set of metadata for all topics, or just
        // the minimal set that has been necessary so far. The full set is simpler
        // and usually more convenient, but can take up a substantial amount of
        // memory if you have many topics and partitions. Defaults to true.
        Full bool

        // How long to wait for a successful metadata response.
        // Disabled by default which means a metadata request against an unreachable
        // cluster (all brokers are unreachable or unresponsive) can take up to
        // `Net.[Dial|Read]Timeout * BrokerCount * (Metadata.Retry.Max + 1) + Metadata.Retry.Backoff * Metadata.Retry.Max`
        // to fail.
        Timeout time.Duration

        // Whether to allow auto-create topics in metadata refresh. If set to true,
        // the broker may auto-create topics that we requested which do not already exist,
        // if it is configured to do so (`auto.create.topics.enable` is true). Defaults to true.
        AllowAutoTopicCreation bool
    }

    // Producer is the namespace for configuration related to producing messages,
    // used by the Producer.
    Producer struct {
        // The maximum permitted size of a message (defaults to 1000000). Should be
        // set equal to or smaller than the broker's `message.max.bytes`.
        MaxMessageBytes int
        // The level of acknowledgement reliability needed from the broker (defaults
        // to WaitForLocal). Equivalent to the `request.required.acks` setting of the
        // JVM producer.
        RequiredAcks RequiredAcks
        // The maximum duration the broker will wait the receipt of the number of
        // RequiredAcks (defaults to 10 seconds). This is only relevant when
        // RequiredAcks is set to WaitForAll or a number > 1. Only supports
        // millisecond resolution, nanoseconds will be truncated. Equivalent to
        // the JVM producer's `request.timeout.ms` setting.
        Timeout time.Duration
        // The type of compression to use on messages (defaults to no compression).
        // Similar to `compression.codec` setting of the JVM producer.
        Compression CompressionCodec
        // The level of compression to use on messages. The meaning depends
        // on the actual compression type used and defaults to default compression
        // level for the codec.
        CompressionLevel int
        // Generates partitioners for choosing the partition to send messages to
        // (defaults to hashing the message key). Similar to the `partitioner.class`
        // setting for the JVM producer.
        Partitioner PartitionerConstructor
        // If enabled, the producer will ensure that exactly one copy of each message is
        // written.
        Idempotent bool
        // Transaction specify
        Transaction struct {
            // Used in transactions to identify an instance of a producer through restarts
            ID  string
            // Amount of time a transaction can remain unresolved (neither committed nor aborted)
            // default is 1 min
            Timeout time.Duration

            Retry struct {
                // The total number of times to retry sending a message (default 50).
                // Similar to the `message.send.max.retries` setting of the JVM producer.
                Max int
                // How long to wait for the cluster to settle between retries
                // (default 10ms). Similar to the `retry.backoff.ms` setting of the
                // JVM producer.
                Backoff time.Duration
                // Called to compute backoff time dynamically. Useful for implementing
                // more sophisticated backoff strategies. This takes precedence over
                // `Backoff` if set.
                BackoffFunc func(retries, maxRetries int) time.Duration
            }
        }

        // Return specifies what channels will be populated. If they are set to true,
        // you must read from the respective channels to prevent deadlock. If,
        // however, this config is used to create a `SyncProducer`, both must be set
        // to true and you shall not read from the channels since the producer does
        // this internally.
        Return struct {
            // If enabled, successfully delivered messages will be returned on the
            // Successes channel (default disabled).
            Successes bool

            // If enabled, messages that failed to deliver will be returned on the
            // Errors channel, including error (default enabled).
            Errors bool
        }

        // The following config options control how often messages are batched up and
        // sent to the broker. By default, messages are sent as fast as possible, and
        // all messages received while the current batch is in-flight are placed
        // into the subsequent batch.
        Flush struct {
            // The best-effort number of bytes needed to trigger a flush. Use the
            // global sarama.MaxRequestSize to set a hard upper limit.
            Bytes int
            // The best-effort number of messages needed to trigger a flush. Use
            // `MaxMessages` to set a hard upper limit.
            Messages int
            // The best-effort frequency of flushes. Equivalent to
            // `queue.buffering.max.ms` setting of JVM producer.
            Frequency time.Duration
            // The maximum number of messages the producer will send in a single
            // broker request. Defaults to 0 for unlimited. Similar to
            // `queue.buffering.max.messages` in the JVM producer.
            MaxMessages int
        }

        Retry struct {
            // The total number of times to retry sending a message (default 3).
            // Similar to the `message.send.max.retries` setting of the JVM producer.
            Max int
            // How long to wait for the cluster to settle between retries
            // (default 100ms). Similar to the `retry.backoff.ms` setting of the
            // JVM producer.
            Backoff time.Duration
            // Called to compute backoff time dynamically. Useful for implementing
            // more sophisticated backoff strategies. This takes precedence over
            // `Backoff` if set.
            BackoffFunc func(retries, maxRetries int) time.Duration
        }

        // Interceptors to be called when the producer dispatcher reads the
        // message for the first time. Interceptors allows to intercept and
        // possible mutate the message before they are published to Kafka
        // cluster. *ProducerMessage modified by the first interceptor's
        // OnSend() is passed to the second interceptor OnSend(), and so on in
        // the interceptor chain.
        Interceptors []ProducerInterceptor
    }

    // Consumer is the namespace for configuration related to consuming messages,
    // used by the Consumer.
    Consumer struct {

        // Group is the namespace for configuring consumer group.
        Group struct {
            Session struct {
                // The timeout used to detect consumer failures when using Kafka's group management facility.
                // The consumer sends periodic heartbeats to indicate its liveness to the broker.
                // If no heartbeats are received by the broker before the expiration of this session timeout,
                // then the broker will remove this consumer from the group and initiate a rebalance.
                // Note that the value must be in the allowable range as configured in the broker configuration
                // by `group.min.session.timeout.ms` and `group.max.session.timeout.ms` (default 10s)
                Timeout time.Duration
            }
            Heartbeat struct {
                // The expected time between heartbeats to the consumer coordinator when using Kafka's group
                // management facilities. Heartbeats are used to ensure that the consumer's session stays active and
                // to facilitate rebalancing when new consumers join or leave the group.
                // The value must be set lower than Consumer.Group.Session.Timeout, but typically should be set no
                // higher than 1/3 of that value.
                // It can be adjusted even lower to control the expected time for normal rebalances (default 3s)
                Interval time.Duration
            }
            Rebalance struct {
                // Strategy for allocating topic partitions to members (default BalanceStrategyRange)
                // Deprecated: Strategy exists for historical compatibility
                // and should not be used. Please use GroupStrategies.
                Strategy BalanceStrategy

                // GroupStrategies is the priority-ordered list of client-side consumer group
                // balancing strategies that will be offered to the coordinator. The first
                // strategy that all group members support will be chosen by the leader.
                // default: [BalanceStrategyRange]
                GroupStrategies []BalanceStrategy

                // The maximum allowed time for each worker to join the group once a rebalance has begun.
                // This is basically a limit on the amount of time needed for all tasks to flush any pending
                // data and commit offsets. If the timeout is exceeded, then the worker will be removed from
                // the group, which will cause offset commit failures (default 60s).
                Timeout time.Duration

                Retry struct {
                    // When a new consumer joins a consumer group the set of consumers attempt to "rebalance"
                    // the load to assign partitions to each consumer. If the set of consumers changes while
                    // this assignment is taking place the rebalance will fail and retry. This setting controls
                    // the maximum number of attempts before giving up (default 4).
                    Max int
                    // Backoff time between retries during rebalance (default 2s)
                    Backoff time.Duration
                }
            }
            Member struct {
                // Custom metadata to include when joining the group. The user data for all joined members
                // can be retrieved by sending a DescribeGroupRequest to the broker that is the
                // coordinator for the group.
                UserData []byte
            }

            // support KIP-345
            InstanceId string

            // If true, consumer offsets will be automatically reset to configured Initial value
            // if the fetched consumer offset is out of range of available offsets. Out of range
            // can happen if the data has been deleted from the server, or during situations of
            // under-replication where a replica does not have all the data yet. It can be
            // dangerous to reset the offset automatically, particularly in the latter case. Defaults
            // to true to maintain existing behavior.
            ResetInvalidOffsets bool
        }

        Retry struct {
            // How long to wait after a failing to read from a partition before
            // trying again (default 2s).
            Backoff time.Duration
            // Called to compute backoff time dynamically. Useful for implementing
            // more sophisticated backoff strategies. This takes precedence over
            // `Backoff` if set.
            BackoffFunc func(retries int) time.Duration
        }

        // Fetch is the namespace for controlling how many bytes are retrieved by any
        // given request.
        Fetch struct {
            // The minimum number of message bytes to fetch in a request - the broker
            // will wait until at least this many are available. The default is 1,
            // as 0 causes the consumer to spin when no messages are available.
            // Equivalent to the JVM's `fetch.min.bytes`.
            Min int32
            // The default number of message bytes to fetch from the broker in each
            // request (default 1MB). This should be larger than the majority of
            // your messages, or else the consumer will spend a lot of time
            // negotiating sizes and not actually consuming. Similar to the JVM's
            // `fetch.message.max.bytes`.
            Default int32
            // The maximum number of message bytes to fetch from the broker in a
            // single request. Messages larger than this will return
            // ErrMessageTooLarge and will not be consumable, so you must be sure
            // this is at least as large as your largest message. Defaults to 0
            // (no limit). Similar to the JVM's `fetch.message.max.bytes`. The
            // global `sarama.MaxResponseSize` still applies.
            Max int32
        }
        // The maximum amount of time the broker will wait for Consumer.Fetch.Min
        // bytes to become available before it returns fewer than that anyways. The
        // default is 250ms, since 0 causes the consumer to spin when no events are
        // available. 100-500ms is a reasonable range for most cases. Kafka only
        // supports precision up to milliseconds; nanoseconds will be truncated.
        // Equivalent to the JVM's `fetch.wait.max.ms`.
        MaxWaitTime time.Duration

        // The maximum amount of time the consumer expects a message takes to
        // process for the user. If writing to the Messages channel takes longer
        // than this, that partition will stop fetching more messages until it
        // can proceed again.
        // Note that, since the Messages channel is buffered, the actual grace time is
        // (MaxProcessingTime * ChannelBufferSize). Defaults to 100ms.
        // If a message is not written to the Messages channel between two ticks
        // of the expiryTicker then a timeout is detected.
        // Using a ticker instead of a timer to detect timeouts should typically
        // result in many fewer calls to Timer functions which may result in a
        // significant performance improvement if many messages are being sent
        // and timeouts are infrequent.
        // The disadvantage of using a ticker instead of a timer is that
        // timeouts will be less accurate. That is, the effective timeout could
        // be between `MaxProcessingTime` and `2 * MaxProcessingTime`. For
        // example, if `MaxProcessingTime` is 100ms then a delay of 180ms
        // between two messages being sent may not be recognized as a timeout.
        MaxProcessingTime time.Duration

        // Return specifies what channels will be populated. If they are set to true,
        // you must read from them to prevent deadlock.
        Return struct {
            // If enabled, any errors that occurred while consuming are returned on
            // the Errors channel (default disabled).
            Errors bool
        }

        // Offsets specifies configuration for how and when to commit consumed
        // offsets. This currently requires the manual use of an OffsetManager
        // but will eventually be automated.
        Offsets struct {
            // Deprecated: CommitInterval exists for historical compatibility
            // and should not be used. Please use Consumer.Offsets.AutoCommit
            CommitInterval time.Duration

            // AutoCommit specifies configuration for commit messages automatically.
            AutoCommit struct {
                // Whether or not to auto-commit updated offsets back to the broker.
                // (default enabled).
                Enable bool

                // How frequently to commit updated offsets. Ineffective unless
                // auto-commit is enabled (default 1s)
                Interval time.Duration
            }

            // The initial offset to use if no offset was previously committed.
            // Should be OffsetNewest or OffsetOldest. Defaults to OffsetNewest.
            Initial int64

            // The retention duration for committed offsets. If zero, disabled
            // (in which case the `offsets.retention.minutes` option on the
            // broker will be used).  Kafka only supports precision up to
            // milliseconds; nanoseconds will be truncated. Requires Kafka
            // broker version 0.9.0 or later.
            // (default is 0: disabled).
            Retention time.Duration

            Retry struct {
                // The total number of times to retry failing commit
                // requests during OffsetManager shutdown (default 3).
                Max int
            }
        }

        // IsolationLevel support 2 mode:
        // 	- use `ReadUncommitted` (default) to consume and return all messages in message channel
        //	- use `ReadCommitted` to hide messages that are part of an aborted transaction
        IsolationLevel IsolationLevel

        // Interceptors to be called just before the record is sent to the
        // messages channel. Interceptors allows to intercept and possible
        // mutate the message before they are returned to the client.
        // *ConsumerMessage modified by the first interceptor's OnConsume() is
        // passed to the second interceptor OnConsume(), and so on in the
        // interceptor chain.
        Interceptors []ConsumerInterceptor
    }

    // A user-provided string sent with every request to the brokers for logging,
    // debugging, and auditing purposes. Defaults to "sarama", but you should
    // probably set it to something specific to your application.
    ClientID string
    // A rack identifier for this client. This can be any string value which
    // indicates where this client is physically located.
    // It corresponds with the broker config 'broker.rack'
    RackID string
    // The number of events to buffer in internal and external channels. This
    // permits the producer and consumer to continue processing some messages
    // in the background while user code is working, greatly improving throughput.
    // Defaults to 256.
    ChannelBufferSize int
    // ApiVersionsRequest determines whether Sarama should send an
    // ApiVersionsRequest message to each broker as part of its initial
    // connection. This defaults to `true` to match the official Java client
    // and most 3rdparty ones.
    ApiVersionsRequest bool
    // The version of Kafka that Sarama will assume it is running against.
    // Defaults to the oldest supported stable version. Since Kafka provides
    // backwards-compatibility, setting it to a version older than you have
    // will not break anything, although it may prevent you from using the
    // latest features. Setting it to a version greater than you are actually
    // running may lead to random breakage.
    Version KafkaVersion
    // The registry to define metrics into.
    // Defaults to a local registry.
    // If you want to disable metrics gathering, set "metrics.UseNilMetrics" to "true"
    // prior to starting Sarama.
    // See Examples on how to use the metrics registry
    MetricRegistry metrics.Registry
}
```

### func NewConfig

```go
func NewConfig() *Config
```

NewConfig returns a new configuration instance with sane defaults.

### func \(\*Config\) Validate

```go
func (c *Config) Validate() error
```

Validate checks a Config instance. It will return a ConfigurationError if the specified values don't make sense.

## type ConfigEntry

```go
type ConfigEntry struct {
    Name      string
    Value     string
    ReadOnly  bool
    Default   bool
    Source    ConfigSource
    Sensitive bool
    Synonyms  []*ConfigSynonym
}
```

## type ConfigResource

```go
type ConfigResource struct {
    Type        ConfigResourceType
    Name        string
    ConfigNames []string
}
```

## type ConfigResourceType

ConfigResourceType is a type for resources that have configs.

```go
type ConfigResourceType int8
```

```go
const (
    // UnknownResource constant type
    UnknownResource ConfigResourceType = 0
    // TopicResource constant type
    TopicResource ConfigResourceType = 2
    // BrokerResource constant type
    BrokerResource ConfigResourceType = 4
    // BrokerLoggerResource constant type
    BrokerLoggerResource ConfigResourceType = 8
)
```

## type ConfigSource

```go
type ConfigSource int8
```

```go
const (
    SourceUnknown ConfigSource = iota
    SourceTopic
    SourceDynamicBroker
    SourceDynamicDefaultBroker
    SourceStaticBroker
    SourceDefault
)
```

### func \(ConfigSource\) String

```go
func (s ConfigSource) String() string
```

## type ConfigSynonym

```go
type ConfigSynonym struct {
    ConfigName  string
    ConfigValue string
    Source      ConfigSource
}
```

## type ConfigurationError

ConfigurationError is the type of error returned from a constructor \(e.g. NewClient, or NewConsumer\) when the specified configuration is invalid.

```go
type ConfigurationError string
```

### func \(ConfigurationError\) Error

```go
func (err ConfigurationError) Error() string
```

## type Consumer

Consumer manages PartitionConsumers which process Kafka messages from brokers. You MUST call Close\(\) on a consumer to avoid leaks, it will not be garbage\-collected automatically when it passes out of scope.

```go
type Consumer interface {
    // Topics returns the set of available topics as retrieved from the cluster
    // metadata. This method is the same as Client.Topics(), and is provided for
    // convenience.
    Topics() ([]string, error)

    // Partitions returns the sorted list of all partition IDs for the given topic.
    // This method is the same as Client.Partitions(), and is provided for convenience.
    Partitions(topic string) ([]int32, error)

    // ConsumePartition creates a PartitionConsumer on the given topic/partition with
    // the given offset. It will return an error if this Consumer is already consuming
    // on the given topic/partition. Offset can be a literal offset, or OffsetNewest
    // or OffsetOldest
    ConsumePartition(topic string, partition int32, offset int64) (PartitionConsumer, error)

    // HighWaterMarks returns the current high water marks for each topic and partition.
    // Consistency between partitions is not guaranteed since high water marks are updated separately.
    HighWaterMarks() map[string]map[int32]int64

    // Close shuts down the consumer. It must be called after all child
    // PartitionConsumers have already been closed.
    Close() error

    // Pause suspends fetching from the requested partitions. Future calls to the broker will not return any
    // records from these partitions until they have been resumed using Resume()/ResumeAll().
    // Note that this method does not affect partition subscription.
    // In particular, it does not cause a group rebalance when automatic assignment is used.
    Pause(topicPartitions map[string][]int32)

    // Resume resumes specified partitions which have been paused with Pause()/PauseAll().
    // New calls to the broker will return records from these partitions if there are any to be fetched.
    Resume(topicPartitions map[string][]int32)

    // Pause suspends fetching from all partitions. Future calls to the broker will not return any
    // records from these partitions until they have been resumed using Resume()/ResumeAll().
    // Note that this method does not affect partition subscription.
    // In particular, it does not cause a group rebalance when automatic assignment is used.
    PauseAll()

    // Resume resumes all partitions which have been paused with Pause()/PauseAll().
    // New calls to the broker will return records from these partitions if there are any to be fetched.
    ResumeAll()
}
```

### func NewConsumer

```go
func NewConsumer(addrs []string, config *Config) (Consumer, error)
```

NewConsumer creates a new consumer using the given broker addresses and configuration.

### func NewConsumerFromClient

```go
func NewConsumerFromClient(client Client) (Consumer, error)
```

NewConsumerFromClient creates a new consumer using the given client. It is still necessary to call Close\(\) on the underlying client when shutting down this consumer.

## type ConsumerError

ConsumerError is what is provided to the user when an error occurs. It wraps an error and includes the topic and partition.

```go
type ConsumerError struct {
    Topic     string
    Partition int32
    Err       error
}
```

### func \(ConsumerError\) Error

```go
func (ce ConsumerError) Error() string
```

### func \(ConsumerError\) Unwrap

```go
func (ce ConsumerError) Unwrap() error
```

## type ConsumerErrors

ConsumerErrors is a type that wraps a batch of errors and implements the Error interface. It can be returned from the PartitionConsumer's Close methods to avoid the need to manually drain errors when stopping.

```go
type ConsumerErrors []*ConsumerError
```

### func \(ConsumerErrors\) Error

```go
func (ce ConsumerErrors) Error() string
```

## type ConsumerGroup

ConsumerGroup is responsible for dividing up processing of topics and partitions over a collection of processes \(the members of the consumer group\).

```go
type ConsumerGroup interface {
    // Consume joins a cluster of consumers for a given list of topics and
    // starts a blocking ConsumerGroupSession through the ConsumerGroupHandler.
    //
    // The life-cycle of a session is represented by the following steps:
    //
    // 1. The consumers join the group (as explained in https://kafka.apache.org/documentation/#intro_consumers)
    //    and is assigned their "fair share" of partitions, aka 'claims'.
    // 2. Before processing starts, the handler's Setup() hook is called to notify the user
    //    of the claims and allow any necessary preparation or alteration of state.
    // 3. For each of the assigned claims the handler's ConsumeClaim() function is then called
    //    in a separate goroutine which requires it to be thread-safe. Any state must be carefully protected
    //    from concurrent reads/writes.
    // 4. The session will persist until one of the ConsumeClaim() functions exits. This can be either when the
    //    parent context is canceled or when a server-side rebalance cycle is initiated.
    // 5. Once all the ConsumeClaim() loops have exited, the handler's Cleanup() hook is called
    //    to allow the user to perform any final tasks before a rebalance.
    // 6. Finally, marked offsets are committed one last time before claims are released.
    //
    // Please note, that once a rebalance is triggered, sessions must be completed within
    // Config.Consumer.Group.Rebalance.Timeout. This means that ConsumeClaim() functions must exit
    // as quickly as possible to allow time for Cleanup() and the final offset commit. If the timeout
    // is exceeded, the consumer will be removed from the group by Kafka, which will cause offset
    // commit failures.
    // This method should be called inside an infinite loop, when a
    // server-side rebalance happens, the consumer session will need to be
    // recreated to get the new claims.
    Consume(ctx context.Context, topics []string, handler ConsumerGroupHandler) error

    // Errors returns a read channel of errors that occurred during the consumer life-cycle.
    // By default, errors are logged and not returned over this channel.
    // If you want to implement any custom error handling, set your config's
    // Consumer.Return.Errors setting to true, and read from this channel.
    Errors() <-chan error

    // Close stops the ConsumerGroup and detaches any running sessions. It is required to call
    // this function before the object passes out of scope, as it will otherwise leak memory.
    Close() error

    // Pause suspends fetching from the requested partitions. Future calls to the broker will not return any
    // records from these partitions until they have been resumed using Resume()/ResumeAll().
    // Note that this method does not affect partition subscription.
    // In particular, it does not cause a group rebalance when automatic assignment is used.
    Pause(partitions map[string][]int32)

    // Resume resumes specified partitions which have been paused with Pause()/PauseAll().
    // New calls to the broker will return records from these partitions if there are any to be fetched.
    Resume(partitions map[string][]int32)

    // Pause suspends fetching from all partitions. Future calls to the broker will not return any
    // records from these partitions until they have been resumed using Resume()/ResumeAll().
    // Note that this method does not affect partition subscription.
    // In particular, it does not cause a group rebalance when automatic assignment is used.
    PauseAll()

    // Resume resumes all partitions which have been paused with Pause()/PauseAll().
    // New calls to the broker will return records from these partitions if there are any to be fetched.
    ResumeAll()
}
```

### func NewConsumerGroup

```go
func NewConsumerGroup(addrs []string, groupID string, config *Config) (ConsumerGroup, error)
```

NewConsumerGroup creates a new consumer group the given broker addresses and configuration.

### func NewConsumerGroupFromClient

```go
func NewConsumerGroupFromClient(groupID string, client Client) (ConsumerGroup, error)
```

NewConsumerGroupFromClient creates a new consumer group using the given client. It is still necessary to call Close\(\) on the underlying client when shutting down this consumer. PLEASE NOTE: consumer groups can only re\-use but not share clients.

## type ConsumerGroupClaim

ConsumerGroupClaim processes Kafka messages from a given topic and partition within a consumer group.

```go
type ConsumerGroupClaim interface {
    // Topic returns the consumed topic name.
    Topic() string

    // Partition returns the consumed partition.
    Partition() int32

    // InitialOffset returns the initial offset that was used as a starting point for this claim.
    InitialOffset() int64

    // HighWaterMarkOffset returns the high water mark offset of the partition,
    // i.e. the offset that will be used for the next message that will be produced.
    // You can use this to determine how far behind the processing is.
    HighWaterMarkOffset() int64

    // Messages returns the read channel for the messages that are returned by
    // the broker. The messages channel will be closed when a new rebalance cycle
    // is due. You must finish processing and mark offsets within
    // Config.Consumer.Group.Session.Timeout before the topic/partition is eventually
    // re-assigned to another group member.
    Messages() <-chan *ConsumerMessage
}
```

## type ConsumerGroupHandler

ConsumerGroupHandler instances are used to handle individual topic/partition claims. It also provides hooks for your consumer group session life\-cycle and allow you to trigger logic before or after the consume loop\(s\).

PLEASE NOTE that handlers are likely be called from several goroutines concurrently, ensure that all state is safely protected against race conditions.

```go
type ConsumerGroupHandler interface {
    // Setup is run at the beginning of a new session, before ConsumeClaim.
    Setup(ConsumerGroupSession) error

    // Cleanup is run at the end of a session, once all ConsumeClaim goroutines have exited
    // but before the offsets are committed for the very last time.
    Cleanup(ConsumerGroupSession) error

    // ConsumeClaim must start a consumer loop of ConsumerGroupClaim's Messages().
    // Once the Messages() channel is closed, the Handler must finish its processing
    // loop and exit.
    ConsumeClaim(ConsumerGroupSession, ConsumerGroupClaim) error
}
```

## type ConsumerGroupMemberAssignment

ConsumerGroupMemberAssignment holds the member assignment for a consume group https://github.com/apache/kafka/blob/trunk/clients/src/main/resources/common/message/ConsumerProtocolAssignment.json

```go
type ConsumerGroupMemberAssignment struct {
    Version  int16
    Topics   map[string][]int32
    UserData []byte
}
```

## type ConsumerGroupMemberMetadata

ConsumerGroupMemberMetadata holds the metadata for consumer group https://github.com/apache/kafka/blob/trunk/clients/src/main/resources/common/message/ConsumerProtocolSubscription.json

```go
type ConsumerGroupMemberMetadata struct {
    Version         int16
    Topics          []string
    UserData        []byte
    OwnedPartitions []*OwnedPartition
}
```

## type ConsumerGroupSession

ConsumerGroupSession represents a consumer group member session.

```go
type ConsumerGroupSession interface {
    // Claims returns information about the claimed partitions by topic.
    Claims() map[string][]int32

    // MemberID returns the cluster member ID.
    MemberID() string

    // GenerationID returns the current generation ID.
    GenerationID() int32

    // MarkOffset marks the provided offset, alongside a metadata string
    // that represents the state of the partition consumer at that point in time. The
    // metadata string can be used by another consumer to restore that state, so it
    // can resume consumption.
    //
    // To follow upstream conventions, you are expected to mark the offset of the
    // next message to read, not the last message read. Thus, when calling `MarkOffset`
    // you should typically add one to the offset of the last consumed message.
    //
    // Note: calling MarkOffset does not necessarily commit the offset to the backend
    // store immediately for efficiency reasons, and it may never be committed if
    // your application crashes. This means that you may end up processing the same
    // message twice, and your processing should ideally be idempotent.
    MarkOffset(topic string, partition int32, offset int64, metadata string)

    // Commit the offset to the backend
    //
    // Note: calling Commit performs a blocking synchronous operation.
    Commit()

    // ResetOffset resets to the provided offset, alongside a metadata string that
    // represents the state of the partition consumer at that point in time. Reset
    // acts as a counterpart to MarkOffset, the difference being that it allows to
    // reset an offset to an earlier or smaller value, where MarkOffset only
    // allows incrementing the offset. cf MarkOffset for more details.
    ResetOffset(topic string, partition int32, offset int64, metadata string)

    // MarkMessage marks a message as consumed.
    MarkMessage(msg *ConsumerMessage, metadata string)

    // Context returns the session context.
    Context() context.Context
}
```

## type ConsumerInterceptor

ConsumerInterceptor allows you to intercept \(and possibly mutate\) the records received by the consumer before they are sent to the messages channel. https://cwiki.apache.org/confluence/display/KAFKA/KIP-42%3A+Add+Producer+and+Consumer+Interceptors#KIP42:AddProducerandConsumerInterceptors-Motivation

```go
type ConsumerInterceptor interface {

    // OnConsume is called when the consumed message is intercepted. Please
    // avoid modifying the message until it's safe to do so, as this is _not_ a
    // copy of the message.
    OnConsume(*ConsumerMessage)
}
```

## type ConsumerMessage

ConsumerMessage encapsulates a Kafka message returned by the consumer.

```go
type ConsumerMessage struct {
    Headers        []*RecordHeader // only set if kafka is version 0.11+
    Timestamp      time.Time       // only set if kafka is version 0.10+, inner message timestamp
    BlockTimestamp time.Time       // only set if kafka is version 0.10+, outer (compressed) block timestamp

    Key, Value []byte
    Topic      string
    Partition  int32
    Offset     int64
}
```

## type ConsumerMetadataRequest

ConsumerMetadataRequest is used for metadata requests

```go
type ConsumerMetadataRequest struct {
    ConsumerGroup string
}
```

## type ConsumerMetadataResponse

ConsumerMetadataResponse holds the response for a consumer group meta data requests

```go
type ConsumerMetadataResponse struct {
    Err             KError
    Coordinator     *Broker
    CoordinatorID   int32  // deprecated: use Coordinator.ID()
    CoordinatorHost string // deprecated: use Coordinator.Addr()
    CoordinatorPort int32  // deprecated: use Coordinator.Addr()
}
```

## type ControlRecord

Control records are returned as a record by fetchRequest However unlike "normal" records, they mean nothing application wise. They only serve internal logic for supporting transactions.

```go
type ControlRecord struct {
    Version          int16
    CoordinatorEpoch int32
    Type             ControlRecordType
}
```

## type ControlRecordType

ControlRecordType ...

```go
type ControlRecordType int
```

```go
const (
    // ControlRecordAbort is a control record for abort
    ControlRecordAbort ControlRecordType = iota
    // ControlRecordCommit is a control record for commit
    ControlRecordCommit
    // ControlRecordUnknown is a control record of unknown type
    ControlRecordUnknown
)
```

## type CoordinatorType

```go
type CoordinatorType int8
```

```go
const (
    CoordinatorGroup CoordinatorType = iota
    CoordinatorTransaction
)
```

## type CreateAclsRequest

CreateAclsRequest is an acl creation request

```go
type CreateAclsRequest struct {
    Version      int16
    AclCreations []*AclCreation
}
```

## type CreateAclsResponse

CreateAclsResponse is a an acl response creation type

```go
type CreateAclsResponse struct {
    ThrottleTime         time.Duration
    AclCreationResponses []*AclCreationResponse
}
```

## type CreatePartitionsRequest

```go
type CreatePartitionsRequest struct {
    TopicPartitions map[string]*TopicPartition
    Timeout         time.Duration
    ValidateOnly    bool
}
```

## type CreatePartitionsResponse

```go
type CreatePartitionsResponse struct {
    ThrottleTime         time.Duration
    TopicPartitionErrors map[string]*TopicPartitionError
}
```

## type CreateTopicsRequest

```go
type CreateTopicsRequest struct {
    Version int16

    TopicDetails map[string]*TopicDetail
    Timeout      time.Duration
    ValidateOnly bool
}
```

## type CreateTopicsResponse

```go
type CreateTopicsResponse struct {
    Version      int16
    ThrottleTime time.Duration
    TopicErrors  map[string]*TopicError
}
```

## type DeleteAclsRequest

DeleteAclsRequest is a delete acl request

```go
type DeleteAclsRequest struct {
    Version int
    Filters []*AclFilter
}
```

## type DeleteAclsResponse

DeleteAclsResponse is a delete acl response

```go
type DeleteAclsResponse struct {
    Version         int16
    ThrottleTime    time.Duration
    FilterResponses []*FilterResponse
}
```

## type DeleteGroupsRequest

```go
type DeleteGroupsRequest struct {
    Groups []string
}
```

### func \(\*DeleteGroupsRequest\) AddGroup

```go
func (r *DeleteGroupsRequest) AddGroup(group string)
```

## type DeleteGroupsResponse

```go
type DeleteGroupsResponse struct {
    ThrottleTime    time.Duration
    GroupErrorCodes map[string]KError
}
```

## type DeleteOffsetsRequest

```go
type DeleteOffsetsRequest struct {
    Group string
    // contains filtered or unexported fields
}
```

### func \(\*DeleteOffsetsRequest\) AddPartition

```go
func (r *DeleteOffsetsRequest) AddPartition(topic string, partitionID int32)
```

## type DeleteOffsetsResponse

```go
type DeleteOffsetsResponse struct {
    // The top-level error code, or 0 if there was no error.
    ErrorCode    KError
    ThrottleTime time.Duration
    // The responses for each partition of the topics.
    Errors map[string]map[int32]KError
}
```

### func \(\*DeleteOffsetsResponse\) AddError

```go
func (r *DeleteOffsetsResponse) AddError(topic string, partition int32, errorCode KError)
```

## type DeleteRecordsRequest

```go
type DeleteRecordsRequest struct {
    Topics  map[string]*DeleteRecordsRequestTopic
    Timeout time.Duration
}
```

## type DeleteRecordsRequestTopic

```go
type DeleteRecordsRequestTopic struct {
    PartitionOffsets map[int32]int64 // partition => offset
}
```

## type DeleteRecordsResponse

```go
type DeleteRecordsResponse struct {
    Version      int16
    ThrottleTime time.Duration
    Topics       map[string]*DeleteRecordsResponseTopic
}
```

## type DeleteRecordsResponsePartition

```go
type DeleteRecordsResponsePartition struct {
    LowWatermark int64
    Err          KError
}
```

## type DeleteRecordsResponseTopic

```go
type DeleteRecordsResponseTopic struct {
    Partitions map[int32]*DeleteRecordsResponsePartition
}
```

## type DeleteTopicsRequest

```go
type DeleteTopicsRequest struct {
    Version int16
    Topics  []string
    Timeout time.Duration
}
```

## type DeleteTopicsResponse

```go
type DeleteTopicsResponse struct {
    Version         int16
    ThrottleTime    time.Duration
    TopicErrorCodes map[string]KError
}
```

## type DescribeAclsRequest

DescribeAclsRequest is a secribe acl request type

```go
type DescribeAclsRequest struct {
    Version int
    AclFilter
}
```

## type DescribeAclsResponse

DescribeAclsResponse is a describe acl response type

```go
type DescribeAclsResponse struct {
    Version      int16
    ThrottleTime time.Duration
    Err          KError
    ErrMsg       *string
    ResourceAcls []*ResourceAcls
}
```

## type DescribeClientQuotasEntry

```go
type DescribeClientQuotasEntry struct {
    Entity []QuotaEntityComponent // The quota entity description.
    Values map[string]float64     // The quota values for the entity.
}
```

## type DescribeClientQuotasRequest

A filter to be applied to matching client quotas. Components: the components to filter on Strict: whether the filter only includes specified components

```go
type DescribeClientQuotasRequest struct {
    Components []QuotaFilterComponent
    Strict     bool
}
```

## type DescribeClientQuotasResponse

```go
type DescribeClientQuotasResponse struct {
    ThrottleTime time.Duration               // The duration in milliseconds for which the request was throttled due to a quota violation, or zero if the request did not violate any quota.
    ErrorCode    KError                      // The error code, or `0` if the quota description succeeded.
    ErrorMsg     *string                     // The error message, or `null` if the quota description succeeded.
    Entries      []DescribeClientQuotasEntry // A result entry.
}
```

## type DescribeConfigsRequest

```go
type DescribeConfigsRequest struct {
    Version         int16
    Resources       []*ConfigResource
    IncludeSynonyms bool
}
```

## type DescribeConfigsResponse

```go
type DescribeConfigsResponse struct {
    Version      int16
    ThrottleTime time.Duration
    Resources    []*ResourceResponse
}
```

## type DescribeGroupsRequest

```go
type DescribeGroupsRequest struct {
    Version                     int16
    Groups                      []string
    IncludeAuthorizedOperations bool
}
```

### func \(\*DescribeGroupsRequest\) AddGroup

```go
func (r *DescribeGroupsRequest) AddGroup(group string)
```

## type DescribeGroupsResponse

```go
type DescribeGroupsResponse struct {
    // Version defines the protocol version to use for encode and decode
    Version int16
    // ThrottleTimeMs contains the duration in milliseconds for which the
    // request was throttled due to a quota violation, or zero if the request
    // did not violate any quota.
    ThrottleTimeMs int32
    // Groups contains each described group.
    Groups []*GroupDescription
}
```

## type DescribeLogDirsRequest

DescribeLogDirsRequest is a describe request to get partitions' log size

```go
type DescribeLogDirsRequest struct {
    // Version 0 and 1 are equal
    // The version number is bumped to indicate that on quota violation brokers send out responses before throttling.
    Version int16

    // If this is an empty array, all topics will be queried
    DescribeTopics []DescribeLogDirsRequestTopic
}
```

## type DescribeLogDirsRequestTopic

DescribeLogDirsRequestTopic is a describe request about the log dir of one or more partitions within a Topic

```go
type DescribeLogDirsRequestTopic struct {
    Topic        string
    PartitionIDs []int32
}
```

## type DescribeLogDirsResponse

```go
type DescribeLogDirsResponse struct {
    ThrottleTime time.Duration

    // Version 0 and 1 are equal
    // The version number is bumped to indicate that on quota violation brokers send out responses before throttling.
    Version int16

    LogDirs []DescribeLogDirsResponseDirMetadata
}
```

## type DescribeLogDirsResponseDirMetadata

```go
type DescribeLogDirsResponseDirMetadata struct {
    ErrorCode KError

    // The absolute log directory path
    Path   string
    Topics []DescribeLogDirsResponseTopic
}
```

## type DescribeLogDirsResponsePartition

DescribeLogDirsResponsePartition describes a partition's log directory

```go
type DescribeLogDirsResponsePartition struct {
    PartitionID int32

    // The size of the log segments of the partition in bytes.
    Size int64

    // The lag of the log's LEO w.r.t. partition's HW (if it is the current log for the partition) or
    // current replica's LEO (if it is the future log for the partition)
    OffsetLag int64

    // True if this log is created by AlterReplicaLogDirsRequest and will replace the current log of
    // the replica in the future.
    IsTemporary bool
}
```

## type DescribeLogDirsResponseTopic

DescribeLogDirsResponseTopic contains a topic's partitions descriptions

```go
type DescribeLogDirsResponseTopic struct {
    Topic      string
    Partitions []DescribeLogDirsResponsePartition
}
```

## type DescribeUserScramCredentialsRequest

DescribeUserScramCredentialsRequest is a request to get list of SCRAM user names

```go
type DescribeUserScramCredentialsRequest struct {
    // Version 0 is currently only supported
    Version int16

    // If this is an empty array, all users will be queried
    DescribeUsers []DescribeUserScramCredentialsRequestUser
}
```

## type DescribeUserScramCredentialsRequestUser

DescribeUserScramCredentialsRequestUser is a describe request about specific user name

```go
type DescribeUserScramCredentialsRequestUser struct {
    Name string
}
```

## type DescribeUserScramCredentialsResponse

```go
type DescribeUserScramCredentialsResponse struct {
    // Version 0 is currently only supported
    Version int16

    ThrottleTime time.Duration

    ErrorCode    KError
    ErrorMessage *string

    Results []*DescribeUserScramCredentialsResult
}
```

## type DescribeUserScramCredentialsResult

```go
type DescribeUserScramCredentialsResult struct {
    User string

    ErrorCode    KError
    ErrorMessage *string

    CredentialInfos []*UserScramCredentialsResponseInfo
}
```

## type DynamicConsistencyPartitioner

DynamicConsistencyPartitioner can optionally be implemented by Partitioners in order to allow more flexibility than is originally allowed by the RequiresConsistency method in the Partitioner interface. This allows partitioners to require consistency sometimes, but not all times. It's useful for, e.g., the HashPartitioner, which does not require consistency if the message key is nil.

```go
type DynamicConsistencyPartitioner interface {
    Partitioner

    // MessageRequiresConsistency is similar to Partitioner.RequiresConsistency,
    // but takes in the message being partitioned so that the partitioner can
    // make a per-message determination.
    MessageRequiresConsistency(message *ProducerMessage) bool
}
```

## type Encoder

Encoder is a simple interface for any type that can be encoded as an array of bytes in order to be sent as the key or value of a Kafka message. Length\(\) is provided as an optimization, and must return the same as len\(\) on the result of Encode\(\).

```go
type Encoder interface {
    Encode() ([]byte, error)
    Length() int
}
```

## type EndTxnRequest

```go
type EndTxnRequest struct {
    TransactionalID   string
    ProducerID        int64
    ProducerEpoch     int16
    TransactionResult bool
}
```

## type EndTxnResponse

```go
type EndTxnResponse struct {
    ThrottleTime time.Duration
    Err          KError
}
```

## type FetchRequest

FetchRequest \(API key 1\) will fetch Kafka messages. Version 3 introduced the MaxBytes field. See https://issues.apache.org/jira/browse/KAFKA-2063 for a discussion of the issues leading up to that.  The KIP is at https://cwiki.apache.org/confluence/display/KAFKA/KIP-74%3A+Add+Fetch+Response+Size+Limit+in+Bytes

```go
type FetchRequest struct {
    // Version defines the protocol version to use for encode and decode
    Version int16
    // ReplicaID contains the broker ID of the follower, of -1 if this request
    // is from a consumer.
    // ReplicaID int32
    // MaxWaitTime contains the maximum time in milliseconds to wait for the response.
    MaxWaitTime int32
    // MinBytes contains the minimum bytes to accumulate in the response.
    MinBytes int32
    // MaxBytes contains the maximum bytes to fetch.  See KIP-74 for cases
    // where this limit may not be honored.
    MaxBytes int32
    // Isolation contains a This setting controls the visibility of
    // transactional records. Using READ_UNCOMMITTED (isolation_level = 0)
    // makes all records visible. With READ_COMMITTED (isolation_level = 1),
    // non-transactional and COMMITTED transactional records are visible. To be
    // more concrete, READ_COMMITTED returns all data from offsets smaller than
    // the current LSO (last stable offset), and enables the inclusion of the
    // list of aborted transactions in the result, which allows consumers to
    // discard ABORTED transactional records
    Isolation IsolationLevel
    // SessionID contains the fetch session ID.
    SessionID int32
    // SessionEpoch contains the epoch of the partition leader as known to the
    // follower replica or a consumer.
    SessionEpoch int32

    // RackID contains a Rack ID of the consumer making this request
    RackID string
    // contains filtered or unexported fields
}
```

### func \(\*FetchRequest\) AddBlock

```go
func (r *FetchRequest) AddBlock(topic string, partitionID int32, fetchOffset int64, maxBytes int32)
```

## type FetchResponse

```go
type FetchResponse struct {
    // Version defines the protocol version to use for encode and decode
    Version int16
    // ThrottleTime contains the duration in milliseconds for which the request
    // was throttled due to a quota violation, or zero if the request did not
    // violate any quota.
    ThrottleTime time.Duration
    // ErrorCode contains the top level response error code.
    ErrorCode int16
    // SessionID contains the fetch session ID, or 0 if this is not part of a fetch session.
    SessionID int32
    // Blocks contains the response topics.
    Blocks map[string]map[int32]*FetchResponseBlock

    LogAppendTime bool
    Timestamp     time.Time
}
```

### func \(\*FetchResponse\) AddControlRecord

```go
func (r *FetchResponse) AddControlRecord(topic string, partition int32, offset int64, producerID int64, recordType ControlRecordType)
```

### func \(\*FetchResponse\) AddControlRecordWithTimestamp

```go
func (r *FetchResponse) AddControlRecordWithTimestamp(topic string, partition int32, offset int64, producerID int64, recordType ControlRecordType, timestamp time.Time)
```

### func \(\*FetchResponse\) AddError

```go
func (r *FetchResponse) AddError(topic string, partition int32, err KError)
```

### func \(\*FetchResponse\) AddMessage

```go
func (r *FetchResponse) AddMessage(topic string, partition int32, key, value Encoder, offset int64)
```

### func \(\*FetchResponse\) AddMessageWithTimestamp

```go
func (r *FetchResponse) AddMessageWithTimestamp(topic string, partition int32, key, value Encoder, offset int64, timestamp time.Time, version int8)
```

### func \(\*FetchResponse\) AddRecord

```go
func (r *FetchResponse) AddRecord(topic string, partition int32, key, value Encoder, offset int64)
```

### func \(\*FetchResponse\) AddRecordBatch

```go
func (r *FetchResponse) AddRecordBatch(topic string, partition int32, key, value Encoder, offset int64, producerID int64, isTransactional bool)
```

### func \(\*FetchResponse\) AddRecordBatchWithTimestamp

```go
func (r *FetchResponse) AddRecordBatchWithTimestamp(topic string, partition int32, key, value Encoder, offset int64, producerID int64, isTransactional bool, timestamp time.Time)
```

AddRecordBatchWithTimestamp is similar to AddRecordWithTimestamp But instead of appending 1 record to a batch, it append a new batch containing 1 record to the fetchResponse Since transaction are handled on batch level \(the whole batch is either committed or aborted\), use this to test transactions

### func \(\*FetchResponse\) AddRecordWithTimestamp

```go
func (r *FetchResponse) AddRecordWithTimestamp(topic string, partition int32, key, value Encoder, offset int64, timestamp time.Time)
```

### func \(\*FetchResponse\) GetBlock

```go
func (r *FetchResponse) GetBlock(topic string, partition int32) *FetchResponseBlock
```

### func \(\*FetchResponse\) SetLastOffsetDelta

```go
func (r *FetchResponse) SetLastOffsetDelta(topic string, partition int32, offset int32)
```

### func \(\*FetchResponse\) SetLastStableOffset

```go
func (r *FetchResponse) SetLastStableOffset(topic string, partition int32, offset int64)
```

## type FetchResponseBlock

```go
type FetchResponseBlock struct {
    // Err contains the error code, or 0 if there was no fetch error.
    Err KError
    // HighWatermarkOffset contains the current high water mark.
    HighWaterMarkOffset int64
    // LastStableOffset contains the last stable offset (or LSO) of the
    // partition. This is the last offset such that the state of all
    // transactional records prior to this offset have been decided (ABORTED or
    // COMMITTED)
    LastStableOffset       int64
    LastRecordsBatchOffset *int64
    // LogStartOffset contains the current log start offset.
    LogStartOffset int64
    // AbortedTransactions contains the aborted transactions.
    AbortedTransactions []*AbortedTransaction
    // PreferredReadReplica contains the preferred read replica for the
    // consumer to use on its next fetch request
    PreferredReadReplica int32
    // RecordsSet contains the record data.
    RecordsSet []*Records

    Partial bool
    Records *Records // deprecated: use FetchResponseBlock.RecordsSet
}
```

## type FilterResponse

FilterResponse is a filter response type

```go
type FilterResponse struct {
    Err          KError
    ErrMsg       *string
    MatchingAcls []*MatchingAcl
}
```

## type FindCoordinatorRequest

```go
type FindCoordinatorRequest struct {
    Version         int16
    CoordinatorKey  string
    CoordinatorType CoordinatorType
}
```

## type FindCoordinatorResponse

```go
type FindCoordinatorResponse struct {
    Version      int16
    ThrottleTime time.Duration
    Err          KError
    ErrMsg       *string
    Coordinator  *Broker
}
```

## type GSSAPIConfig

```go
type GSSAPIConfig struct {
    AuthType           int
    KeyTabPath         string
    KerberosConfigPath string
    ServiceName        string
    Username           string
    Password           string
    Realm              string
    DisablePAFXFAST    bool
}
```

## type GSSAPIKerberosAuth

```go
type GSSAPIKerberosAuth struct {
    Config *GSSAPIConfig

    NewKerberosClientFunc func(config *GSSAPIConfig) (KerberosClient, error)
    // contains filtered or unexported fields
}
```

### func \(\*GSSAPIKerberosAuth\) Authorize

```go
func (krbAuth *GSSAPIKerberosAuth) Authorize(broker *Broker) error
```

This does the handshake for authorization

## type GSSApiHandlerFunc

```go
type GSSApiHandlerFunc func([]byte) []byte
```

## type GroupDescription

GroupDescription contains each described group.

```go
type GroupDescription struct {
    // Version defines the protocol version to use for encode and decode
    Version int16
    // Err contains the describe error as the KError type.
    Err KError
    // ErrorCode contains the describe error, or 0 if there was no error.
    ErrorCode int16
    // GroupId contains the group ID string.
    GroupId string
    // State contains the group state string, or the empty string.
    State string
    // ProtocolType contains the group protocol type, or the empty string.
    ProtocolType string
    // Protocol contains the group protocol data, or the empty string.
    Protocol string
    // Members contains the group members.
    Members map[string]*GroupMemberDescription
    // AuthorizedOperations contains a 32-bit bitfield to represent authorized
    // operations for this group.
    AuthorizedOperations int32
}
```

## type GroupMember

```go
type GroupMember struct {
    MemberId        string
    GroupInstanceId *string
    Metadata        []byte
}
```

## type GroupMemberDescription

GroupMemberDescription contains the group members.

```go
type GroupMemberDescription struct {
    // Version defines the protocol version to use for encode and decode
    Version int16
    // MemberId contains the member ID assigned by the group coordinator.
    MemberId string
    // GroupInstanceId contains the unique identifier of the consumer instance
    // provided by end user.
    GroupInstanceId *string
    // ClientId contains the client ID used in the member's latest join group
    // request.
    ClientId string
    // ClientHost contains the client host.
    ClientHost string
    // MemberMetadata contains the metadata corresponding to the current group
    // protocol in use.
    MemberMetadata []byte
    // MemberAssignment contains the current assignment provided by the group
    // leader.
    MemberAssignment []byte
}
```

### func \(\*GroupMemberDescription\) GetMemberAssignment

```go
func (gmd *GroupMemberDescription) GetMemberAssignment() (*ConsumerGroupMemberAssignment, error)
```

### func \(\*GroupMemberDescription\) GetMemberMetadata

```go
func (gmd *GroupMemberDescription) GetMemberMetadata() (*ConsumerGroupMemberMetadata, error)
```

## type GroupProtocol

```go
type GroupProtocol struct {
    Name     string
    Metadata []byte
}
```

## type HashPartitionerOption

HashPartitionerOption lets you modify default values of the partitioner

```go
type HashPartitionerOption func(*hashPartitioner)
```

### func WithAbsFirst

```go
func WithAbsFirst() HashPartitionerOption
```

WithAbsFirst means that the partitioner handles absolute values in the same way as the reference Java implementation

### func WithCustomFallbackPartitioner

```go
func WithCustomFallbackPartitioner(randomHP Partitioner) HashPartitionerOption
```

WithCustomFallbackPartitioner lets you specify what HashPartitioner should be used in case a Distribution Key is empty

### func WithCustomHashFunction

```go
func WithCustomHashFunction(hasher func() hash.Hash32) HashPartitionerOption
```

WithCustomHashFunction lets you specify what hash function to use for the partitioning

## type HeartbeatRequest

```go
type HeartbeatRequest struct {
    Version         int16
    GroupId         string
    GenerationId    int32
    MemberId        string
    GroupInstanceId *string
}
```

## type HeartbeatResponse

```go
type HeartbeatResponse struct {
    Version      int16
    ThrottleTime int32
    Err          KError
}
```

## type IncrementalAlterConfigsEntry

```go
type IncrementalAlterConfigsEntry struct {
    Operation IncrementalAlterConfigsOperation
    Value     *string
}
```

## type IncrementalAlterConfigsOperation

```go
type IncrementalAlterConfigsOperation int8
```

```go
const (
    IncrementalAlterConfigsOperationSet IncrementalAlterConfigsOperation = iota
    IncrementalAlterConfigsOperationDelete
    IncrementalAlterConfigsOperationAppend
    IncrementalAlterConfigsOperationSubtract
)
```

## type IncrementalAlterConfigsRequest

IncrementalAlterConfigsRequest is an incremental alter config request type

```go
type IncrementalAlterConfigsRequest struct {
    Resources    []*IncrementalAlterConfigsResource
    ValidateOnly bool
}
```

## type IncrementalAlterConfigsResource

```go
type IncrementalAlterConfigsResource struct {
    Type          ConfigResourceType
    Name          string
    ConfigEntries map[string]IncrementalAlterConfigsEntry
}
```

## type IncrementalAlterConfigsResponse

IncrementalAlterConfigsResponse is a response type for incremental alter config

```go
type IncrementalAlterConfigsResponse struct {
    ThrottleTime time.Duration
    Resources    []*AlterConfigsResourceResponse
}
```

## type InitProducerIDRequest

```go
type InitProducerIDRequest struct {
    Version            int16
    TransactionalID    *string
    TransactionTimeout time.Duration
    ProducerID         int64
    ProducerEpoch      int16
}
```

## type InitProducerIDResponse

```go
type InitProducerIDResponse struct {
    ThrottleTime  time.Duration
    Err           KError
    Version       int16
    ProducerID    int64
    ProducerEpoch int16
}
```

## type IsolationLevel

```go
type IsolationLevel int8
```

```go
const (
    ReadUncommitted IsolationLevel = iota
    ReadCommitted
)
```

## type JoinGroupRequest

```go
type JoinGroupRequest struct {
    Version               int16
    GroupId               string
    SessionTimeout        int32
    RebalanceTimeout      int32
    MemberId              string
    GroupInstanceId       *string
    ProtocolType          string
    GroupProtocols        map[string][]byte // deprecated; use OrderedGroupProtocols
    OrderedGroupProtocols []*GroupProtocol
}
```

### func \(\*JoinGroupRequest\) AddGroupProtocol

```go
func (r *JoinGroupRequest) AddGroupProtocol(name string, metadata []byte)
```

### func \(\*JoinGroupRequest\) AddGroupProtocolMetadata

```go
func (r *JoinGroupRequest) AddGroupProtocolMetadata(name string, metadata *ConsumerGroupMemberMetadata) error
```

## type JoinGroupResponse

```go
type JoinGroupResponse struct {
    Version       int16
    ThrottleTime  int32
    Err           KError
    GenerationId  int32
    GroupProtocol string
    LeaderId      string
    MemberId      string
    Members       []GroupMember
}
```

### func \(\*JoinGroupResponse\) GetMembers

```go
func (r *JoinGroupResponse) GetMembers() (map[string]ConsumerGroupMemberMetadata, error)
```

## type KError

KError is the type of error that can be returned directly by the Kafka broker. See https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol#AGuideToTheKafkaProtocol-ErrorCodes

```go
type KError int16
```

Numeric error codes returned by the Kafka server.

```go
const (
    ErrNoError                            KError = 0
    ErrUnknown                            KError = -1
    ErrOffsetOutOfRange                   KError = 1
    ErrInvalidMessage                     KError = 2
    ErrUnknownTopicOrPartition            KError = 3
    ErrInvalidMessageSize                 KError = 4
    ErrLeaderNotAvailable                 KError = 5
    ErrNotLeaderForPartition              KError = 6
    ErrRequestTimedOut                    KError = 7
    ErrBrokerNotAvailable                 KError = 8
    ErrReplicaNotAvailable                KError = 9
    ErrMessageSizeTooLarge                KError = 10
    ErrStaleControllerEpochCode           KError = 11
    ErrOffsetMetadataTooLarge             KError = 12
    ErrNetworkException                   KError = 13
    ErrOffsetsLoadInProgress              KError = 14
    ErrConsumerCoordinatorNotAvailable    KError = 15
    ErrNotCoordinatorForConsumer          KError = 16
    ErrInvalidTopic                       KError = 17
    ErrMessageSetSizeTooLarge             KError = 18
    ErrNotEnoughReplicas                  KError = 19
    ErrNotEnoughReplicasAfterAppend       KError = 20
    ErrInvalidRequiredAcks                KError = 21
    ErrIllegalGeneration                  KError = 22
    ErrInconsistentGroupProtocol          KError = 23
    ErrInvalidGroupId                     KError = 24
    ErrUnknownMemberId                    KError = 25
    ErrInvalidSessionTimeout              KError = 26
    ErrRebalanceInProgress                KError = 27
    ErrInvalidCommitOffsetSize            KError = 28
    ErrTopicAuthorizationFailed           KError = 29
    ErrGroupAuthorizationFailed           KError = 30
    ErrClusterAuthorizationFailed         KError = 31
    ErrInvalidTimestamp                   KError = 32
    ErrUnsupportedSASLMechanism           KError = 33
    ErrIllegalSASLState                   KError = 34
    ErrUnsupportedVersion                 KError = 35
    ErrTopicAlreadyExists                 KError = 36
    ErrInvalidPartitions                  KError = 37
    ErrInvalidReplicationFactor           KError = 38
    ErrInvalidReplicaAssignment           KError = 39
    ErrInvalidConfig                      KError = 40
    ErrNotController                      KError = 41
    ErrInvalidRequest                     KError = 42
    ErrUnsupportedForMessageFormat        KError = 43
    ErrPolicyViolation                    KError = 44
    ErrOutOfOrderSequenceNumber           KError = 45
    ErrDuplicateSequenceNumber            KError = 46
    ErrInvalidProducerEpoch               KError = 47
    ErrInvalidTxnState                    KError = 48
    ErrInvalidProducerIDMapping           KError = 49
    ErrInvalidTransactionTimeout          KError = 50
    ErrConcurrentTransactions             KError = 51
    ErrTransactionCoordinatorFenced       KError = 52
    ErrTransactionalIDAuthorizationFailed KError = 53
    ErrSecurityDisabled                   KError = 54
    ErrOperationNotAttempted              KError = 55
    ErrKafkaStorageError                  KError = 56
    ErrLogDirNotFound                     KError = 57
    ErrSASLAuthenticationFailed           KError = 58
    ErrUnknownProducerID                  KError = 59
    ErrReassignmentInProgress             KError = 60
    ErrDelegationTokenAuthDisabled        KError = 61
    ErrDelegationTokenNotFound            KError = 62
    ErrDelegationTokenOwnerMismatch       KError = 63
    ErrDelegationTokenRequestNotAllowed   KError = 64
    ErrDelegationTokenAuthorizationFailed KError = 65
    ErrDelegationTokenExpired             KError = 66
    ErrInvalidPrincipalType               KError = 67
    ErrNonEmptyGroup                      KError = 68
    ErrGroupIDNotFound                    KError = 69
    ErrFetchSessionIDNotFound             KError = 70
    ErrInvalidFetchSessionEpoch           KError = 71
    ErrListenerNotFound                   KError = 72
    ErrTopicDeletionDisabled              KError = 73
    ErrFencedLeaderEpoch                  KError = 74
    ErrUnknownLeaderEpoch                 KError = 75
    ErrUnsupportedCompressionType         KError = 76
    ErrStaleBrokerEpoch                   KError = 77
    ErrOffsetNotAvailable                 KError = 78
    ErrMemberIdRequired                   KError = 79
    ErrPreferredLeaderNotAvailable        KError = 80
    ErrGroupMaxSizeReached                KError = 81
    ErrFencedInstancedId                  KError = 82
    ErrEligibleLeadersNotAvailable        KError = 83
    ErrElectionNotNeeded                  KError = 84
    ErrNoReassignmentInProgress           KError = 85
    ErrGroupSubscribedToTopic             KError = 86
    ErrInvalidRecord                      KError = 87
    ErrUnstableOffsetCommit               KError = 88
    ErrThrottlingQuotaExceeded            KError = 89
    ErrProducerFenced                     KError = 90
)
```

### func \(KError\) Error

```go
func (err KError) Error() string
```

## type KafkaGSSAPIHandler

```go
type KafkaGSSAPIHandler struct {
    // contains filtered or unexported fields
}
```

### func \(\*KafkaGSSAPIHandler\) MockKafkaGSSAPI

```go
func (h *KafkaGSSAPIHandler) MockKafkaGSSAPI(buffer []byte) []byte
```

## type KafkaVersion

KafkaVersion instances represent versions of the upstream Kafka broker.

```go
type KafkaVersion struct {
    // contains filtered or unexported fields
}
```

### func ParseKafkaVersion

```go
func ParseKafkaVersion(s string) (KafkaVersion, error)
```

ParseKafkaVersion parses and returns kafka version or error from a string

### func \(KafkaVersion\) IsAtLeast

```go
func (v KafkaVersion) IsAtLeast(other KafkaVersion) bool
```

IsAtLeast return true if and only if the version it is called on is greater than or equal to the version passed in:

```
V1.IsAtLeast(V2) // false
V2.IsAtLeast(V1) // true
```

### func \(KafkaVersion\) String

```go
func (v KafkaVersion) String() string
```

## type KerberosClient

```go
type KerberosClient interface {
    Login() error
    GetServiceTicket(spn string) (messages.Ticket, types.EncryptionKey, error)
    Domain() string
    CName() types.PrincipalName
    Destroy()
}
```

### func NewKerberosClient

```go
func NewKerberosClient(config *GSSAPIConfig) (KerberosClient, error)
```

NewKerberosClient creates kerberos client used to obtain TGT and TGS tokens. It uses pure go Kerberos 5 solution \(RFC\-4121 and RFC\-4120\). uses gokrb5 library underlying which is a pure go kerberos client with some GSS\-API capabilities.

## type KerberosGoKrb5Client

```go
type KerberosGoKrb5Client struct {
    krb5client.Client
}
```

### func \(\*KerberosGoKrb5Client\) CName

```go
func (c *KerberosGoKrb5Client) CName() types.PrincipalName
```

### func \(\*KerberosGoKrb5Client\) Domain

```go
func (c *KerberosGoKrb5Client) Domain() string
```

## type LeaveGroupRequest

```go
type LeaveGroupRequest struct {
    Version  int16
    GroupId  string
    MemberId string           // Removed in Version 3
    Members  []MemberIdentity // Added in Version 3
}
```

## type LeaveGroupResponse

```go
type LeaveGroupResponse struct {
    Version      int16
    ThrottleTime int32
    Err          KError
    Members      []MemberResponse
}
```

## type ListGroupsRequest

```go
type ListGroupsRequest struct{}
```

## type ListGroupsResponse

```go
type ListGroupsResponse struct {
    Err    KError
    Groups map[string]string
}
```

## type ListPartitionReassignmentsRequest

```go
type ListPartitionReassignmentsRequest struct {
    TimeoutMs int32

    Version int16
    // contains filtered or unexported fields
}
```

### func \(\*ListPartitionReassignmentsRequest\) AddBlock

```go
func (r *ListPartitionReassignmentsRequest) AddBlock(topic string, partitionIDs []int32)
```

## type ListPartitionReassignmentsResponse

```go
type ListPartitionReassignmentsResponse struct {
    Version        int16
    ThrottleTimeMs int32
    ErrorCode      KError
    ErrorMessage   *string
    TopicStatus    map[string]map[int32]*PartitionReplicaReassignmentsStatus
}
```

### func \(\*ListPartitionReassignmentsResponse\) AddBlock

```go
func (r *ListPartitionReassignmentsResponse) AddBlock(topic string, partition int32, replicas, addingReplicas, removingReplicas []int32)
```

## type MatchingAcl

MatchingAcl is a matching acl type

```go
type MatchingAcl struct {
    Err    KError
    ErrMsg *string
    Resource
    Acl
}
```

## type MemberIdentity

```go
type MemberIdentity struct {
    MemberId        string
    GroupInstanceId *string
}
```

## type MemberResponse

```go
type MemberResponse struct {
    MemberId        string
    GroupInstanceId *string
    Err             KError
}
```

## type Message

Message is a kafka message type

```go
type Message struct {
    Codec            CompressionCodec // codec used to compress the message contents
    CompressionLevel int              // compression level
    LogAppendTime    bool             // the used timestamp is LogAppendTime
    Key              []byte           // the message key, may be nil
    Value            []byte           // the message contents
    Set              *MessageSet      // the message set a message might wrap
    Version          int8             // v1 requires Kafka 0.10
    Timestamp        time.Time        // the timestamp of the message (version 1+ only)
    // contains filtered or unexported fields
}
```

## type MessageBlock

```go
type MessageBlock struct {
    Offset int64
    Msg    *Message
}
```

### func \(\*MessageBlock\) Messages

```go
func (msb *MessageBlock) Messages() []*MessageBlock
```

Messages convenience helper which returns either all the messages that are wrapped in this block

## type MessageSet

```go
type MessageSet struct {
    PartialTrailingMessage bool // whether the set on the wire contained an incomplete trailing MessageBlock
    OverflowMessage        bool // whether the set on the wire contained an overflow message
    Messages               []*MessageBlock
}
```

## type MetadataRequest

```go
type MetadataRequest struct {
    Version                int16
    Topics                 []string
    AllowAutoTopicCreation bool
}
```

## type MetadataResponse

```go
type MetadataResponse struct {
    Version        int16
    ThrottleTimeMs int32
    Brokers        []*Broker
    ClusterID      *string
    ControllerID   int32
    Topics         []*TopicMetadata
}
```

### func \(\*MetadataResponse\) AddBroker

```go
func (r *MetadataResponse) AddBroker(addr string, id int32)
```

### func \(\*MetadataResponse\) AddTopic

```go
func (r *MetadataResponse) AddTopic(topic string, err KError) *TopicMetadata
```

### func \(\*MetadataResponse\) AddTopicPartition

```go
func (r *MetadataResponse) AddTopicPartition(topic string, partition, brokerID int32, replicas, isr []int32, offline []int32, err KError)
```

## type MockAlterConfigsResponse

```go
type MockAlterConfigsResponse struct {
    // contains filtered or unexported fields
}
```

### func NewMockAlterConfigsResponse

```go
func NewMockAlterConfigsResponse(t TestReporter) *MockAlterConfigsResponse
```

### func \(\*MockAlterConfigsResponse\) For

```go
func (mr *MockAlterConfigsResponse) For(reqBody versionedDecoder) encoderWithHeader
```

## type MockAlterConfigsResponseWithErrorCode

```go
type MockAlterConfigsResponseWithErrorCode struct {
    // contains filtered or unexported fields
}
```

### func NewMockAlterConfigsResponseWithErrorCode

```go
func NewMockAlterConfigsResponseWithErrorCode(t TestReporter) *MockAlterConfigsResponseWithErrorCode
```

### func \(\*MockAlterConfigsResponseWithErrorCode\) For

```go
func (mr *MockAlterConfigsResponseWithErrorCode) For(reqBody versionedDecoder) encoderWithHeader
```

## type MockAlterPartitionReassignmentsResponse

```go
type MockAlterPartitionReassignmentsResponse struct {
    // contains filtered or unexported fields
}
```

### func NewMockAlterPartitionReassignmentsResponse

```go
func NewMockAlterPartitionReassignmentsResponse(t TestReporter) *MockAlterPartitionReassignmentsResponse
```

### func \(\*MockAlterPartitionReassignmentsResponse\) For

```go
func (mr *MockAlterPartitionReassignmentsResponse) For(reqBody versionedDecoder) encoderWithHeader
```

## type MockApiVersionsResponse

```go
type MockApiVersionsResponse struct {
    // contains filtered or unexported fields
}
```

### func NewMockApiVersionsResponse

```go
func NewMockApiVersionsResponse(t TestReporter) *MockApiVersionsResponse
```

### func \(\*MockApiVersionsResponse\) For

```go
func (m *MockApiVersionsResponse) For(reqBody versionedDecoder) encoderWithHeader
```

### func \(\*MockApiVersionsResponse\) SetApiKeys

```go
func (m *MockApiVersionsResponse) SetApiKeys(apiKeys []ApiVersionsResponseKey) *MockApiVersionsResponse
```

## type MockBroker

MockBroker is a mock Kafka broker that is used in unit tests. It is exposed to facilitate testing of higher level or specialized consumers and producers built on top of Sarama. Note that it does not 'mimic' the Kafka API protocol, but rather provides a facility to do that. It takes care of the TCP transport, request unmarshalling, response marshaling, and makes it the test writer responsibility to program correct according to the Kafka API protocol MockBroker behavior.

MockBroker is implemented as a TCP server listening on a kernel\-selected localhost port that can accept many connections. It reads Kafka requests from that connection and returns responses programmed by the SetHandlerByMap function. If a MockBroker receives a request that it has no programmed response for, then it returns nothing and the request times out.

A set of MockRequest builders to define mappings used by MockBroker is provided by Sarama. But users can develop MockRequests of their own and use them along with or instead of the standard ones.

When running tests with MockBroker it is strongly recommended to specify a timeout to \`go test\` so that if the broker hangs waiting for a response, the test panics.

It is not necessary to prefix message length or correlation ID to your response bytes, the server does that automatically as a convenience.

```go
type MockBroker struct {
    // contains filtered or unexported fields
}
```

### func NewMockBroker

```go
func NewMockBroker(t TestReporter, brokerID int32) *MockBroker
```

NewMockBroker launches a fake Kafka broker. It takes a TestReporter as provided by the test framework and a channel of responses to use.  If an error occurs it is simply logged to the TestReporter and the broker exits.

### func NewMockBrokerAddr

```go
func NewMockBrokerAddr(t TestReporter, brokerID int32, addr string) *MockBroker
```

NewMockBrokerAddr behaves like newMockBroker but listens on the address you give it rather than just some ephemeral port.

### func NewMockBrokerListener

```go
func NewMockBrokerListener(t TestReporter, brokerID int32, listener net.Listener) *MockBroker
```

NewMockBrokerListener behaves like newMockBrokerAddr but accepts connections on the listener specified.

### func \(\*MockBroker\) Addr

```go
func (b *MockBroker) Addr() string
```

Addr returns the broker connection string in the form "\<address\>:\<port\>".

### func \(\*MockBroker\) BrokerID

```go
func (b *MockBroker) BrokerID() int32
```

BrokerID returns broker ID assigned to the broker.

### func \(\*MockBroker\) Close

```go
func (b *MockBroker) Close()
```

Close terminates the broker blocking until it stops internal goroutines and releases all resources.

### func \(\*MockBroker\) History

```go
func (b *MockBroker) History() []RequestResponse
```

History returns a slice of RequestResponse pairs in the order they were processed by the broker. Note that in case of multiple connections to the broker the order expected by a test can be different from the order recorded in the history, unless some synchronization is implemented in the test.

### func \(\*MockBroker\) Port

```go
func (b *MockBroker) Port() int32
```

Port returns the TCP port number the broker is listening for requests on.

### func \(\*MockBroker\) Returns

```go
func (b *MockBroker) Returns(e encoderWithHeader)
```

### func \(\*MockBroker\) SetGSSAPIHandler

```go
func (b *MockBroker) SetGSSAPIHandler(handler GSSApiHandlerFunc)
```

### func \(\*MockBroker\) SetHandlerByMap

```go
func (b *MockBroker) SetHandlerByMap(handlerMap map[string]MockResponse)
```

SetHandlerByMap defines mapping of Request types to MockResponses. When a request is received by the broker, it looks up the request type in the map and uses the found MockResponse instance to generate an appropriate reply. If the request type is not found in the map then nothing is sent.

### func \(\*MockBroker\) SetLatency

```go
func (b *MockBroker) SetLatency(latency time.Duration)
```

SetLatency makes broker pause for the specified period every time before replying.

### func \(\*MockBroker\) SetNotifier

```go
func (b *MockBroker) SetNotifier(notifier RequestNotifierFunc)
```

SetNotifier set a function that will get invoked whenever a request has been processed successfully and will provide the number of bytes read and written

## type MockConsumerMetadataResponse

MockConsumerMetadataResponse is a \`ConsumerMetadataResponse\` builder.

```go
type MockConsumerMetadataResponse struct {
    // contains filtered or unexported fields
}
```

### func NewMockConsumerMetadataResponse

```go
func NewMockConsumerMetadataResponse(t TestReporter) *MockConsumerMetadataResponse
```

### func \(\*MockConsumerMetadataResponse\) For

```go
func (mr *MockConsumerMetadataResponse) For(reqBody versionedDecoder) encoderWithHeader
```

### func \(\*MockConsumerMetadataResponse\) SetCoordinator

```go
func (mr *MockConsumerMetadataResponse) SetCoordinator(group string, broker *MockBroker) *MockConsumerMetadataResponse
```

### func \(\*MockConsumerMetadataResponse\) SetError

```go
func (mr *MockConsumerMetadataResponse) SetError(group string, kerror KError) *MockConsumerMetadataResponse
```

## type MockCreateAclsResponse

```go
type MockCreateAclsResponse struct {
    // contains filtered or unexported fields
}
```

### func NewMockCreateAclsResponse

```go
func NewMockCreateAclsResponse(t TestReporter) *MockCreateAclsResponse
```

### func \(\*MockCreateAclsResponse\) For

```go
func (mr *MockCreateAclsResponse) For(reqBody versionedDecoder) encoderWithHeader
```

## type MockCreateAclsResponseError

```go
type MockCreateAclsResponseError struct {
    // contains filtered or unexported fields
}
```

### func NewMockCreateAclsResponseWithError

```go
func NewMockCreateAclsResponseWithError(t TestReporter) *MockCreateAclsResponseError
```

### func \(\*MockCreateAclsResponseError\) For

```go
func (mr *MockCreateAclsResponseError) For(reqBody versionedDecoder) encoderWithHeader
```

## type MockCreatePartitionsResponse

```go
type MockCreatePartitionsResponse struct {
    // contains filtered or unexported fields
}
```

### func NewMockCreatePartitionsResponse

```go
func NewMockCreatePartitionsResponse(t TestReporter) *MockCreatePartitionsResponse
```

### func \(\*MockCreatePartitionsResponse\) For

```go
func (mr *MockCreatePartitionsResponse) For(reqBody versionedDecoder) encoderWithHeader
```

## type MockCreateTopicsResponse

```go
type MockCreateTopicsResponse struct {
    // contains filtered or unexported fields
}
```

### func NewMockCreateTopicsResponse

```go
func NewMockCreateTopicsResponse(t TestReporter) *MockCreateTopicsResponse
```

### func \(\*MockCreateTopicsResponse\) For

```go
func (mr *MockCreateTopicsResponse) For(reqBody versionedDecoder) encoderWithHeader
```

## type MockDeleteAclsResponse

```go
type MockDeleteAclsResponse struct {
    // contains filtered or unexported fields
}
```

### func NewMockDeleteAclsResponse

```go
func NewMockDeleteAclsResponse(t TestReporter) *MockDeleteAclsResponse
```

### func \(\*MockDeleteAclsResponse\) For

```go
func (mr *MockDeleteAclsResponse) For(reqBody versionedDecoder) encoderWithHeader
```

## type MockDeleteGroupsResponse

```go
type MockDeleteGroupsResponse struct {
    // contains filtered or unexported fields
}
```

### func NewMockDeleteGroupsRequest

```go
func NewMockDeleteGroupsRequest(t TestReporter) *MockDeleteGroupsResponse
```

### func \(\*MockDeleteGroupsResponse\) For

```go
func (m *MockDeleteGroupsResponse) For(reqBody versionedDecoder) encoderWithHeader
```

### func \(\*MockDeleteGroupsResponse\) SetDeletedGroups

```go
func (m *MockDeleteGroupsResponse) SetDeletedGroups(groups []string) *MockDeleteGroupsResponse
```

## type MockDeleteOffsetResponse

```go
type MockDeleteOffsetResponse struct {
    // contains filtered or unexported fields
}
```

### func NewMockDeleteOffsetRequest

```go
func NewMockDeleteOffsetRequest(t TestReporter) *MockDeleteOffsetResponse
```

### func \(\*MockDeleteOffsetResponse\) For

```go
func (m *MockDeleteOffsetResponse) For(reqBody versionedDecoder) encoderWithHeader
```

### func \(\*MockDeleteOffsetResponse\) SetDeletedOffset

```go
func (m *MockDeleteOffsetResponse) SetDeletedOffset(errorCode KError, topic string, partition int32, errorPartition KError) *MockDeleteOffsetResponse
```

## type MockDeleteRecordsResponse

```go
type MockDeleteRecordsResponse struct {
    // contains filtered or unexported fields
}
```

### func NewMockDeleteRecordsResponse

```go
func NewMockDeleteRecordsResponse(t TestReporter) *MockDeleteRecordsResponse
```

### func \(\*MockDeleteRecordsResponse\) For

```go
func (mr *MockDeleteRecordsResponse) For(reqBody versionedDecoder) encoderWithHeader
```

## type MockDeleteTopicsResponse

```go
type MockDeleteTopicsResponse struct {
    // contains filtered or unexported fields
}
```

### func NewMockDeleteTopicsResponse

```go
func NewMockDeleteTopicsResponse(t TestReporter) *MockDeleteTopicsResponse
```

### func \(\*MockDeleteTopicsResponse\) For

```go
func (mr *MockDeleteTopicsResponse) For(reqBody versionedDecoder) encoderWithHeader
```

## type MockDescribeConfigsResponse

```go
type MockDescribeConfigsResponse struct {
    // contains filtered or unexported fields
}
```

### func NewMockDescribeConfigsResponse

```go
func NewMockDescribeConfigsResponse(t TestReporter) *MockDescribeConfigsResponse
```

### func \(\*MockDescribeConfigsResponse\) For

```go
func (mr *MockDescribeConfigsResponse) For(reqBody versionedDecoder) encoderWithHeader
```

## type MockDescribeConfigsResponseWithErrorCode

```go
type MockDescribeConfigsResponseWithErrorCode struct {
    // contains filtered or unexported fields
}
```

### func NewMockDescribeConfigsResponseWithErrorCode

```go
func NewMockDescribeConfigsResponseWithErrorCode(t TestReporter) *MockDescribeConfigsResponseWithErrorCode
```

### func \(\*MockDescribeConfigsResponseWithErrorCode\) For

```go
func (mr *MockDescribeConfigsResponseWithErrorCode) For(reqBody versionedDecoder) encoderWithHeader
```

## type MockDescribeGroupsResponse

```go
type MockDescribeGroupsResponse struct {
    // contains filtered or unexported fields
}
```

### func NewMockDescribeGroupsResponse

```go
func NewMockDescribeGroupsResponse(t TestReporter) *MockDescribeGroupsResponse
```

### func \(\*MockDescribeGroupsResponse\) AddGroupDescription

```go
func (m *MockDescribeGroupsResponse) AddGroupDescription(groupID string, description *GroupDescription) *MockDescribeGroupsResponse
```

### func \(\*MockDescribeGroupsResponse\) For

```go
func (m *MockDescribeGroupsResponse) For(reqBody versionedDecoder) encoderWithHeader
```

## type MockDescribeLogDirsResponse

```go
type MockDescribeLogDirsResponse struct {
    // contains filtered or unexported fields
}
```

### func NewMockDescribeLogDirsResponse

```go
func NewMockDescribeLogDirsResponse(t TestReporter) *MockDescribeLogDirsResponse
```

### func \(\*MockDescribeLogDirsResponse\) For

```go
func (m *MockDescribeLogDirsResponse) For(reqBody versionedDecoder) encoderWithHeader
```

### func \(\*MockDescribeLogDirsResponse\) SetLogDirs

```go
func (m *MockDescribeLogDirsResponse) SetLogDirs(logDirPath string, topicPartitions map[string]int) *MockDescribeLogDirsResponse
```

## type MockFetchResponse

MockFetchResponse is a \`FetchResponse\` builder.

```go
type MockFetchResponse struct {
    // contains filtered or unexported fields
}
```

### func NewMockFetchResponse

```go
func NewMockFetchResponse(t TestReporter, batchSize int) *MockFetchResponse
```

### func \(\*MockFetchResponse\) For

```go
func (mfr *MockFetchResponse) For(reqBody versionedDecoder) encoderWithHeader
```

### func \(\*MockFetchResponse\) SetHighWaterMark

```go
func (mfr *MockFetchResponse) SetHighWaterMark(topic string, partition int32, offset int64) *MockFetchResponse
```

### func \(\*MockFetchResponse\) SetMessage

```go
func (mfr *MockFetchResponse) SetMessage(topic string, partition int32, offset int64, msg Encoder) *MockFetchResponse
```

### func \(\*MockFetchResponse\) SetMessageWithKey

```go
func (mfr *MockFetchResponse) SetMessageWithKey(topic string, partition int32, offset int64, key, msg Encoder) *MockFetchResponse
```

## type MockFindCoordinatorResponse

MockFindCoordinatorResponse is a \`FindCoordinatorResponse\` builder.

```go
type MockFindCoordinatorResponse struct {
    // contains filtered or unexported fields
}
```

### func NewMockFindCoordinatorResponse

```go
func NewMockFindCoordinatorResponse(t TestReporter) *MockFindCoordinatorResponse
```

### func \(\*MockFindCoordinatorResponse\) For

```go
func (mr *MockFindCoordinatorResponse) For(reqBody versionedDecoder) encoderWithHeader
```

### func \(\*MockFindCoordinatorResponse\) SetCoordinator

```go
func (mr *MockFindCoordinatorResponse) SetCoordinator(coordinatorType CoordinatorType, group string, broker *MockBroker) *MockFindCoordinatorResponse
```

### func \(\*MockFindCoordinatorResponse\) SetError

```go
func (mr *MockFindCoordinatorResponse) SetError(coordinatorType CoordinatorType, group string, kerror KError) *MockFindCoordinatorResponse
```

## type MockHeartbeatResponse

```go
type MockHeartbeatResponse struct {
    Err KError
    // contains filtered or unexported fields
}
```

### func NewMockHeartbeatResponse

```go
func NewMockHeartbeatResponse(t TestReporter) *MockHeartbeatResponse
```

### func \(\*MockHeartbeatResponse\) For

```go
func (m *MockHeartbeatResponse) For(reqBody versionedDecoder) encoderWithHeader
```

### func \(\*MockHeartbeatResponse\) SetError

```go
func (m *MockHeartbeatResponse) SetError(kerr KError) *MockHeartbeatResponse
```

## type MockIncrementalAlterConfigsResponse

```go
type MockIncrementalAlterConfigsResponse struct {
    // contains filtered or unexported fields
}
```

### func NewMockIncrementalAlterConfigsResponse

```go
func NewMockIncrementalAlterConfigsResponse(t TestReporter) *MockIncrementalAlterConfigsResponse
```

### func \(\*MockIncrementalAlterConfigsResponse\) For

```go
func (mr *MockIncrementalAlterConfigsResponse) For(reqBody versionedDecoder) encoderWithHeader
```

## type MockIncrementalAlterConfigsResponseWithErrorCode

```go
type MockIncrementalAlterConfigsResponseWithErrorCode struct {
    // contains filtered or unexported fields
}
```

### func NewMockIncrementalAlterConfigsResponseWithErrorCode

```go
func NewMockIncrementalAlterConfigsResponseWithErrorCode(t TestReporter) *MockIncrementalAlterConfigsResponseWithErrorCode
```

### func \(\*MockIncrementalAlterConfigsResponseWithErrorCode\) For

```go
func (mr *MockIncrementalAlterConfigsResponseWithErrorCode) For(reqBody versionedDecoder) encoderWithHeader
```

## type MockJoinGroupResponse

```go
type MockJoinGroupResponse struct {
    ThrottleTime  int32
    Err           KError
    GenerationId  int32
    GroupProtocol string
    LeaderId      string
    MemberId      string
    Members       []GroupMember
    // contains filtered or unexported fields
}
```

### func NewMockJoinGroupResponse

```go
func NewMockJoinGroupResponse(t TestReporter) *MockJoinGroupResponse
```

### func \(\*MockJoinGroupResponse\) For

```go
func (m *MockJoinGroupResponse) For(reqBody versionedDecoder) encoderWithHeader
```

### func \(\*MockJoinGroupResponse\) SetError

```go
func (m *MockJoinGroupResponse) SetError(kerr KError) *MockJoinGroupResponse
```

### func \(\*MockJoinGroupResponse\) SetGenerationId

```go
func (m *MockJoinGroupResponse) SetGenerationId(id int32) *MockJoinGroupResponse
```

### func \(\*MockJoinGroupResponse\) SetGroupProtocol

```go
func (m *MockJoinGroupResponse) SetGroupProtocol(proto string) *MockJoinGroupResponse
```

### func \(\*MockJoinGroupResponse\) SetLeaderId

```go
func (m *MockJoinGroupResponse) SetLeaderId(id string) *MockJoinGroupResponse
```

### func \(\*MockJoinGroupResponse\) SetMember

```go
func (m *MockJoinGroupResponse) SetMember(id string, meta *ConsumerGroupMemberMetadata) *MockJoinGroupResponse
```

### func \(\*MockJoinGroupResponse\) SetMemberId

```go
func (m *MockJoinGroupResponse) SetMemberId(id string) *MockJoinGroupResponse
```

### func \(\*MockJoinGroupResponse\) SetThrottleTime

```go
func (m *MockJoinGroupResponse) SetThrottleTime(t int32) *MockJoinGroupResponse
```

## type MockKerberosClient

```go
type MockKerberosClient struct {
    ASRep messages.ASRep
    // contains filtered or unexported fields
}
```

### func \(\*MockKerberosClient\) CName

```go
func (c *MockKerberosClient) CName() types.PrincipalName
```

### func \(\*MockKerberosClient\) Destroy

```go
func (c *MockKerberosClient) Destroy()
```

### func \(\*MockKerberosClient\) Domain

```go
func (c *MockKerberosClient) Domain() string
```

### func \(\*MockKerberosClient\) GetServiceTicket

```go
func (c *MockKerberosClient) GetServiceTicket(spn string) (messages.Ticket, types.EncryptionKey, error)
```

### func \(\*MockKerberosClient\) Login

```go
func (c *MockKerberosClient) Login() error
```

## type MockLeaveGroupResponse

```go
type MockLeaveGroupResponse struct {
    Err KError
    // contains filtered or unexported fields
}
```

### func NewMockLeaveGroupResponse

```go
func NewMockLeaveGroupResponse(t TestReporter) *MockLeaveGroupResponse
```

### func \(\*MockLeaveGroupResponse\) For

```go
func (m *MockLeaveGroupResponse) For(reqBody versionedDecoder) encoderWithHeader
```

### func \(\*MockLeaveGroupResponse\) SetError

```go
func (m *MockLeaveGroupResponse) SetError(kerr KError) *MockLeaveGroupResponse
```

## type MockListAclsResponse

```go
type MockListAclsResponse struct {
    // contains filtered or unexported fields
}
```

### func NewMockListAclsResponse

```go
func NewMockListAclsResponse(t TestReporter) *MockListAclsResponse
```

### func \(\*MockListAclsResponse\) For

```go
func (mr *MockListAclsResponse) For(reqBody versionedDecoder) encoderWithHeader
```

## type MockListGroupsResponse

```go
type MockListGroupsResponse struct {
    // contains filtered or unexported fields
}
```

### func NewMockListGroupsResponse

```go
func NewMockListGroupsResponse(t TestReporter) *MockListGroupsResponse
```

### func \(\*MockListGroupsResponse\) AddGroup

```go
func (m *MockListGroupsResponse) AddGroup(groupID, protocolType string) *MockListGroupsResponse
```

### func \(\*MockListGroupsResponse\) For

```go
func (m *MockListGroupsResponse) For(reqBody versionedDecoder) encoderWithHeader
```

## type MockListPartitionReassignmentsResponse

```go
type MockListPartitionReassignmentsResponse struct {
    // contains filtered or unexported fields
}
```

### func NewMockListPartitionReassignmentsResponse

```go
func NewMockListPartitionReassignmentsResponse(t TestReporter) *MockListPartitionReassignmentsResponse
```

### func \(\*MockListPartitionReassignmentsResponse\) For

```go
func (mr *MockListPartitionReassignmentsResponse) For(reqBody versionedDecoder) encoderWithHeader
```

## type MockMetadataResponse

MockMetadataResponse is a \`MetadataResponse\` builder.

```go
type MockMetadataResponse struct {
    // contains filtered or unexported fields
}
```

### func NewMockMetadataResponse

```go
func NewMockMetadataResponse(t TestReporter) *MockMetadataResponse
```

### func \(\*MockMetadataResponse\) For

```go
func (mmr *MockMetadataResponse) For(reqBody versionedDecoder) encoderWithHeader
```

### func \(\*MockMetadataResponse\) SetBroker

```go
func (mmr *MockMetadataResponse) SetBroker(addr string, brokerID int32) *MockMetadataResponse
```

### func \(\*MockMetadataResponse\) SetController

```go
func (mmr *MockMetadataResponse) SetController(brokerID int32) *MockMetadataResponse
```

### func \(\*MockMetadataResponse\) SetLeader

```go
func (mmr *MockMetadataResponse) SetLeader(topic string, partition, brokerID int32) *MockMetadataResponse
```

## type MockOffsetCommitResponse

MockOffsetCommitResponse is a \`OffsetCommitResponse\` builder.

```go
type MockOffsetCommitResponse struct {
    // contains filtered or unexported fields
}
```

### func NewMockOffsetCommitResponse

```go
func NewMockOffsetCommitResponse(t TestReporter) *MockOffsetCommitResponse
```

### func \(\*MockOffsetCommitResponse\) For

```go
func (mr *MockOffsetCommitResponse) For(reqBody versionedDecoder) encoderWithHeader
```

### func \(\*MockOffsetCommitResponse\) SetError

```go
func (mr *MockOffsetCommitResponse) SetError(group, topic string, partition int32, kerror KError) *MockOffsetCommitResponse
```

## type MockOffsetFetchResponse

MockOffsetFetchResponse is a \`OffsetFetchResponse\` builder.

```go
type MockOffsetFetchResponse struct {
    // contains filtered or unexported fields
}
```

### func NewMockOffsetFetchResponse

```go
func NewMockOffsetFetchResponse(t TestReporter) *MockOffsetFetchResponse
```

### func \(\*MockOffsetFetchResponse\) For

```go
func (mr *MockOffsetFetchResponse) For(reqBody versionedDecoder) encoderWithHeader
```

### func \(\*MockOffsetFetchResponse\) SetError

```go
func (mr *MockOffsetFetchResponse) SetError(kerror KError) *MockOffsetFetchResponse
```

### func \(\*MockOffsetFetchResponse\) SetOffset

```go
func (mr *MockOffsetFetchResponse) SetOffset(group, topic string, partition int32, offset int64, metadata string, kerror KError) *MockOffsetFetchResponse
```

## type MockOffsetResponse

MockOffsetResponse is an \`OffsetResponse\` builder.

```go
type MockOffsetResponse struct {
    // contains filtered or unexported fields
}
```

### func NewMockOffsetResponse

```go
func NewMockOffsetResponse(t TestReporter) *MockOffsetResponse
```

### func \(\*MockOffsetResponse\) For

```go
func (mor *MockOffsetResponse) For(reqBody versionedDecoder) encoderWithHeader
```

### func \(\*MockOffsetResponse\) SetOffset

```go
func (mor *MockOffsetResponse) SetOffset(topic string, partition int32, time, offset int64) *MockOffsetResponse
```

## type MockProduceResponse

MockProduceResponse is a \`ProduceResponse\` builder.

```go
type MockProduceResponse struct {
    // contains filtered or unexported fields
}
```

### func NewMockProduceResponse

```go
func NewMockProduceResponse(t TestReporter) *MockProduceResponse
```

### func \(\*MockProduceResponse\) For

```go
func (mr *MockProduceResponse) For(reqBody versionedDecoder) encoderWithHeader
```

### func \(\*MockProduceResponse\) SetError

```go
func (mr *MockProduceResponse) SetError(topic string, partition int32, kerror KError) *MockProduceResponse
```

### func \(\*MockProduceResponse\) SetVersion

```go
func (mr *MockProduceResponse) SetVersion(version int16) *MockProduceResponse
```

## type MockResponse

MockResponse is a response builder interface it defines one method that allows generating a response based on a request body. MockResponses are used to program behavior of MockBroker in tests.

```go
type MockResponse interface {
    For(reqBody versionedDecoder) (res encoderWithHeader)
}
```

## type MockSaslAuthenticateResponse

```go
type MockSaslAuthenticateResponse struct {
    // contains filtered or unexported fields
}
```

### func NewMockSaslAuthenticateResponse

```go
func NewMockSaslAuthenticateResponse(t TestReporter) *MockSaslAuthenticateResponse
```

### func \(\*MockSaslAuthenticateResponse\) For

```go
func (msar *MockSaslAuthenticateResponse) For(reqBody versionedDecoder) encoderWithHeader
```

### func \(\*MockSaslAuthenticateResponse\) SetAuthBytes

```go
func (msar *MockSaslAuthenticateResponse) SetAuthBytes(saslAuthBytes []byte) *MockSaslAuthenticateResponse
```

### func \(\*MockSaslAuthenticateResponse\) SetError

```go
func (msar *MockSaslAuthenticateResponse) SetError(kerror KError) *MockSaslAuthenticateResponse
```

### func \(\*MockSaslAuthenticateResponse\) SetSessionLifetimeMs

```go
func (msar *MockSaslAuthenticateResponse) SetSessionLifetimeMs(sessionLifetimeMs int64) *MockSaslAuthenticateResponse
```

## type MockSaslHandshakeResponse

```go
type MockSaslHandshakeResponse struct {
    // contains filtered or unexported fields
}
```

### func NewMockSaslHandshakeResponse

```go
func NewMockSaslHandshakeResponse(t TestReporter) *MockSaslHandshakeResponse
```

### func \(\*MockSaslHandshakeResponse\) For

```go
func (mshr *MockSaslHandshakeResponse) For(reqBody versionedDecoder) encoderWithHeader
```

### func \(\*MockSaslHandshakeResponse\) SetEnabledMechanisms

```go
func (mshr *MockSaslHandshakeResponse) SetEnabledMechanisms(enabledMechanisms []string) *MockSaslHandshakeResponse
```

### func \(\*MockSaslHandshakeResponse\) SetError

```go
func (mshr *MockSaslHandshakeResponse) SetError(kerror KError) *MockSaslHandshakeResponse
```

## type MockSequence

MockSequence is a mock response builder that is created from a sequence of concrete responses. Every time when a \`MockBroker\` calls its \`For\` method the next response from the sequence is returned. When the end of the sequence is reached the last element from the sequence is returned.

```go
type MockSequence struct {
    // contains filtered or unexported fields
}
```

### func NewMockSequence

```go
func NewMockSequence(responses ...interface{}) *MockSequence
```

### func \(\*MockSequence\) For

```go
func (mc *MockSequence) For(reqBody versionedDecoder) (res encoderWithHeader)
```

## type MockSyncGroupResponse

```go
type MockSyncGroupResponse struct {
    Err              KError
    MemberAssignment []byte
    // contains filtered or unexported fields
}
```

### func NewMockSyncGroupResponse

```go
func NewMockSyncGroupResponse(t TestReporter) *MockSyncGroupResponse
```

### func \(\*MockSyncGroupResponse\) For

```go
func (m *MockSyncGroupResponse) For(reqBody versionedDecoder) encoderWithHeader
```

### func \(\*MockSyncGroupResponse\) SetError

```go
func (m *MockSyncGroupResponse) SetError(kerr KError) *MockSyncGroupResponse
```

### func \(\*MockSyncGroupResponse\) SetMemberAssignment

```go
func (m *MockSyncGroupResponse) SetMemberAssignment(assignment *ConsumerGroupMemberAssignment) *MockSyncGroupResponse
```

## type MockWrapper

MockWrapper is a mock response builder that returns a particular concrete response regardless of the actual request passed to the \`For\` method.

```go
type MockWrapper struct {
    // contains filtered or unexported fields
}
```

### func NewMockWrapper

```go
func NewMockWrapper(res encoderWithHeader) *MockWrapper
```

### func \(\*MockWrapper\) For

```go
func (mw *MockWrapper) For(reqBody versionedDecoder) (res encoderWithHeader)
```

## type OffsetCommitRequest

```go
type OffsetCommitRequest struct {
    ConsumerGroup           string
    ConsumerGroupGeneration int32   // v1 or later
    ConsumerID              string  // v1 or later
    GroupInstanceId         *string // v7 or later
    RetentionTime           int64   // v2 or later

    // Version can be:
    // - 0 (kafka 0.8.1 and later)
    // - 1 (kafka 0.8.2 and later)
    // - 2 (kafka 0.9.0 and later)
    // - 3 (kafka 0.11.0 and later)
    // - 4 (kafka 2.0.0 and later)
    // - 5&6 (kafka 2.1.0 and later)
    // - 7 (kafka 2.3.0 and later)
    Version int16
    // contains filtered or unexported fields
}
```

### func \(\*OffsetCommitRequest\) AddBlock

```go
func (r *OffsetCommitRequest) AddBlock(topic string, partitionID int32, offset int64, leaderEpoch int32, timestamp int64, metadata string)
```

### func \(\*OffsetCommitRequest\) Offset

```go
func (r *OffsetCommitRequest) Offset(topic string, partitionID int32) (int64, string, error)
```

## type OffsetCommitResponse

```go
type OffsetCommitResponse struct {
    Version        int16
    ThrottleTimeMs int32
    Errors         map[string]map[int32]KError
}
```

### func \(\*OffsetCommitResponse\) AddError

```go
func (r *OffsetCommitResponse) AddError(topic string, partition int32, kerror KError)
```

## type OffsetFetchRequest

```go
type OffsetFetchRequest struct {
    Version       int16
    ConsumerGroup string
    RequireStable bool // requires v7+
    // contains filtered or unexported fields
}
```

### func \(\*OffsetFetchRequest\) AddPartition

```go
func (r *OffsetFetchRequest) AddPartition(topic string, partitionID int32)
```

### func \(\*OffsetFetchRequest\) ZeroPartitions

```go
func (r *OffsetFetchRequest) ZeroPartitions()
```

## type OffsetFetchResponse

```go
type OffsetFetchResponse struct {
    Version        int16
    ThrottleTimeMs int32
    Blocks         map[string]map[int32]*OffsetFetchResponseBlock
    Err            KError
}
```

### func \(\*OffsetFetchResponse\) AddBlock

```go
func (r *OffsetFetchResponse) AddBlock(topic string, partition int32, block *OffsetFetchResponseBlock)
```

### func \(\*OffsetFetchResponse\) GetBlock

```go
func (r *OffsetFetchResponse) GetBlock(topic string, partition int32) *OffsetFetchResponseBlock
```

## type OffsetFetchResponseBlock

```go
type OffsetFetchResponseBlock struct {
    Offset      int64
    LeaderEpoch int32
    Metadata    string
    Err         KError
}
```

## type OffsetManager

OffsetManager uses Kafka to store and fetch consumed partition offsets.

```go
type OffsetManager interface {
    // ManagePartition creates a PartitionOffsetManager on the given topic/partition.
    // It will return an error if this OffsetManager is already managing the given
    // topic/partition.
    ManagePartition(topic string, partition int32) (PartitionOffsetManager, error)

    // Close stops the OffsetManager from managing offsets. It is required to call
    // this function before an OffsetManager object passes out of scope, as it
    // will otherwise leak memory. You must call this after all the
    // PartitionOffsetManagers are closed.
    Close() error

    // Commit commits the offsets. This method can be used if AutoCommit.Enable is
    // set to false.
    Commit()
}
```

### func NewOffsetManagerFromClient

```go
func NewOffsetManagerFromClient(group string, client Client) (OffsetManager, error)
```

NewOffsetManagerFromClient creates a new OffsetManager from the given client. It is still necessary to call Close\(\) on the underlying client when finished with the partition manager.

## type OffsetRequest

```go
type OffsetRequest struct {
    Version        int16
    IsolationLevel IsolationLevel
    // contains filtered or unexported fields
}
```

### func \(\*OffsetRequest\) AddBlock

```go
func (r *OffsetRequest) AddBlock(topic string, partitionID int32, time int64, maxOffsets int32)
```

### func \(\*OffsetRequest\) ReplicaID

```go
func (r *OffsetRequest) ReplicaID() int32
```

### func \(\*OffsetRequest\) SetReplicaID

```go
func (r *OffsetRequest) SetReplicaID(id int32)
```

## type OffsetResponse

```go
type OffsetResponse struct {
    Version        int16
    ThrottleTimeMs int32
    Blocks         map[string]map[int32]*OffsetResponseBlock
}
```

### func \(\*OffsetResponse\) AddTopicPartition

```go
func (r *OffsetResponse) AddTopicPartition(topic string, partition int32, offset int64)
```

### func \(\*OffsetResponse\) GetBlock

```go
func (r *OffsetResponse) GetBlock(topic string, partition int32) *OffsetResponseBlock
```

## type OffsetResponseBlock

```go
type OffsetResponseBlock struct {
    Err       KError
    Offsets   []int64 // Version 0
    Offset    int64   // Version 1
    Timestamp int64   // Version 1
}
```

## type OwnedPartition

```go
type OwnedPartition struct {
    Topic      string
    Partitions []int32
}
```

## type PacketDecodingError

PacketDecodingError is returned when there was an error \(other than truncated data\) decoding the Kafka broker's response. This can be a bad CRC or length field, or any other invalid value.

```go
type PacketDecodingError struct {
    Info string
}
```

### func \(PacketDecodingError\) Error

```go
func (err PacketDecodingError) Error() string
```

## type PacketEncodingError

PacketEncodingError is returned from a failure while encoding a Kafka packet. This can happen, for example, if you try to encode a string over 2^15 characters in length, since Kafka's encoding rules do not permit that.

```go
type PacketEncodingError struct {
    Info string
}
```

### func \(PacketEncodingError\) Error

```go
func (err PacketEncodingError) Error() string
```

## type PartitionConsumer

PartitionConsumer processes Kafka messages from a given topic and partition. You MUST call one of Close\(\) or AsyncClose\(\) on a PartitionConsumer to avoid leaks; it will not be garbage\-collected automatically when it passes out of scope.

The simplest way of using a PartitionConsumer is to loop over its Messages channel using a for/range loop. The PartitionConsumer will only stop itself in one case: when the offset being consumed is reported as out of range by the brokers. In this case you should decide what you want to do \(try a different offset, notify a human, etc\) and handle it appropriately. For all other error cases, it will just keep retrying. By default, it logs these errors to sarama.Logger; if you want to be notified directly of all errors, set your config's Consumer.Return.Errors to true and read from the Errors channel, using a select statement or a separate goroutine. Check out the Consumer examples to see implementations of these different approaches.

To terminate such a for/range loop while the loop is executing, call AsyncClose. This will kick off the process of consumer tear\-down & return immediately. Continue to loop, servicing the Messages channel until the teardown process AsyncClose initiated closes it \(thus terminating the for/range loop\). If you've already ceased reading Messages, call Close; this will signal the PartitionConsumer's goroutines to begin shutting down \(just like AsyncClose\), but will also drain the Messages channel, harvest all errors & return them once cleanup has completed.

```go
type PartitionConsumer interface {
    // AsyncClose initiates a shutdown of the PartitionConsumer. This method will return immediately, after which you
    // should continue to service the 'Messages' and 'Errors' channels until they are empty. It is required to call this
    // function, or Close before a consumer object passes out of scope, as it will otherwise leak memory. You must call
    // this before calling Close on the underlying client.
    AsyncClose()

    // Close stops the PartitionConsumer from fetching messages. It will initiate a shutdown just like AsyncClose, drain
    // the Messages channel, harvest any errors & return them to the caller. Note that if you are continuing to service
    // the Messages channel when this function is called, you will be competing with Close for messages; consider
    // calling AsyncClose, instead. It is required to call this function (or AsyncClose) before a consumer object passes
    // out of scope, as it will otherwise leak memory. You must call this before calling Close on the underlying client.
    Close() error

    // Messages returns the read channel for the messages that are returned by
    // the broker.
    Messages() <-chan *ConsumerMessage

    // Errors returns a read channel of errors that occurred during consuming, if
    // enabled. By default, errors are logged and not returned over this channel.
    // If you want to implement any custom error handling, set your config's
    // Consumer.Return.Errors setting to true, and read from this channel.
    Errors() <-chan *ConsumerError

    // HighWaterMarkOffset returns the high water mark offset of the partition,
    // i.e. the offset that will be used for the next message that will be produced.
    // You can use this to determine how far behind the processing is.
    HighWaterMarkOffset() int64

    // Pause suspends fetching from this partition. Future calls to the broker will not return
    // any records from these partition until it have been resumed using Resume().
    // Note that this method does not affect partition subscription.
    // In particular, it does not cause a group rebalance when automatic assignment is used.
    Pause()

    // Resume resumes this partition which have been paused with Pause().
    // New calls to the broker will return records from these partitions if there are any to be fetched.
    // If the partition was not previously paused, this method is a no-op.
    Resume()

    // IsPaused indicates if this partition consumer is paused or not
    IsPaused() bool
}
```

## type PartitionError

PartitionError is a partition error type

```go
type PartitionError struct {
    Partition int32
    Err       KError
}
```

## type PartitionMetadata

```go
type PartitionMetadata struct {
    Err             KError
    ID              int32
    Leader          int32
    Replicas        []int32
    Isr             []int32
    OfflineReplicas []int32
}
```

## type PartitionOffsetManager

PartitionOffsetManager uses Kafka to store and fetch consumed partition offsets. You MUST call Close\(\) on a partition offset manager to avoid leaks, it will not be garbage\-collected automatically when it passes out of scope.

```go
type PartitionOffsetManager interface {
    // NextOffset returns the next offset that should be consumed for the managed
    // partition, accompanied by metadata which can be used to reconstruct the state
    // of the partition consumer when it resumes. NextOffset() will return
    // `config.Consumer.Offsets.Initial` and an empty metadata string if no offset
    // was committed for this partition yet.
    NextOffset() (int64, string)

    // MarkOffset marks the provided offset, alongside a metadata string
    // that represents the state of the partition consumer at that point in time. The
    // metadata string can be used by another consumer to restore that state, so it
    // can resume consumption.
    //
    // To follow upstream conventions, you are expected to mark the offset of the
    // next message to read, not the last message read. Thus, when calling `MarkOffset`
    // you should typically add one to the offset of the last consumed message.
    //
    // Note: calling MarkOffset does not necessarily commit the offset to the backend
    // store immediately for efficiency reasons, and it may never be committed if
    // your application crashes. This means that you may end up processing the same
    // message twice, and your processing should ideally be idempotent.
    MarkOffset(offset int64, metadata string)

    // ResetOffset resets to the provided offset, alongside a metadata string that
    // represents the state of the partition consumer at that point in time. Reset
    // acts as a counterpart to MarkOffset, the difference being that it allows to
    // reset an offset to an earlier or smaller value, where MarkOffset only
    // allows incrementing the offset. cf MarkOffset for more details.
    ResetOffset(offset int64, metadata string)

    // Errors returns a read channel of errors that occur during offset management, if
    // enabled. By default, errors are logged and not returned over this channel. If
    // you want to implement any custom error handling, set your config's
    // Consumer.Return.Errors setting to true, and read from this channel.
    Errors() <-chan *ConsumerError

    // AsyncClose initiates a shutdown of the PartitionOffsetManager. This method will
    // return immediately, after which you should wait until the 'errors' channel has
    // been drained and closed. It is required to call this function, or Close before
    // a consumer object passes out of scope, as it will otherwise leak memory. You
    // must call this before calling Close on the underlying client.
    AsyncClose()

    // Close stops the PartitionOffsetManager from managing offsets. It is required to
    // call this function (or AsyncClose) before a PartitionOffsetManager object
    // passes out of scope, as it will otherwise leak memory. You must call this
    // before calling Close on the underlying client.
    Close() error
}
```

## type PartitionOffsetMetadata

```go
type PartitionOffsetMetadata struct {
    Partition int32
    Offset    int64
    Metadata  *string
}
```

## type PartitionReplicaReassignmentsStatus

```go
type PartitionReplicaReassignmentsStatus struct {
    Replicas         []int32
    AddingReplicas   []int32
    RemovingReplicas []int32
}
```

## type Partitioner

Partitioner is anything that, given a Kafka message and a number of partitions indexed \[0...numPartitions\-1\], decides to which partition to send the message. RandomPartitioner, RoundRobinPartitioner and HashPartitioner are provided as simple default implementations.

```go
type Partitioner interface {
    // Partition takes a message and partition count and chooses a partition
    Partition(message *ProducerMessage, numPartitions int32) (int32, error)

    // RequiresConsistency indicates to the user of the partitioner whether the
    // mapping of key->partition is consistent or not. Specifically, if a
    // partitioner requires consistency then it must be allowed to choose from all
    // partitions (even ones known to be unavailable), and its choice must be
    // respected by the caller. The obvious example is the HashPartitioner.
    RequiresConsistency() bool
}
```

### func NewHashPartitioner

```go
func NewHashPartitioner(topic string) Partitioner
```

NewHashPartitioner returns a Partitioner which behaves as follows. If the message's key is nil then a random partition is chosen. Otherwise the FNV\-1a hash of the encoded bytes of the message key is used, modulus the number of partitions. This ensures that messages with the same key always end up on the same partition.

### func NewManualPartitioner

```go
func NewManualPartitioner(topic string) Partitioner
```

NewManualPartitioner returns a Partitioner which uses the partition manually set in the provided ProducerMessage's Partition field as the partition to produce to.

### func NewRandomPartitioner

```go
func NewRandomPartitioner(topic string) Partitioner
```

NewRandomPartitioner returns a Partitioner which chooses a random partition each time.

### func NewReferenceHashPartitioner

```go
func NewReferenceHashPartitioner(topic string) Partitioner
```

NewReferenceHashPartitioner is like NewHashPartitioner except that it handles absolute values in the same way as the reference Java implementation. NewHashPartitioner was supposed to do that but it had a mistake and now there are people depending on both behaviors. This will all go away on the next major version bump.

### func NewRoundRobinPartitioner

```go
func NewRoundRobinPartitioner(topic string) Partitioner
```

NewRoundRobinPartitioner returns a Partitioner which walks through the available partitions one at a time.

## type PartitionerConstructor

PartitionerConstructor is the type for a function capable of constructing new Partitioners.

```go
type PartitionerConstructor func(topic string) Partitioner
```

### func NewCustomHashPartitioner

```go
func NewCustomHashPartitioner(hasher func() hash.Hash32) PartitionerConstructor
```

NewCustomHashPartitioner is a wrapper around NewHashPartitioner, allowing the use of custom hasher. The argument is a function providing the instance, implementing the hash.Hash32 interface. This is to ensure that each partition dispatcher gets its own hasher, to avoid concurrency issues by sharing an instance.

### func NewCustomPartitioner

```go
func NewCustomPartitioner(options ...HashPartitionerOption) PartitionerConstructor
```

NewCustomPartitioner creates a default Partitioner but lets you specify the behavior of each component via options

## type ProduceCallback

ProduceCallback function is called once the produce response has been parsed or could not be read.

```go
type ProduceCallback func(*ProduceResponse, error)
```

## type ProduceRequest

```go
type ProduceRequest struct {
    TransactionalID *string
    RequiredAcks    RequiredAcks
    Timeout         int32
    Version         int16 // v1 requires Kafka 0.9, v2 requires Kafka 0.10, v3 requires Kafka 0.11
    // contains filtered or unexported fields
}
```

### func \(\*ProduceRequest\) AddBatch

```go
func (r *ProduceRequest) AddBatch(topic string, partition int32, batch *RecordBatch)
```

### func \(\*ProduceRequest\) AddMessage

```go
func (r *ProduceRequest) AddMessage(topic string, partition int32, msg *Message)
```

### func \(\*ProduceRequest\) AddSet

```go
func (r *ProduceRequest) AddSet(topic string, partition int32, set *MessageSet)
```

## type ProduceResponse

```go
type ProduceResponse struct {
    Blocks       map[string]map[int32]*ProduceResponseBlock // v0, responses
    Version      int16
    ThrottleTime time.Duration // v1, throttle_time_ms
}
```

### func \(\*ProduceResponse\) AddTopicPartition

```go
func (r *ProduceResponse) AddTopicPartition(topic string, partition int32, err KError)
```

### func \(\*ProduceResponse\) GetBlock

```go
func (r *ProduceResponse) GetBlock(topic string, partition int32) *ProduceResponseBlock
```

## type ProduceResponseBlock

partition\_responses in protocol

```go
type ProduceResponseBlock struct {
    Err         KError    // v0, error_code
    Offset      int64     // v0, base_offset
    Timestamp   time.Time // v2, log_append_time, and the broker is configured with `LogAppendTime`
    StartOffset int64     // v5, log_start_offset
}
```

## type ProducerError

ProducerError is the type of error generated when the producer fails to deliver a message. It contains the original ProducerMessage as well as the actual error value.

```go
type ProducerError struct {
    Msg *ProducerMessage
    Err error
}
```

### func \(ProducerError\) Error

```go
func (pe ProducerError) Error() string
```

### func \(ProducerError\) Unwrap

```go
func (pe ProducerError) Unwrap() error
```

## type ProducerErrors

ProducerErrors is a type that wraps a batch of "ProducerError"s and implements the Error interface. It can be returned from the Producer's Close method to avoid the need to manually drain the Errors channel when closing a producer.

```go
type ProducerErrors []*ProducerError
```

### func \(ProducerErrors\) Error

```go
func (pe ProducerErrors) Error() string
```

## type ProducerInterceptor

ProducerInterceptor allows you to intercept \(and possibly mutate\) the records received by the producer before they are published to the Kafka cluster. https://cwiki.apache.org/confluence/display/KAFKA/KIP-42%3A+Add+Producer+and+Consumer+Interceptors#KIP42:AddProducerandConsumerInterceptors-Motivation

```go
type ProducerInterceptor interface {

    // OnSend is called when the producer message is intercepted. Please avoid
    // modifying the message until it's safe to do so, as this is _not_ a copy
    // of the message.
    OnSend(*ProducerMessage)
}
```

## type ProducerMessage

ProducerMessage is the collection of elements passed to the Producer in order to send a message.

```go
type ProducerMessage struct {
    Topic string // The Kafka topic for this message.
    // The partitioning key for this message. Pre-existing Encoders include
    // StringEncoder and ByteEncoder.
    Key Encoder
    // The actual message to store in Kafka. Pre-existing Encoders include
    // StringEncoder and ByteEncoder.
    Value Encoder

    // The headers are key-value pairs that are transparently passed
    // by Kafka between producers and consumers.
    Headers []RecordHeader

    // This field is used to hold arbitrary data you wish to include so it
    // will be available when receiving on the Successes and Errors channels.
    // Sarama completely ignores this field and is only to be used for
    // pass-through data.
    Metadata interface{}

    // Offset is the offset of the message stored on the broker. This is only
    // guaranteed to be defined if the message was successfully delivered and
    // RequiredAcks is not NoResponse.
    Offset int64
    // Partition is the partition that the message was sent to. This is only
    // guaranteed to be defined if the message was successfully delivered.
    Partition int32
    // Timestamp can vary in behavior depending on broker configuration, being
    // in either one of the CreateTime or LogAppendTime modes (default CreateTime),
    // and requiring version at least 0.10.0.
    //
    // When configured to CreateTime, the timestamp is specified by the producer
    // either by explicitly setting this field, or when the message is added
    // to a produce set.
    //
    // When configured to LogAppendTime, the timestamp assigned to the message
    // by the broker. This is only guaranteed to be defined if the message was
    // successfully delivered and RequiredAcks is not NoResponse.
    Timestamp time.Time
    // contains filtered or unexported fields
}
```

### func \(\*ProducerMessage\) ByteSize

```go
func (m *ProducerMessage) ByteSize(version int) int
```

## type ProducerTxnStatusFlag

ProducerTxnStatusFlag mark current transaction status.

```go
type ProducerTxnStatusFlag int16
```

```go
const (
    // ProducerTxnFlagUninitialized when txnmgr is created
    ProducerTxnFlagUninitialized ProducerTxnStatusFlag = 1 << iota
    // ProducerTxnFlagInitializing when txnmgr is initilizing
    ProducerTxnFlagInitializing
    // ProducerTxnFlagReady when is ready to receive transaction
    ProducerTxnFlagReady
    // ProducerTxnFlagInTransaction when transaction is started
    ProducerTxnFlagInTransaction
    // ProducerTxnFlagEndTransaction when transaction will be committed
    ProducerTxnFlagEndTransaction
    // ProducerTxnFlagInError whan having abortable or fatal error
    ProducerTxnFlagInError
    // ProducerTxnFlagCommittingTransaction when committing txn
    ProducerTxnFlagCommittingTransaction
    // ProducerTxnFlagAbortingTransaction when committing txn
    ProducerTxnFlagAbortingTransaction
    // ProducerTxnFlagAbortableError when producer encounter an abortable error
    // Must call AbortTxn in this case.
    ProducerTxnFlagAbortableError
    // ProducerTxnFlagFatalError when producer encounter an fatal error
    // Must Close an recreate it.
    ProducerTxnFlagFatalError
)
```

### func \(ProducerTxnStatusFlag\) String

```go
func (s ProducerTxnStatusFlag) String() string
```

## type QuotaEntityComponent

```go
type QuotaEntityComponent struct {
    EntityType QuotaEntityType
    MatchType  QuotaMatchType
    Name       string
}
```

## type QuotaEntityType

```go
type QuotaEntityType string
```

ref: https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/quota/ClientQuotaEntity.java

```go
const (
    QuotaEntityUser     QuotaEntityType = "user"
    QuotaEntityClientID QuotaEntityType = "client-id"
    QuotaEntityIP       QuotaEntityType = "ip"
)
```

## type QuotaFilterComponent

Describe a component for applying a client quota filter. EntityType: the entity type the filter component applies to \("user", "client\-id", "ip"\) MatchType: the match type of the filter component \(any, exact, default\) Match: the name that's matched exactly \(used when MatchType is QuotaMatchExact\)

```go
type QuotaFilterComponent struct {
    EntityType QuotaEntityType
    MatchType  QuotaMatchType
    Match      string
}
```

## type QuotaMatchType

```go
type QuotaMatchType int
```

ref: https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/requests/DescribeClientQuotasRequest.java

```go
const (
    QuotaMatchExact QuotaMatchType = iota
    QuotaMatchDefault
    QuotaMatchAny
)
```

## type Record

Record is kafka record type

```go
type Record struct {
    Headers []*RecordHeader

    Attributes     int8
    TimestampDelta time.Duration
    OffsetDelta    int64
    Key            []byte
    Value          []byte
    // contains filtered or unexported fields
}
```

## type RecordBatch

```go
type RecordBatch struct {
    FirstOffset           int64
    PartitionLeaderEpoch  int32
    Version               int8
    Codec                 CompressionCodec
    CompressionLevel      int
    Control               bool
    LogAppendTime         bool
    LastOffsetDelta       int32
    FirstTimestamp        time.Time
    MaxTimestamp          time.Time
    ProducerID            int64
    ProducerEpoch         int16
    FirstSequence         int32
    Records               []*Record
    PartialTrailingRecord bool
    IsTransactional       bool
    // contains filtered or unexported fields
}
```

### func \(\*RecordBatch\) LastOffset

```go
func (b *RecordBatch) LastOffset() int64
```

## type RecordHeader

RecordHeader stores key and value for a record header

```go
type RecordHeader struct {
    Key   []byte
    Value []byte
}
```

## type Records

Records implements a union type containing either a RecordBatch or a legacy MessageSet.

```go
type Records struct {
    MsgSet      *MessageSet
    RecordBatch *RecordBatch
    // contains filtered or unexported fields
}
```

## type RequestNotifierFunc

RequestNotifierFunc is invoked when a mock broker processes a request successfully and will provides the number of bytes read and written.

```go
type RequestNotifierFunc func(bytesRead, bytesWritten int)
```

## type RequestResponse

RequestResponse represents a Request/Response pair processed by MockBroker.

```go
type RequestResponse struct {
    Request  protocolBody
    Response encoder
}
```

## type RequiredAcks

RequiredAcks is used in Produce Requests to tell the broker how many replica acknowledgements it must see before responding. Any of the constants defined here are valid. On broker versions prior to 0.8.2.0 any other positive int16 is also valid \(the broker will wait for that many acknowledgements\) but in 0.8.2.0 and later this will raise an exception \(it has been replaced by setting the \`min.isr\` value in the brokers configuration\).

```go
type RequiredAcks int16
```

```go
const (
    // NoResponse doesn't send any response, the TCP ACK is all you get.
    NoResponse RequiredAcks = 0
    // WaitForLocal waits for only the local commit to succeed before responding.
    WaitForLocal RequiredAcks = 1
    // WaitForAll waits for all in-sync replicas to commit before responding.
    // The minimum number of in-sync replicas is configured on the broker via
    // the `min.insync.replicas` configuration key.
    WaitForAll RequiredAcks = -1
)
```

## type Resource

Resource holds information about acl resource type

```go
type Resource struct {
    ResourceType        AclResourceType
    ResourceName        string
    ResourcePatternType AclResourcePatternType
}
```

## type ResourceAcls

ResourceAcls is an acl resource type

```go
type ResourceAcls struct {
    Resource
    Acls []*Acl
}
```

## type ResourceResponse

```go
type ResourceResponse struct {
    ErrorCode int16
    ErrorMsg  string
    Type      ConfigResourceType
    Name      string
    Configs   []*ConfigEntry
}
```

## type SASLMechanism

SASLMechanism specifies the SASL mechanism the client uses to authenticate with the broker

```go
type SASLMechanism string
```

## type SCRAMClient

SCRAMClient is a an interface to a SCRAM client implementation.

```go
type SCRAMClient interface {
    // Begin prepares the client for the SCRAM exchange
    // with the server with a user name and a password
    Begin(userName, password, authzID string) error
    // Step steps client through the SCRAM exchange. It is
    // called repeatedly until it errors or `Done` returns true.
    Step(challenge string) (response string, err error)
    // Done should return true when the SCRAM conversation
    // is over.
    Done() bool
}
```

## type SaslAuthenticateRequest

```go
type SaslAuthenticateRequest struct {
    // Version defines the protocol version to use for encode and decode
    Version       int16
    SaslAuthBytes []byte
}
```

## type SaslAuthenticateResponse

```go
type SaslAuthenticateResponse struct {
    // Version defines the protocol version to use for encode and decode
    Version           int16
    Err               KError
    ErrorMessage      *string
    SaslAuthBytes     []byte
    SessionLifetimeMs int64
}
```

## type SaslHandshakeRequest

```go
type SaslHandshakeRequest struct {
    Mechanism string
    Version   int16
}
```

## type SaslHandshakeResponse

```go
type SaslHandshakeResponse struct {
    Err               KError
    EnabledMechanisms []string
}
```

## type ScramMechanismType

```go
type ScramMechanismType int8
```

```go
const (
    SCRAM_MECHANISM_UNKNOWN ScramMechanismType = iota // 0
    SCRAM_MECHANISM_SHA_256                           // 1
    SCRAM_MECHANISM_SHA_512                           // 2
)
```

### func \(ScramMechanismType\) String

```go
func (s ScramMechanismType) String() string
```

## type StdLogger

StdLogger is used to log error messages.

```go
type StdLogger interface {
    Print(v ...interface{})
    Printf(format string, v ...interface{})
    Println(v ...interface{})
}
```

DebugLogger is the instance of a StdLogger that Sarama writes more verbose debug information to. By default it is set to redirect all debug to the default Logger above, but you can optionally set it to another StdLogger instance to \(e.g.,\) discard debug information

```go
var DebugLogger StdLogger = &debugLogger{}
```

## type StickyAssignorUserData

```go
type StickyAssignorUserData interface {
    // contains filtered or unexported methods
}
```

## type StickyAssignorUserDataV0

StickyAssignorUserDataV0 holds topic partition information for an assignment

```go
type StickyAssignorUserDataV0 struct {
    Topics map[string][]int32
    // contains filtered or unexported fields
}
```

## type StickyAssignorUserDataV1

StickyAssignorUserDataV1 holds topic partition information for an assignment

```go
type StickyAssignorUserDataV1 struct {
    Topics     map[string][]int32
    Generation int32
    // contains filtered or unexported fields
}
```

## type StringEncoder

StringEncoder implements the Encoder interface for Go strings so that they can be used as the Key or Value in a ProducerMessage.

```go
type StringEncoder string
```

### func \(StringEncoder\) Encode

```go
func (s StringEncoder) Encode() ([]byte, error)
```

### func \(StringEncoder\) Length

```go
func (s StringEncoder) Length() int
```

## type SyncGroupRequest

```go
type SyncGroupRequest struct {
    // Version defines the protocol version to use for encode and decode
    Version int16
    // GroupId contains the unique group identifier.
    GroupId string
    // GenerationId contains the generation of the group.
    GenerationId int32
    // MemberId contains the member ID assigned by the group.
    MemberId string
    // GroupInstanceId contains the unique identifier of the consumer instance provided by end user.
    GroupInstanceId *string
    // GroupAssignments contains each assignment.
    GroupAssignments []SyncGroupRequestAssignment
}
```

### func \(\*SyncGroupRequest\) AddGroupAssignment

```go
func (r *SyncGroupRequest) AddGroupAssignment(memberId string, memberAssignment []byte)
```

### func \(\*SyncGroupRequest\) AddGroupAssignmentMember

```go
func (r *SyncGroupRequest) AddGroupAssignmentMember(memberId string, memberAssignment *ConsumerGroupMemberAssignment) error
```

## type SyncGroupRequestAssignment

```go
type SyncGroupRequestAssignment struct {
    // MemberId contains the ID of the member to assign.
    MemberId string
    // Assignment contains the member assignment.
    Assignment []byte
}
```

## type SyncGroupResponse

```go
type SyncGroupResponse struct {
    // Version defines the protocol version to use for encode and decode
    Version int16
    // ThrottleTimeMs contains the duration in milliseconds for which the
    // request was throttled due to a quota violation, or zero if the request
    // did not violate any quota.
    ThrottleTime int32
    // Err contains the error code, or 0 if there was no error.
    Err KError
    // MemberAssignment contains the member assignment.
    MemberAssignment []byte
}
```

### func \(\*SyncGroupResponse\) GetMemberAssignment

```go
func (r *SyncGroupResponse) GetMemberAssignment() (*ConsumerGroupMemberAssignment, error)
```

## type SyncProducer

SyncProducer publishes Kafka messages, blocking until they have been acknowledged. It routes messages to the correct broker, refreshing metadata as appropriate, and parses responses for errors. You must call Close\(\) on a producer to avoid leaks, it may not be garbage\-collected automatically when it passes out of scope.

The SyncProducer comes with two caveats: it will generally be less efficient than the AsyncProducer, and the actual durability guarantee provided when a message is acknowledged depend on the configured value of \`Producer.RequiredAcks\`. There are configurations where a message acknowledged by the SyncProducer can still sometimes be lost.

For implementation reasons, the SyncProducer requires \`Producer.Return.Errors\` and \`Producer.Return.Successes\` to be set to true in its configuration.

```go
type SyncProducer interface {

    // SendMessage produces a given message, and returns only when it either has
    // succeeded or failed to produce. It will return the partition and the offset
    // of the produced message, or an error if the message failed to produce.
    SendMessage(msg *ProducerMessage) (partition int32, offset int64, err error)

    // SendMessages produces a given set of messages, and returns only when all
    // messages in the set have either succeeded or failed. Note that messages
    // can succeed and fail individually; if some succeed and some fail,
    // SendMessages will return an error.
    SendMessages(msgs []*ProducerMessage) error

    // Close shuts down the producer; you must call this function before a producer
    // object passes out of scope, as it may otherwise leak memory.
    // You must call this before calling Close on the underlying client.
    Close() error

    // TxnStatus return current producer transaction status.
    TxnStatus() ProducerTxnStatusFlag

    // IsTransactional return true when current producer is is transactional.
    IsTransactional() bool

    // BeginTxn mark current transaction as ready.
    BeginTxn() error

    // CommitTxn commit current transaction.
    CommitTxn() error

    // AbortTxn abort current transaction.
    AbortTxn() error

    // AddOffsetsToTxn add associated offsets to current transaction.
    AddOffsetsToTxn(offsets map[string][]*PartitionOffsetMetadata, groupId string) error

    // AddMessageToTxn add message offsets to current transaction.
    AddMessageToTxn(msg *ConsumerMessage, groupId string, metadata *string) error
}
```

### func NewSyncProducer

```go
func NewSyncProducer(addrs []string, config *Config) (SyncProducer, error)
```

NewSyncProducer creates a new SyncProducer using the given broker addresses and configuration.

### func NewSyncProducerFromClient

```go
func NewSyncProducerFromClient(client Client) (SyncProducer, error)
```

NewSyncProducerFromClient creates a new SyncProducer using the given client. It is still necessary to call Close\(\) on the underlying client when shutting down this producer.

## type TestReporter

TestReporter has methods matching go's testing.T to avoid importing \`testing\` in the main part of the library.

```go
type TestReporter interface {
    Error(...interface{})
    Errorf(string, ...interface{})
    Fatal(...interface{})
    Fatalf(string, ...interface{})
}
```

## type Timestamp

```go
type Timestamp struct {
    *time.Time
}
```

## type TopicDetail

```go
type TopicDetail struct {
    NumPartitions     int32
    ReplicationFactor int16
    ReplicaAssignment map[int32][]int32
    ConfigEntries     map[string]*string
}
```

## type TopicError

```go
type TopicError struct {
    Err    KError
    ErrMsg *string
}
```

### func \(\*TopicError\) Error

```go
func (t *TopicError) Error() string
```

### func \(\*TopicError\) Unwrap

```go
func (t *TopicError) Unwrap() error
```

## type TopicMetadata

```go
type TopicMetadata struct {
    Err        KError
    Name       string
    IsInternal bool // Only valid for Version >= 1
    Partitions []*PartitionMetadata
}
```

## type TopicPartition

```go
type TopicPartition struct {
    Count      int32
    Assignment [][]int32
}
```

## type TopicPartitionError

```go
type TopicPartitionError struct {
    Err    KError
    ErrMsg *string
}
```

### func \(\*TopicPartitionError\) Error

```go
func (t *TopicPartitionError) Error() string
```

### func \(\*TopicPartitionError\) Unwrap

```go
func (t *TopicPartitionError) Unwrap() error
```

## type TxnOffsetCommitRequest

```go
type TxnOffsetCommitRequest struct {
    TransactionalID string
    GroupID         string
    ProducerID      int64
    ProducerEpoch   int16
    Topics          map[string][]*PartitionOffsetMetadata
}
```

## type TxnOffsetCommitResponse

```go
type TxnOffsetCommitResponse struct {
    ThrottleTime time.Duration
    Topics       map[string][]*PartitionError
}
```

## type UserScramCredentialsResponseInfo

```go
type UserScramCredentialsResponseInfo struct {
    Mechanism  ScramMechanismType
    Iterations int32
}
```

## type ZstdDecoderParams

```go
type ZstdDecoderParams struct {
}
```

## type ZstdEncoderParams

```go
type ZstdEncoderParams struct {
    Level int
}
```



Generated by [gomarkdoc](<https://github.com/princjef/gomarkdoc>)
